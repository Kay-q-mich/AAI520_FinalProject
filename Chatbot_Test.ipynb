{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09ChAkvhw54a",
    "outputId": "6a99efef-0a51-4420-e690-94907a703930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fieaX4UqR0Rs",
    "outputId": "b5ac2ea0-1f8d-4763-c16e-5fb8212fda4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Target: Not the hacking and gagging and spitting part.  Please.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Load the Cornell Movie Dialogs Corpus files\n",
    "data_dir = '/content/drive/MyDrive/AAI520-NLP/Final_Project/data'\n",
    "lines_file = os.path.join(data_dir, 'movie_lines.txt')\n",
    "conversations_file = os.path.join(data_dir, 'movie_conversations.txt')\n",
    "\n",
    "# Load movie lines and conversations\n",
    "def load_lines(file_path):\n",
    "    # Load all the lines from the movie_lines.txt file and store them in a dictionary.\n",
    "    lines = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file.readlines():\n",
    "            parts = line.split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                # Line ID -> Dialogue text\n",
    "                lines[parts[0]] = parts[4].strip()\n",
    "    return lines\n",
    "\n",
    "def load_conversations(file_path, lines):\n",
    "    # Load conversations from movie_conversations.txt and match with the corresponding lines.\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file.readlines():\n",
    "            parts = line.split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                line_ids = eval(parts[3])  # Extract the list of line IDs\n",
    "                conversation = [lines[line_id] for line_id in line_ids if line_id in lines]\n",
    "                conversations.append(conversation)\n",
    "    return conversations\n",
    "\n",
    "# Load the data\n",
    "lines = load_lines(lines_file)\n",
    "conversations = load_conversations(conversations_file, lines)\n",
    "\n",
    "# Create input-output pairs from conversations\n",
    "def create_conversation_pairs(conversations, context_size=2):\n",
    "    \"\"\"Create input-output pairs from the conversations using a sliding window approach.\"\"\"\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation) - context_size):\n",
    "            # Join the context lines as input\n",
    "            input_text = \" \".join(conversation[i:i + context_size])\n",
    "            # Next line is the target\n",
    "            target_text = conversation[i + context_size]\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "    return input_texts, target_texts\n",
    "\n",
    "# Generate input and target pairs\n",
    "input_texts, target_texts = create_conversation_pairs(conversations, context_size=2)\n",
    "\n",
    "# Print a sample input-output pair\n",
    "print(\"Input:\", input_texts[0])\n",
    "print(\"Target:\", target_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tTKYCrU0ywmB"
   },
   "outputs": [],
   "source": [
    "# Use a small sample of the data\n",
    "\n",
    "input_texts = input_texts[:1000]\n",
    "target_texts = target_texts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbIMH5azg5Nd",
    "outputId": "8bfb0212-de31-4676-b575-80cdc24b2b4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token as eos_token\n",
    "\n",
    "# Ensure you're using the right device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset class for tokenized inputs\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer, max_length=128):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize inputs and targets\n",
    "        inputs = self.tokenizer(self.input_texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(self.target_texts[idx], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Move to the appropriate device (GPU/CPU)\n",
    "        inputs = {key: value.squeeze(0).to(device) for key, value in inputs.items()}\n",
    "        targets = {key: value.squeeze(0).to(device) for key, value in targets.items()}\n",
    "\n",
    "        # Set labels\n",
    "        inputs['labels'] = targets['input_ids']\n",
    "\n",
    "        return inputs\n",
    "\n",
    "# Create the dataset for training and evaluation\n",
    "train_input_texts = input_texts[:800]\n",
    "train_target_texts = target_texts[:800]\n",
    "\n",
    "eval_input_texts = input_texts[800:1000]\n",
    "eval_target_texts = target_texts[800:1000]\n",
    "\n",
    "# Instantiate custom datasets\n",
    "train_dataset = CustomDataset(train_input_texts, train_target_texts, tokenizer)\n",
    "eval_dataset = CustomDataset(eval_input_texts, eval_target_texts, tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",            # Save model at the end of each epoch\n",
    "    logging_dir=\"./logs\",             # Directory for logs\n",
    "    logging_steps=10,                 # Log every 10 steps\n",
    "    per_device_train_batch_size=2,    # Batch size for training\n",
    "    num_train_epochs=3,               # Number of epochs\n",
    "    report_to=\"all\",                  # Report to stdout and log file\n",
    "    load_best_model_at_end=True,      # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Data collator for padding the data\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = model.to(device)  # Move model to the GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "d0dCfM9J7IxD",
    "outputId": "3e6cb90a-899e-49d7-dd70-bf7d4aeec722"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 02:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.365400</td>\n",
       "      <td>3.766725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.514500</td>\n",
       "      <td>3.983235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.508300</td>\n",
       "      <td>4.198936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=2.8556591272354126, metrics={'train_runtime': 179.5998, 'train_samples_per_second': 13.363, 'train_steps_per_second': 6.682, 'total_flos': 156775219200000.0, 'train_loss': 2.8556591272354126, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer with train and eval datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,      # Use the custom train_dataset\n",
    "    eval_dataset=eval_dataset,        # Use the custom eval_dataset\n",
    "    data_collator=data_collator,      # Collate the data with padding\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ztn4B-ahPLp",
    "outputId": "087bf7b1-995a-4e97-98cc-5b136784915d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! How are you?  I'm fine.\n",
      "Chatbot: What's your favorite movie?  You know, I'm not a fan of movies.\n",
      "Chatbot: Do you like pizza?  I like pizza.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate responses using the fine-tuned model\n",
    "def generate_response(prompt, model, tokenizer, max_length=50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response with modified settings\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,                 # Limit response length\n",
    "        pad_token_id=tokenizer.eos_token_id,   # Ensure padding uses EOS token\n",
    "        no_repeat_ngram_size=3,                # Prevent repeating 3-grams\n",
    "        top_k=50,                              # Consider top 50 words by probability\n",
    "        top_p=0.9,                             # Use nucleus sampling with 90% probability mass\n",
    "        temperature=0.7,                       # Control randomness\n",
    "        early_stopping=True                    # Stop early at a coherent response\n",
    "    )\n",
    "\n",
    "    # Decode the output and stop at the first period, exclamation mark, or question mark\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Stop at the first complete sentence\n",
    "    for end_char in [\".\", \"!\", \"?\"]:\n",
    "        if end_char in response:\n",
    "            response = response.split(end_char)[0] + end_char\n",
    "            break\n",
    "\n",
    "    return response\n",
    "\n",
    "# Sample conversation\n",
    "user_input = \"Hi! How are you?\"\n",
    "response = generate_response(user_input, model, tokenizer)\n",
    "print(\"Chatbot:\", response)\n",
    "\n",
    "user_input = \"What's your favorite movie?\"\n",
    "response = generate_response(user_input, model, tokenizer)\n",
    "print(\"Chatbot:\", response)\n",
    "\n",
    "user_input = \"Do you like pizza?\"\n",
    "response = generate_response(user_input, model, tokenizer)\n",
    "print(\"Chatbot:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY6lQTge0iXR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
