{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3e6bmzNFFEi"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOqr4GXTHlVq"
      },
      "source": [
        " - All necessary libraries, tools, and utilities needed to approach the language model. Torch and related imports are for deep learning tasks, and utilities like csv, os, json for handling the actual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bfd5vNJwrDkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c52418-7d7a-48ac-ec93-2ef9aadf0516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ojbWCkV1alXV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "import spacy\n",
        "import nltk\n",
        "from torch.jit import script, trace\n",
        "from io import open\n",
        "from torch import optim\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoGpo7oSHy6w"
      },
      "source": [
        " - Given the computational resources required of training a model, we will begin setup for GPU resource instead of CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OcZzMa1DFDqL"
      },
      "outputs": [],
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe5ICOtOIFhK"
      },
      "source": [
        " - Next we mount our Google Drive to access the files. For my notebook the file structure in the Google Drive is the Drive -> Folder named \"Movie_Corpus\" -> All necessary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m1it9u9fb-T4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eb2-yT5Nb8xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07ccacd-8fe7-4b36-9dfb-e0846d5e82ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8ewjcZIVk8"
      },
      "source": [
        " - For simplicity and cleaner code, I set a few paths from now that would be necessary in multiple functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8fjcZg-Tb6qg"
      },
      "outputs": [],
      "source": [
        "#Google Drive paths for read in/saving\n",
        "corpus_name = \"Movie_Corpus\"\n",
        "corpus = os.path.join(\"/content/drive/My Drive/\", corpus_name)\n",
        "save_dir = os.path.join(\"/content/drive/My Drive/Movie_Corpus\", \"save\")\n",
        "datafile = os.path.join(\"/content/drive/My Drive/Movie_Corpus\", \"datafile.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ge40QHIfLH"
      },
      "source": [
        " - Reading in the first file, utterances.jsonl saved in the Movie_Corpus folder, to see the data we are working with. It is important to note, on Kaggle all files are loaded in as .txt files before we change them into dataframes. However, there were issues of having nested lists, contiguous token errors, dictionary break downs, and even an illegal action error that appears for which I could not account. From the official website, I loaded in a second version of the data as .json or .jsonl files to more easily work with data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "ejFbfaxAxUvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ktpbcP71arYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701238b7-3832-4b92-af9a-075aa7938b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n"
          ]
        }
      ],
      "source": [
        "#Print first few lines of our utterance file\n",
        "def printLines(file, n=5):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfXTaG1JfcJ"
      },
      "source": [
        " - We can note that the metadata is structured in a dictionary. The two classes of data we ideally want are the lines that form conversations. Below we will parse the lines in the utterances file to extract the necessary data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B5o035Skas4f"
      },
      "outputs": [],
      "source": [
        "#Splits each line and fill into empty dictionaries\n",
        "def LinesAndConvos(fileName):\n",
        "    #Dictionaries to store individual lines and conversations\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    #Open with iso-encoding instead of UUT-8\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            #Extract fields for line object\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "            #Extract fields for conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                #Add line to existing convo\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "    return lines, conversations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyBWcsN5LbCg"
      },
      "source": [
        " - We then further process the information by converting the derived conversations into pairs of inputs and outputs (an input being a dialogue from a character in the corpus, and the output the response of another character to the input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eH4i-PeALX32"
      },
      "outputs": [],
      "source": [
        "#Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    #List to store question-answer pairs\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        #Iterate over all lines of convo\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):\n",
        "            #strip whitespace\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            #Next line becomes target\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            #Filter out samples without valid input/targets\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    #Now we can return extracted sentence pairs\n",
        "    return qa_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - From this step, we can initialize empty dictionaries, read in our reformatted utterences file, and have our idealized dataset to further clean and refine for our model."
      ],
      "metadata": {
        "id": "vApisirJyer1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9F7XSG3fat7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c65b95-d49c-4b9d-9b6c-9de0a931057e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus into lines and conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b'They do to!\\tThey do not!\\n'\n",
            "b'She okay?\\tI hope so.\\n'\n",
            "b\"Wow\\tLet's go.\\n\"\n",
            "b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n",
            "b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n"
          ]
        }
      ],
      "source": [
        "#Forgot to define above, path to newly created file\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "#Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "#Initialize lines dict and conversations dict\n",
        "lines = {}\n",
        "conversations = {}\n",
        "\n",
        "#Load lines and convos\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = LinesAndConvos(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "#Create and write a new csv file in utf-8 encoding\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "#Ensure it worked, let's see\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - This is where most of the preprocessing takes place. The Vocabulary class was created to manage the vocabulary of our chatbot and handle mapping between words, indices, match frequencies of word use in our dialogue, and filter out some terms to increase training speed.This is where most of the preprocessing takes place. Now tokenization is not explicitly called due to it's computational and time requirements, but it is mimicked. Many of the functions within the class such as addSentence() and normalizeString() account for basic tokenization by splitting individual words, converting terms to lowercase, removing accents and non-alphabetical characters, and adding spaces around punctuation. Further down we also call on indexesFromSentence to convert a sentence into indices using our dictionary."
      ],
      "metadata": {
        "id": "tT2s35HfyqpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wzhq-Qbqauya"
      },
      "outputs": [],
      "source": [
        "#Crucial tokens for vocabulary.\n",
        "#Padds short sentences\n",
        "PAD_token = 0\n",
        "#SOS token is Start-of-sentence\n",
        "SOS_token = 1\n",
        "#EOS is end-of-sentence\n",
        "EOS_token = 2\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        #Name of vocabulary\n",
        "        self.name = name\n",
        "        #Flag to check if trimming has occured (not yet)\n",
        "        self.trimmed = False\n",
        "        #Dictionaries to map words to index and their respective frequencies\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        #Set at 3 to account for 3 Pad,SOS, and Eos\n",
        "        self.num_words = 3\n",
        "\n",
        "    #Sentence input split into individual words and add to vocab\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    #Adds a word if it isn't already in the vocab list\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    #Trims out words below predefined threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        #Trimming is done\n",
        "        self.trimmed = True\n",
        "        #Store words meeting min_count\n",
        "        keep_words = []\n",
        "        #Keep words that occur more than that amount\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
        "\n",
        "        #Reinitialize dictionaries to keep those frequent words\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "        #Add only frequent words back to vocab\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Turn unicode to ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "SnhUOvP06gYx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated normalizeString function with NLTK tokenization\n",
        "def normalizeString(s):\n",
        "    # First converting to lowercase and removing accents\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    # Separate out punctuation\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # Remove non-alphabet letters\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    # Replace spaces with single space\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ],
      "metadata": {
        "id": "0mowIMN93LtM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Max length of sentence considered\n",
        "MAX_LENGTH = 15"
      ],
      "metadata": {
        "id": "_pPC8t8m45wV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ByMxEdFHSlJq"
      },
      "outputs": [],
      "source": [
        "#Read query/response pairs and return a voc object\n",
        "def readVocab(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    #Read and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    #Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    vocab = Vocabulary(corpus_name)\n",
        "    return vocab, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJcJTJIUYCdu"
      },
      "source": [
        " - Now that most of the preprocessing is done, we simply need to filter the pairs based on a set max length. This helps with computational processing and in the end with chatbot coherence and efficiency for it's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XkK1Z_bLSm-J"
      },
      "outputs": [],
      "source": [
        "#Check if sentence pairs are under max length\n",
        "def filterPair(p):\n",
        "    #Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_WRiLJpYRq2"
      },
      "source": [
        " - Based on that, we retain only those sentence pairs under our maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VRibBaqUSoE0"
      },
      "outputs": [],
      "source": [
        "#Filter pairs using ``filterPair`` condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOef9s1dYj1b"
      },
      "source": [
        " - We have a lot of moving parts operating at this point. So we will utilize LoadAndPrepare function to narrow things down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HqHHhnrhSpqa"
      },
      "outputs": [],
      "source": [
        "#Using everything, return a populated voc object and pairs list\n",
        "def loadAndPrepare(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    #Read and normalize the sentence pairs\n",
        "    vocab, pairs = readVocab(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    #Filter pairs based on the length\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    #Counting words, building vocab\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        #Add query sentence to vocab\n",
        "        vocab.addSentence(pair[0])\n",
        "        #Add response sentence to vocab\n",
        "        vocab.addSentence(pair[1])\n",
        "    print(\"Counted words:\", vocab.num_words)\n",
        "    return vocab, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBCvr2ZBZSdO"
      },
      "source": [
        " - Lastly we need to save our vocabulary. We set a directory in our Google Drive for it,  we can validate the data loading and preparation are performed correctly, showing a few samples to visually confirm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0KEiBHgSrCW",
        "outputId": "b318bf1c-9a18-42ed-d1a7-2ac12aae0127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 111392 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 26989\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['no', 'okay you re gonna need to learn how to lie .']\n",
            "['i figured you d get to the good stuff eventually .', 'what good stuff ?']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n"
          ]
        }
      ],
      "source": [
        "#Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "vocab, pairs = loadAndPrepare(corpus, corpus_name, datafile, save_dir)\n",
        "#Let's see a few\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9o77Ui0Zx_t"
      },
      "source": [
        " - On the other end of the spectrum, we are going to further refine our data by trimming words that do not occur often. This way we can reduce the complexity of the model and it's response while also increaseing it's coherence in chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGvqpDy0azDs",
        "outputId": "bc11544a-5880-45e3-ae2b-66d9a3e254ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 18532 / 26986 = 0.6867\n",
            "Trimmed from 111392 pairs to 104099, 0.9345 of total\n",
            "Trimmed from 104099 pairs to 104099, 1.0000 of total\n"
          ]
        }
      ],
      "source": [
        "#minimum frequency for trimming\n",
        "MIN_COUNT = 2\n",
        "\n",
        "def trimRareWords(vocab, pairs, MIN_COUNT):\n",
        "    #Trim words under threshold from vocabulary\n",
        "    vocab.trim(MIN_COUNT)\n",
        "    #Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        #Check input seqeunce.\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in vocab.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        #Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in vocab.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "        #Only keep pairs without trimmed words in input/output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "#Trim voc and pairs\n",
        "pairs = trimRareWords(vocab, pairs, MIN_COUNT)\n",
        "\n",
        "#Trim voc and pairs\n",
        "pairs = trimRareWords(vocab, pairs, MIN_COUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - As seen above, we still maintain most of our vocabulary, so it is a solid optimization step with little downsides. We then move forward and convert the sentences we've kept into indices representing each word. The key factor here is including our End-Of-Sequence token at the end for our future input. This step is necessary overall to convert the data into a type for the model to process."
      ],
      "metadata": {
        "id": "8yi-aGaHG74a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update function to convert sentence to indices\n",
        "def indexesFromSentence(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
      ],
      "metadata": {
        "id": "WKRBH0Pa3Wux"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGOmxFdw9wIr"
      },
      "source": [
        " - We then pad the sequences to ensure all data input into the model is of the same length. Without this step, the model would crash due to unexpected InputErrors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BhW0r3Va2l79"
      },
      "outputs": [],
      "source": [
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-CQDSja9-nu"
      },
      "source": [
        " - We also set up a binary matrix to create a binary mask for the padded sequences to help identify which instances were valid tokens (indicated with a 1) and which required padding (indicated with a 0). With this method we can ignore padded tokens during our loss calculations, which maintains the purity of our evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zyZKUe6e2nve"
      },
      "outputs": [],
      "source": [
        "def binaryMatrix(l, value=PAD_token):\n",
        "    #Set empty dictionary\n",
        "    m = []\n",
        "    #Loop over each sequence and append for non-padded tokens\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdEBCSeT-KWu"
      },
      "source": [
        " - We then have two functions to handle input and outputs. InputVar returns the tensor and lengths of the original sentences before padding. OutputVar prepares the target sentences for processing by converting them to padded tensors, initializing teh binarymask, and returning the maximum target length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GddOvcyB2pgG"
      },
      "outputs": [],
      "source": [
        "#Convert batch of input sentences\n",
        "def inputVar(l, vocab):\n",
        "    #Convert each sentence into list of indices\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    #Create tensor containing og lengths of each sentence before padding\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    #pad sentences to equal lengths\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    #Convert to tensor in pytorch\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ww-6v4Hf2r1H"
      },
      "outputs": [],
      "source": [
        "def outputVar(l, vocab):\n",
        "    #Convert batch of target sentences to indices\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    #Find length of longest sentence in batch as reference padding\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    #Pad all sentences to that length\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    #Create binary mask where valid and padded tokens are marked\n",
        "    mask = binaryMatrix(padList)\n",
        "    #Convert binary mask into tensor\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    #Convert padded sentences into tensor\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrRS4v3_DBm"
      },
      "source": [
        " - Lastly we convert the batches of input/output sentence pairs to feed into our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated batch2TrainData function to use NLTK tokenization\n",
        "def batch2TrainData(vocab, pair_batch):\n",
        "    # Sort batch of sentence pairs by length\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    # Separate input/output into two separate lists\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, vocab)\n",
        "    output, mask, max_target_len = outputVar(output_batch, vocab)\n",
        "    return inp, lengths, output, mask, max_target_len\n"
      ],
      "metadata": {
        "id": "tZfN8RUh3bn_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0S63Scpa0J9",
        "outputId": "ef9734e5-2ac7-48a5-cdeb-e3df9591a8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[  20,   34,   44,   46,  105],\n",
            "        [  29,    4,  361,   17,   17],\n",
            "        [   5,   20,   20,   99, 1147],\n",
            "        [  65,   29,  492,   14,   14],\n",
            "        [1684,  112,  105,    2,    2],\n",
            "        [ 490,   20,   46,    0,    0],\n",
            "        [ 281, 2704,  350,    0,    0],\n",
            "        [  44,   10,   14,    0,    0],\n",
            "        [  57,    2,    2,    0,    0],\n",
            "        [  82,    0,    0,    0,    0],\n",
            "        [  14,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([12,  9,  9,  5,  5])\n",
            "target_variable: tensor([[  234,   514,   351,    11,    11],\n",
            "        [   20,   551,    14,  1108,   131],\n",
            "        [13413,    14,    18,     5,   120],\n",
            "        [  195,    37,    14,    95,   539],\n",
            "        [   10,   332,     2,  1845,    14],\n",
            "        [    2,    84,     0,    50,     2],\n",
            "        [    0,   140,     0,   206,     0],\n",
            "        [    0,   198,     0,    11,     0],\n",
            "        [    0,   105,     0,   183,     0],\n",
            "        [    0,     5,     0,   140,     0],\n",
            "        [    0,   673,     0,   184,     0],\n",
            "        [    0,   812,     0,    14,     0],\n",
            "        [    0,    14,     0,     2,     0],\n",
            "        [    0,     2,     0,     0,     0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False, False, False]])\n",
            "max_target_len: 14\n"
          ]
        }
      ],
      "source": [
        "#Test out and see\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - It is important to note that although we did not use explicit tokenization methods from NLTK/SpaCy, we still result with appropriate tensors."
      ],
      "metadata": {
        "id": "cVo0Gf8DI5HW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling"
      ],
      "metadata": {
        "id": "Ppg8wihAJAcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The model used here was a Sequence-to-Sequence model with specialized Luong attention mechanisms and bidirectional gated-recurrent-units (for input, unidirectional in output). It's similar to RNN models (hence the function name) and is composed of two essential portions: encoder and decoder. It operates by feeding an input sentence into the encoder which transforms it into a hidden representation of itself for which the decoder generates an output (our chatbot response) from decoding it.\n",
        "\n",
        " - The encoder itself is, as mentioned above, a bidirectional GRU, used due to the effectiveness of capturing contextual information and long-range dependencies from both forward and backwards processing. The resulting concatenated hidden state is passed to the decoder for text generation."
      ],
      "metadata": {
        "id": "n4Ahr-_cJB4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yYnqS8qNa1Gy"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        '''Clarification in initializing GRU - the input_size and hidden_size parameters are both set to 'hidden_size'\n",
        "        #because our input size is a word embedding with number of features == hidden_size'''\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        #Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        #Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        #Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        #Unpacking padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        #Sum together the bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        #Return output and final hidden state\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yzpYBK1BB5L"
      },
      "source": [
        " - The real gem of this implementation is the Luong attention mechanism. It helps with potentially long inputs from the user which was a proven issue in previous attempts of this final project, and also helps against vanishing gradients to aid with context. There is also an added flexibility in training by means of the scoring options which we will cover below (dot, general, concatenation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5yWObw9Da3ik"
      },
      "outputs": [],
      "source": [
        "#Luong attention layer from paper (need to add above)\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        #Of the 3 variants of attention mechanism to use between dot, general, and concat\n",
        "        #We choose concat\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            #We also apply linear transformation to align dimensions of hidden state and encoder outputs for attention calc\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    #Attention score calculation (dot product) of hidden state of decoder and encoder output\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    #Same thing but energy tensor created from applying linear transformation to encoder output\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    #Same thing as dot score too, but concatenates decoders hidden state and encoder output before passing through linear transformation\n",
        "    #And then non-linear activation function (favorite=tanh)\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        #Calculate the attention weights (energies) based on the given method chosen, all 3 for exploration\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "        #Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "        #Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8CTQBZIIa57_"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        #Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        #Embedding layer will map word indices to dense vectors\n",
        "        self.embedding = embedding\n",
        "        #Dropout for stabilization via regularization/overfitting\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        #Unidirectional GRU\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        #Linear layer to convert final hidden state into output space for each word\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        #Convert finall hidden state to output\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        #calculate attention score\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    #Forward pass for decoder to generate\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        #Important Note: run this one step (word) at a time\n",
        "        #Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        #Forward through GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        #Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        #Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        #Concatenate weighted context vector and GRU output using Luong equation 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        #Predict next word using Luong equation 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # eturn output and final hidden state\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The Negative Log-Likelihood calculation of loss for the model is calculated through the below function. It helps in calculation with various sizes of input and output sequences."
      ],
      "metadata": {
        "id": "UIPYJkB4LGIT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2kHsYCnBa7Vn"
      },
      "outputs": [],
      "source": [
        "#Masked negative log-likelihood\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    #Count total number of non-padded elements\n",
        "    nTotal = mask.sum()\n",
        "    #Get predicted values from input corresponding to correct target values\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    #Average loss for current batch\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KRiFUt3Ha8dP"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "    #Reset gradients to zero\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    #Set everything to GPU\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    #Lengths for RNN packing should always be on the CPU because of tensorflow library\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    #Initialize variables and dictionary to store\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    #Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    #Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    #Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    #Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    #Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            #Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            #Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            #No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            #Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    #Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    #Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "    #Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PR6THkSAOETu"
      },
      "outputs": [],
      "source": [
        "def save_model(encoder, decoder, model_name):\n",
        "    model_path = f\"/content/drive/My Drive/Movie_Corpus/{model_name}.pth\"\n",
        "    torch.save({\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "    }, model_path)\n",
        "    print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uKT7Ivc7a98e"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n",
        "               encoder_n_layers, decoder_n_layers, n_iteration, batch_size, print_every, clip):\n",
        "    #Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                        for _ in range(n_iteration)]\n",
        "\n",
        "    #Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    #Loop through trianing\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        #Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "        #Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        #Progress:\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n",
        "            print_loss = 0\n",
        "    #Save the model out of loop so only final iteration of training is kept\n",
        "    save_model(encoder, decoder, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Greedy Search Decoding is used to generate the sequence of output tokens from our model. It was chosen for computational efficiency as with many decisions throughout this project, and simply selects the highest probability token at each step of decoding as the response."
      ],
      "metadata": {
        "id": "iIWVyrsBLdye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oFzAb55zbAqB"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        #Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        #Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        #Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        #Initialize tensors to append decoded words\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            #Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            #Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            #Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            #Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        #Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rsZnThULbCYh"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, vocab, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    #words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence)]\n",
        "    #Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    #Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    #Set to GPU and tensors to CPU\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    #Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    #indexes to words\n",
        "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DATkkRIsWtro"
      },
      "outputs": [],
      "source": [
        "def evaluateInput(encoder, decoder, searcher, vocab):\n",
        "    input_sentence = ''\n",
        "    while True:\n",
        "        try:\n",
        "            #Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            #Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            #Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            #Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, vocab, input_sentence)\n",
        "            # ormat and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Our model configurations are listed below. One of the incredible features to a Luong attention mechanism is accounted for here via attention score calculations. There are 3 types: dot, general, and concat. In testing, dot has proven to be the most effective considering the simplicity of our model. The operate as follows:\n",
        "\n",
        "  - Dot Product: Calculates the attention score as the dot product between the decoder's hidden state and the encoder's output. Computationally inexpensive, easy to use.\n",
        "\n",
        "  - General: Similar to dot product, but applies a learnable linear transformation to the encoder's output before taking the dot product. More flexible and complex.\n",
        "\n",
        "  - Concat: It concatenates the decoder hidden state with the encoder output and passes them through a feed-forward network with a non-linear activation (tanh or sigmoid). It is the most complex and intensive."
      ],
      "metadata": {
        "id": "7_o4vfXbLvSm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "X7JMXu9bbDim"
      },
      "outputs": [],
      "source": [
        "#Configure models\n",
        "model_name = 'AAI_520'\n",
        "attn_model = 'dot'\n",
        "#``attn_model = 'general'``\n",
        "#``attn_model = 'concat'``\n",
        "hidden_size = 750\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "n_iteration = 33000\n",
        "print_every = 1\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - After setting our model's configuration, we initialize everything and build our model."
      ],
      "metadata": {
        "id": "T1Sbph3_NW-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Z9GeiIh1bFmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9478a34-a494-4d12-b05a-6a7b173db3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models initialized from scratch and ready to go!\n"
          ]
        }
      ],
      "source": [
        "print('Building encoder and decoder ...')\n",
        "#Initialize word embeddings\n",
        "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
        "\n",
        "#Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "#Move models to GPU\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models initialized from scratch and ready to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Lastly, we start training our model below on GPU, in this case A100 runtime in Google Colab. Training can range between 10 minutes to 10 hours depending on configuration."
      ],
      "metadata": {
        "id": "qsEjElbUNV-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "lfWIfvxXbG4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad39f54-8c8e-4952-f75b-b96801f1eeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 28002; Percent complete: 84.9%; Average loss: 0.8810\n",
            "Iteration: 28003; Percent complete: 84.9%; Average loss: 0.8670\n",
            "Iteration: 28004; Percent complete: 84.9%; Average loss: 0.9069\n",
            "Iteration: 28005; Percent complete: 84.9%; Average loss: 0.6201\n",
            "Iteration: 28006; Percent complete: 84.9%; Average loss: 0.6413\n",
            "Iteration: 28007; Percent complete: 84.9%; Average loss: 0.7754\n",
            "Iteration: 28008; Percent complete: 84.9%; Average loss: 0.7967\n",
            "Iteration: 28009; Percent complete: 84.9%; Average loss: 0.9296\n",
            "Iteration: 28010; Percent complete: 84.9%; Average loss: 0.9124\n",
            "Iteration: 28011; Percent complete: 84.9%; Average loss: 0.7750\n",
            "Iteration: 28012; Percent complete: 84.9%; Average loss: 0.6362\n",
            "Iteration: 28013; Percent complete: 84.9%; Average loss: 0.8366\n",
            "Iteration: 28014; Percent complete: 84.9%; Average loss: 0.8957\n",
            "Iteration: 28015; Percent complete: 84.9%; Average loss: 0.9892\n",
            "Iteration: 28016; Percent complete: 84.9%; Average loss: 0.8585\n",
            "Iteration: 28017; Percent complete: 84.9%; Average loss: 0.8653\n",
            "Iteration: 28018; Percent complete: 84.9%; Average loss: 0.7896\n",
            "Iteration: 28019; Percent complete: 84.9%; Average loss: 0.7400\n",
            "Iteration: 28020; Percent complete: 84.9%; Average loss: 0.6548\n",
            "Iteration: 28021; Percent complete: 84.9%; Average loss: 0.7138\n",
            "Iteration: 28022; Percent complete: 84.9%; Average loss: 0.8071\n",
            "Iteration: 28023; Percent complete: 84.9%; Average loss: 0.8622\n",
            "Iteration: 28024; Percent complete: 84.9%; Average loss: 0.8386\n",
            "Iteration: 28025; Percent complete: 84.9%; Average loss: 0.6424\n",
            "Iteration: 28026; Percent complete: 84.9%; Average loss: 0.7547\n",
            "Iteration: 28027; Percent complete: 84.9%; Average loss: 0.6831\n",
            "Iteration: 28028; Percent complete: 84.9%; Average loss: 0.7910\n",
            "Iteration: 28029; Percent complete: 84.9%; Average loss: 0.8189\n",
            "Iteration: 28030; Percent complete: 84.9%; Average loss: 0.8094\n",
            "Iteration: 28031; Percent complete: 84.9%; Average loss: 0.6594\n",
            "Iteration: 28032; Percent complete: 84.9%; Average loss: 0.7196\n",
            "Iteration: 28033; Percent complete: 84.9%; Average loss: 0.9397\n",
            "Iteration: 28034; Percent complete: 85.0%; Average loss: 0.7584\n",
            "Iteration: 28035; Percent complete: 85.0%; Average loss: 0.7234\n",
            "Iteration: 28036; Percent complete: 85.0%; Average loss: 0.8202\n",
            "Iteration: 28037; Percent complete: 85.0%; Average loss: 0.7348\n",
            "Iteration: 28038; Percent complete: 85.0%; Average loss: 0.7432\n",
            "Iteration: 28039; Percent complete: 85.0%; Average loss: 0.7464\n",
            "Iteration: 28040; Percent complete: 85.0%; Average loss: 0.7875\n",
            "Iteration: 28041; Percent complete: 85.0%; Average loss: 0.5881\n",
            "Iteration: 28042; Percent complete: 85.0%; Average loss: 0.9000\n",
            "Iteration: 28043; Percent complete: 85.0%; Average loss: 0.7531\n",
            "Iteration: 28044; Percent complete: 85.0%; Average loss: 0.8389\n",
            "Iteration: 28045; Percent complete: 85.0%; Average loss: 0.8841\n",
            "Iteration: 28046; Percent complete: 85.0%; Average loss: 0.6835\n",
            "Iteration: 28047; Percent complete: 85.0%; Average loss: 0.7336\n",
            "Iteration: 28048; Percent complete: 85.0%; Average loss: 0.7667\n",
            "Iteration: 28049; Percent complete: 85.0%; Average loss: 0.8312\n",
            "Iteration: 28050; Percent complete: 85.0%; Average loss: 0.8021\n",
            "Iteration: 28051; Percent complete: 85.0%; Average loss: 0.8487\n",
            "Iteration: 28052; Percent complete: 85.0%; Average loss: 1.0641\n",
            "Iteration: 28053; Percent complete: 85.0%; Average loss: 0.7998\n",
            "Iteration: 28054; Percent complete: 85.0%; Average loss: 0.8019\n",
            "Iteration: 28055; Percent complete: 85.0%; Average loss: 0.7851\n",
            "Iteration: 28056; Percent complete: 85.0%; Average loss: 0.7410\n",
            "Iteration: 28057; Percent complete: 85.0%; Average loss: 0.8355\n",
            "Iteration: 28058; Percent complete: 85.0%; Average loss: 0.7609\n",
            "Iteration: 28059; Percent complete: 85.0%; Average loss: 0.8525\n",
            "Iteration: 28060; Percent complete: 85.0%; Average loss: 0.7751\n",
            "Iteration: 28061; Percent complete: 85.0%; Average loss: 0.7812\n",
            "Iteration: 28062; Percent complete: 85.0%; Average loss: 0.6411\n",
            "Iteration: 28063; Percent complete: 85.0%; Average loss: 0.7780\n",
            "Iteration: 28064; Percent complete: 85.0%; Average loss: 0.8104\n",
            "Iteration: 28065; Percent complete: 85.0%; Average loss: 0.8322\n",
            "Iteration: 28066; Percent complete: 85.0%; Average loss: 0.8839\n",
            "Iteration: 28067; Percent complete: 85.1%; Average loss: 0.6694\n",
            "Iteration: 28068; Percent complete: 85.1%; Average loss: 0.6197\n",
            "Iteration: 28069; Percent complete: 85.1%; Average loss: 0.7181\n",
            "Iteration: 28070; Percent complete: 85.1%; Average loss: 0.8011\n",
            "Iteration: 28071; Percent complete: 85.1%; Average loss: 0.9309\n",
            "Iteration: 28072; Percent complete: 85.1%; Average loss: 0.7172\n",
            "Iteration: 28073; Percent complete: 85.1%; Average loss: 0.7072\n",
            "Iteration: 28074; Percent complete: 85.1%; Average loss: 0.7562\n",
            "Iteration: 28075; Percent complete: 85.1%; Average loss: 0.6832\n",
            "Iteration: 28076; Percent complete: 85.1%; Average loss: 0.6843\n",
            "Iteration: 28077; Percent complete: 85.1%; Average loss: 0.9131\n",
            "Iteration: 28078; Percent complete: 85.1%; Average loss: 0.8389\n",
            "Iteration: 28079; Percent complete: 85.1%; Average loss: 0.6739\n",
            "Iteration: 28080; Percent complete: 85.1%; Average loss: 0.8195\n",
            "Iteration: 28081; Percent complete: 85.1%; Average loss: 0.7123\n",
            "Iteration: 28082; Percent complete: 85.1%; Average loss: 0.7461\n",
            "Iteration: 28083; Percent complete: 85.1%; Average loss: 0.8375\n",
            "Iteration: 28084; Percent complete: 85.1%; Average loss: 0.8379\n",
            "Iteration: 28085; Percent complete: 85.1%; Average loss: 0.8994\n",
            "Iteration: 28086; Percent complete: 85.1%; Average loss: 0.7372\n",
            "Iteration: 28087; Percent complete: 85.1%; Average loss: 1.0837\n",
            "Iteration: 28088; Percent complete: 85.1%; Average loss: 0.8724\n",
            "Iteration: 28089; Percent complete: 85.1%; Average loss: 1.0927\n",
            "Iteration: 28090; Percent complete: 85.1%; Average loss: 0.6507\n",
            "Iteration: 28091; Percent complete: 85.1%; Average loss: 0.8728\n",
            "Iteration: 28092; Percent complete: 85.1%; Average loss: 0.7685\n",
            "Iteration: 28093; Percent complete: 85.1%; Average loss: 0.7661\n",
            "Iteration: 28094; Percent complete: 85.1%; Average loss: 0.9142\n",
            "Iteration: 28095; Percent complete: 85.1%; Average loss: 0.7223\n",
            "Iteration: 28096; Percent complete: 85.1%; Average loss: 0.8419\n",
            "Iteration: 28097; Percent complete: 85.1%; Average loss: 0.7519\n",
            "Iteration: 28098; Percent complete: 85.1%; Average loss: 0.7196\n",
            "Iteration: 28099; Percent complete: 85.1%; Average loss: 0.7238\n",
            "Iteration: 28100; Percent complete: 85.2%; Average loss: 0.7478\n",
            "Iteration: 28101; Percent complete: 85.2%; Average loss: 0.7978\n",
            "Iteration: 28102; Percent complete: 85.2%; Average loss: 0.7070\n",
            "Iteration: 28103; Percent complete: 85.2%; Average loss: 0.7377\n",
            "Iteration: 28104; Percent complete: 85.2%; Average loss: 0.8741\n",
            "Iteration: 28105; Percent complete: 85.2%; Average loss: 0.6913\n",
            "Iteration: 28106; Percent complete: 85.2%; Average loss: 0.7010\n",
            "Iteration: 28107; Percent complete: 85.2%; Average loss: 0.7546\n",
            "Iteration: 28108; Percent complete: 85.2%; Average loss: 0.8187\n",
            "Iteration: 28109; Percent complete: 85.2%; Average loss: 0.7906\n",
            "Iteration: 28110; Percent complete: 85.2%; Average loss: 0.9085\n",
            "Iteration: 28111; Percent complete: 85.2%; Average loss: 0.8636\n",
            "Iteration: 28112; Percent complete: 85.2%; Average loss: 0.7231\n",
            "Iteration: 28113; Percent complete: 85.2%; Average loss: 0.9220\n",
            "Iteration: 28114; Percent complete: 85.2%; Average loss: 0.8169\n",
            "Iteration: 28115; Percent complete: 85.2%; Average loss: 0.8308\n",
            "Iteration: 28116; Percent complete: 85.2%; Average loss: 0.6956\n",
            "Iteration: 28117; Percent complete: 85.2%; Average loss: 0.8677\n",
            "Iteration: 28118; Percent complete: 85.2%; Average loss: 0.7673\n",
            "Iteration: 28119; Percent complete: 85.2%; Average loss: 0.7230\n",
            "Iteration: 28120; Percent complete: 85.2%; Average loss: 0.7269\n",
            "Iteration: 28121; Percent complete: 85.2%; Average loss: 0.8051\n",
            "Iteration: 28122; Percent complete: 85.2%; Average loss: 0.7702\n",
            "Iteration: 28123; Percent complete: 85.2%; Average loss: 0.8240\n",
            "Iteration: 28124; Percent complete: 85.2%; Average loss: 0.8271\n",
            "Iteration: 28125; Percent complete: 85.2%; Average loss: 0.6058\n",
            "Iteration: 28126; Percent complete: 85.2%; Average loss: 0.8170\n",
            "Iteration: 28127; Percent complete: 85.2%; Average loss: 0.6774\n",
            "Iteration: 28128; Percent complete: 85.2%; Average loss: 0.6979\n",
            "Iteration: 28129; Percent complete: 85.2%; Average loss: 0.6433\n",
            "Iteration: 28130; Percent complete: 85.2%; Average loss: 0.8254\n",
            "Iteration: 28131; Percent complete: 85.2%; Average loss: 0.8980\n",
            "Iteration: 28132; Percent complete: 85.2%; Average loss: 0.7270\n",
            "Iteration: 28133; Percent complete: 85.3%; Average loss: 0.7852\n",
            "Iteration: 28134; Percent complete: 85.3%; Average loss: 0.7112\n",
            "Iteration: 28135; Percent complete: 85.3%; Average loss: 0.7492\n",
            "Iteration: 28136; Percent complete: 85.3%; Average loss: 0.7804\n",
            "Iteration: 28137; Percent complete: 85.3%; Average loss: 0.6378\n",
            "Iteration: 28138; Percent complete: 85.3%; Average loss: 0.7980\n",
            "Iteration: 28139; Percent complete: 85.3%; Average loss: 0.7490\n",
            "Iteration: 28140; Percent complete: 85.3%; Average loss: 0.8179\n",
            "Iteration: 28141; Percent complete: 85.3%; Average loss: 0.9591\n",
            "Iteration: 28142; Percent complete: 85.3%; Average loss: 0.7327\n",
            "Iteration: 28143; Percent complete: 85.3%; Average loss: 0.7599\n",
            "Iteration: 28144; Percent complete: 85.3%; Average loss: 0.8583\n",
            "Iteration: 28145; Percent complete: 85.3%; Average loss: 0.6988\n",
            "Iteration: 28146; Percent complete: 85.3%; Average loss: 0.8081\n",
            "Iteration: 28147; Percent complete: 85.3%; Average loss: 0.8977\n",
            "Iteration: 28148; Percent complete: 85.3%; Average loss: 0.6552\n",
            "Iteration: 28149; Percent complete: 85.3%; Average loss: 0.6746\n",
            "Iteration: 28150; Percent complete: 85.3%; Average loss: 0.8211\n",
            "Iteration: 28151; Percent complete: 85.3%; Average loss: 0.8390\n",
            "Iteration: 28152; Percent complete: 85.3%; Average loss: 0.7273\n",
            "Iteration: 28153; Percent complete: 85.3%; Average loss: 0.8028\n",
            "Iteration: 28154; Percent complete: 85.3%; Average loss: 0.9281\n",
            "Iteration: 28155; Percent complete: 85.3%; Average loss: 0.9049\n",
            "Iteration: 28156; Percent complete: 85.3%; Average loss: 0.8402\n",
            "Iteration: 28157; Percent complete: 85.3%; Average loss: 0.7388\n",
            "Iteration: 28158; Percent complete: 85.3%; Average loss: 0.9851\n",
            "Iteration: 28159; Percent complete: 85.3%; Average loss: 0.7750\n",
            "Iteration: 28160; Percent complete: 85.3%; Average loss: 0.8498\n",
            "Iteration: 28161; Percent complete: 85.3%; Average loss: 0.8150\n",
            "Iteration: 28162; Percent complete: 85.3%; Average loss: 0.8049\n",
            "Iteration: 28163; Percent complete: 85.3%; Average loss: 0.9143\n",
            "Iteration: 28164; Percent complete: 85.3%; Average loss: 0.7400\n",
            "Iteration: 28165; Percent complete: 85.3%; Average loss: 0.8821\n",
            "Iteration: 28166; Percent complete: 85.4%; Average loss: 1.0117\n",
            "Iteration: 28167; Percent complete: 85.4%; Average loss: 1.0005\n",
            "Iteration: 28168; Percent complete: 85.4%; Average loss: 0.7218\n",
            "Iteration: 28169; Percent complete: 85.4%; Average loss: 0.9614\n",
            "Iteration: 28170; Percent complete: 85.4%; Average loss: 1.0307\n",
            "Iteration: 28171; Percent complete: 85.4%; Average loss: 0.7980\n",
            "Iteration: 28172; Percent complete: 85.4%; Average loss: 0.6535\n",
            "Iteration: 28173; Percent complete: 85.4%; Average loss: 0.5954\n",
            "Iteration: 28174; Percent complete: 85.4%; Average loss: 0.8089\n",
            "Iteration: 28175; Percent complete: 85.4%; Average loss: 0.6077\n",
            "Iteration: 28176; Percent complete: 85.4%; Average loss: 0.8292\n",
            "Iteration: 28177; Percent complete: 85.4%; Average loss: 0.9408\n",
            "Iteration: 28178; Percent complete: 85.4%; Average loss: 0.7321\n",
            "Iteration: 28179; Percent complete: 85.4%; Average loss: 0.7309\n",
            "Iteration: 28180; Percent complete: 85.4%; Average loss: 0.8161\n",
            "Iteration: 28181; Percent complete: 85.4%; Average loss: 0.7927\n",
            "Iteration: 28182; Percent complete: 85.4%; Average loss: 0.7185\n",
            "Iteration: 28183; Percent complete: 85.4%; Average loss: 0.7234\n",
            "Iteration: 28184; Percent complete: 85.4%; Average loss: 0.6590\n",
            "Iteration: 28185; Percent complete: 85.4%; Average loss: 0.6712\n",
            "Iteration: 28186; Percent complete: 85.4%; Average loss: 0.7817\n",
            "Iteration: 28187; Percent complete: 85.4%; Average loss: 0.7688\n",
            "Iteration: 28188; Percent complete: 85.4%; Average loss: 0.9233\n",
            "Iteration: 28189; Percent complete: 85.4%; Average loss: 0.7974\n",
            "Iteration: 28190; Percent complete: 85.4%; Average loss: 0.6993\n",
            "Iteration: 28191; Percent complete: 85.4%; Average loss: 0.8700\n",
            "Iteration: 28192; Percent complete: 85.4%; Average loss: 0.6315\n",
            "Iteration: 28193; Percent complete: 85.4%; Average loss: 0.6897\n",
            "Iteration: 28194; Percent complete: 85.4%; Average loss: 0.7792\n",
            "Iteration: 28195; Percent complete: 85.4%; Average loss: 0.8068\n",
            "Iteration: 28196; Percent complete: 85.4%; Average loss: 0.8012\n",
            "Iteration: 28197; Percent complete: 85.4%; Average loss: 0.8006\n",
            "Iteration: 28198; Percent complete: 85.4%; Average loss: 0.6933\n",
            "Iteration: 28199; Percent complete: 85.5%; Average loss: 0.7642\n",
            "Iteration: 28200; Percent complete: 85.5%; Average loss: 0.7154\n",
            "Iteration: 28201; Percent complete: 85.5%; Average loss: 0.8539\n",
            "Iteration: 28202; Percent complete: 85.5%; Average loss: 0.7313\n",
            "Iteration: 28203; Percent complete: 85.5%; Average loss: 0.7972\n",
            "Iteration: 28204; Percent complete: 85.5%; Average loss: 0.7115\n",
            "Iteration: 28205; Percent complete: 85.5%; Average loss: 0.7598\n",
            "Iteration: 28206; Percent complete: 85.5%; Average loss: 0.7544\n",
            "Iteration: 28207; Percent complete: 85.5%; Average loss: 0.8617\n",
            "Iteration: 28208; Percent complete: 85.5%; Average loss: 0.7392\n",
            "Iteration: 28209; Percent complete: 85.5%; Average loss: 0.6965\n",
            "Iteration: 28210; Percent complete: 85.5%; Average loss: 0.6920\n",
            "Iteration: 28211; Percent complete: 85.5%; Average loss: 0.7338\n",
            "Iteration: 28212; Percent complete: 85.5%; Average loss: 0.7662\n",
            "Iteration: 28213; Percent complete: 85.5%; Average loss: 0.6710\n",
            "Iteration: 28214; Percent complete: 85.5%; Average loss: 0.7058\n",
            "Iteration: 28215; Percent complete: 85.5%; Average loss: 0.6873\n",
            "Iteration: 28216; Percent complete: 85.5%; Average loss: 0.7727\n",
            "Iteration: 28217; Percent complete: 85.5%; Average loss: 0.8873\n",
            "Iteration: 28218; Percent complete: 85.5%; Average loss: 0.7753\n",
            "Iteration: 28219; Percent complete: 85.5%; Average loss: 0.8058\n",
            "Iteration: 28220; Percent complete: 85.5%; Average loss: 1.1007\n",
            "Iteration: 28221; Percent complete: 85.5%; Average loss: 0.7170\n",
            "Iteration: 28222; Percent complete: 85.5%; Average loss: 0.8420\n",
            "Iteration: 28223; Percent complete: 85.5%; Average loss: 0.9592\n",
            "Iteration: 28224; Percent complete: 85.5%; Average loss: 0.6840\n",
            "Iteration: 28225; Percent complete: 85.5%; Average loss: 0.8981\n",
            "Iteration: 28226; Percent complete: 85.5%; Average loss: 0.7905\n",
            "Iteration: 28227; Percent complete: 85.5%; Average loss: 0.6579\n",
            "Iteration: 28228; Percent complete: 85.5%; Average loss: 0.8205\n",
            "Iteration: 28229; Percent complete: 85.5%; Average loss: 0.7635\n",
            "Iteration: 28230; Percent complete: 85.5%; Average loss: 0.7745\n",
            "Iteration: 28231; Percent complete: 85.5%; Average loss: 0.8437\n",
            "Iteration: 28232; Percent complete: 85.6%; Average loss: 0.8919\n",
            "Iteration: 28233; Percent complete: 85.6%; Average loss: 0.6274\n",
            "Iteration: 28234; Percent complete: 85.6%; Average loss: 0.7838\n",
            "Iteration: 28235; Percent complete: 85.6%; Average loss: 0.6782\n",
            "Iteration: 28236; Percent complete: 85.6%; Average loss: 0.8029\n",
            "Iteration: 28237; Percent complete: 85.6%; Average loss: 0.7580\n",
            "Iteration: 28238; Percent complete: 85.6%; Average loss: 0.6159\n",
            "Iteration: 28239; Percent complete: 85.6%; Average loss: 0.7071\n",
            "Iteration: 28240; Percent complete: 85.6%; Average loss: 0.8708\n",
            "Iteration: 28241; Percent complete: 85.6%; Average loss: 0.8734\n",
            "Iteration: 28242; Percent complete: 85.6%; Average loss: 0.7774\n",
            "Iteration: 28243; Percent complete: 85.6%; Average loss: 0.9804\n",
            "Iteration: 28244; Percent complete: 85.6%; Average loss: 0.8704\n",
            "Iteration: 28245; Percent complete: 85.6%; Average loss: 0.8574\n",
            "Iteration: 28246; Percent complete: 85.6%; Average loss: 0.8150\n",
            "Iteration: 28247; Percent complete: 85.6%; Average loss: 0.8847\n",
            "Iteration: 28248; Percent complete: 85.6%; Average loss: 0.6322\n",
            "Iteration: 28249; Percent complete: 85.6%; Average loss: 0.8866\n",
            "Iteration: 28250; Percent complete: 85.6%; Average loss: 0.6872\n",
            "Iteration: 28251; Percent complete: 85.6%; Average loss: 0.8252\n",
            "Iteration: 28252; Percent complete: 85.6%; Average loss: 0.9239\n",
            "Iteration: 28253; Percent complete: 85.6%; Average loss: 0.6373\n",
            "Iteration: 28254; Percent complete: 85.6%; Average loss: 0.6477\n",
            "Iteration: 28255; Percent complete: 85.6%; Average loss: 0.6639\n",
            "Iteration: 28256; Percent complete: 85.6%; Average loss: 0.9355\n",
            "Iteration: 28257; Percent complete: 85.6%; Average loss: 0.7254\n",
            "Iteration: 28258; Percent complete: 85.6%; Average loss: 1.0024\n",
            "Iteration: 28259; Percent complete: 85.6%; Average loss: 0.8206\n",
            "Iteration: 28260; Percent complete: 85.6%; Average loss: 0.7514\n",
            "Iteration: 28261; Percent complete: 85.6%; Average loss: 0.7247\n",
            "Iteration: 28262; Percent complete: 85.6%; Average loss: 0.8158\n",
            "Iteration: 28263; Percent complete: 85.6%; Average loss: 0.7289\n",
            "Iteration: 28264; Percent complete: 85.6%; Average loss: 0.6975\n",
            "Iteration: 28265; Percent complete: 85.7%; Average loss: 1.0060\n",
            "Iteration: 28266; Percent complete: 85.7%; Average loss: 0.8202\n",
            "Iteration: 28267; Percent complete: 85.7%; Average loss: 0.8717\n",
            "Iteration: 28268; Percent complete: 85.7%; Average loss: 0.6647\n",
            "Iteration: 28269; Percent complete: 85.7%; Average loss: 0.8408\n",
            "Iteration: 28270; Percent complete: 85.7%; Average loss: 0.7785\n",
            "Iteration: 28271; Percent complete: 85.7%; Average loss: 0.8558\n",
            "Iteration: 28272; Percent complete: 85.7%; Average loss: 0.7212\n",
            "Iteration: 28273; Percent complete: 85.7%; Average loss: 0.9012\n",
            "Iteration: 28274; Percent complete: 85.7%; Average loss: 0.6904\n",
            "Iteration: 28275; Percent complete: 85.7%; Average loss: 0.8624\n",
            "Iteration: 28276; Percent complete: 85.7%; Average loss: 0.6775\n",
            "Iteration: 28277; Percent complete: 85.7%; Average loss: 0.7073\n",
            "Iteration: 28278; Percent complete: 85.7%; Average loss: 0.7339\n",
            "Iteration: 28279; Percent complete: 85.7%; Average loss: 0.7667\n",
            "Iteration: 28280; Percent complete: 85.7%; Average loss: 0.9003\n",
            "Iteration: 28281; Percent complete: 85.7%; Average loss: 0.6210\n",
            "Iteration: 28282; Percent complete: 85.7%; Average loss: 0.8850\n",
            "Iteration: 28283; Percent complete: 85.7%; Average loss: 0.7239\n",
            "Iteration: 28284; Percent complete: 85.7%; Average loss: 0.8568\n",
            "Iteration: 28285; Percent complete: 85.7%; Average loss: 0.8519\n",
            "Iteration: 28286; Percent complete: 85.7%; Average loss: 0.6881\n",
            "Iteration: 28287; Percent complete: 85.7%; Average loss: 0.7581\n",
            "Iteration: 28288; Percent complete: 85.7%; Average loss: 0.6808\n",
            "Iteration: 28289; Percent complete: 85.7%; Average loss: 0.7615\n",
            "Iteration: 28290; Percent complete: 85.7%; Average loss: 1.0212\n",
            "Iteration: 28291; Percent complete: 85.7%; Average loss: 0.7777\n",
            "Iteration: 28292; Percent complete: 85.7%; Average loss: 0.7355\n",
            "Iteration: 28293; Percent complete: 85.7%; Average loss: 0.6547\n",
            "Iteration: 28294; Percent complete: 85.7%; Average loss: 0.8426\n",
            "Iteration: 28295; Percent complete: 85.7%; Average loss: 0.8451\n",
            "Iteration: 28296; Percent complete: 85.7%; Average loss: 0.8040\n",
            "Iteration: 28297; Percent complete: 85.7%; Average loss: 0.7919\n",
            "Iteration: 28298; Percent complete: 85.8%; Average loss: 0.7748\n",
            "Iteration: 28299; Percent complete: 85.8%; Average loss: 0.7071\n",
            "Iteration: 28300; Percent complete: 85.8%; Average loss: 0.8656\n",
            "Iteration: 28301; Percent complete: 85.8%; Average loss: 0.8170\n",
            "Iteration: 28302; Percent complete: 85.8%; Average loss: 0.7756\n",
            "Iteration: 28303; Percent complete: 85.8%; Average loss: 0.8515\n",
            "Iteration: 28304; Percent complete: 85.8%; Average loss: 0.6431\n",
            "Iteration: 28305; Percent complete: 85.8%; Average loss: 0.8569\n",
            "Iteration: 28306; Percent complete: 85.8%; Average loss: 0.7785\n",
            "Iteration: 28307; Percent complete: 85.8%; Average loss: 0.6625\n",
            "Iteration: 28308; Percent complete: 85.8%; Average loss: 0.8352\n",
            "Iteration: 28309; Percent complete: 85.8%; Average loss: 0.7515\n",
            "Iteration: 28310; Percent complete: 85.8%; Average loss: 0.7486\n",
            "Iteration: 28311; Percent complete: 85.8%; Average loss: 0.6947\n",
            "Iteration: 28312; Percent complete: 85.8%; Average loss: 0.7984\n",
            "Iteration: 28313; Percent complete: 85.8%; Average loss: 0.7672\n",
            "Iteration: 28314; Percent complete: 85.8%; Average loss: 0.5981\n",
            "Iteration: 28315; Percent complete: 85.8%; Average loss: 0.8780\n",
            "Iteration: 28316; Percent complete: 85.8%; Average loss: 1.0215\n",
            "Iteration: 28317; Percent complete: 85.8%; Average loss: 0.6363\n",
            "Iteration: 28318; Percent complete: 85.8%; Average loss: 0.8387\n",
            "Iteration: 28319; Percent complete: 85.8%; Average loss: 0.6787\n",
            "Iteration: 28320; Percent complete: 85.8%; Average loss: 0.6792\n",
            "Iteration: 28321; Percent complete: 85.8%; Average loss: 0.8912\n",
            "Iteration: 28322; Percent complete: 85.8%; Average loss: 0.7326\n",
            "Iteration: 28323; Percent complete: 85.8%; Average loss: 0.7115\n",
            "Iteration: 28324; Percent complete: 85.8%; Average loss: 1.0457\n",
            "Iteration: 28325; Percent complete: 85.8%; Average loss: 0.7633\n",
            "Iteration: 28326; Percent complete: 85.8%; Average loss: 1.0302\n",
            "Iteration: 28327; Percent complete: 85.8%; Average loss: 0.8263\n",
            "Iteration: 28328; Percent complete: 85.8%; Average loss: 0.6441\n",
            "Iteration: 28329; Percent complete: 85.8%; Average loss: 0.6698\n",
            "Iteration: 28330; Percent complete: 85.8%; Average loss: 0.9136\n",
            "Iteration: 28331; Percent complete: 85.9%; Average loss: 0.9375\n",
            "Iteration: 28332; Percent complete: 85.9%; Average loss: 0.8779\n",
            "Iteration: 28333; Percent complete: 85.9%; Average loss: 0.8195\n",
            "Iteration: 28334; Percent complete: 85.9%; Average loss: 0.8329\n",
            "Iteration: 28335; Percent complete: 85.9%; Average loss: 0.7162\n",
            "Iteration: 28336; Percent complete: 85.9%; Average loss: 0.7763\n",
            "Iteration: 28337; Percent complete: 85.9%; Average loss: 0.7189\n",
            "Iteration: 28338; Percent complete: 85.9%; Average loss: 0.8560\n",
            "Iteration: 28339; Percent complete: 85.9%; Average loss: 0.6625\n",
            "Iteration: 28340; Percent complete: 85.9%; Average loss: 0.8263\n",
            "Iteration: 28341; Percent complete: 85.9%; Average loss: 0.7103\n",
            "Iteration: 28342; Percent complete: 85.9%; Average loss: 0.7358\n",
            "Iteration: 28343; Percent complete: 85.9%; Average loss: 0.8501\n",
            "Iteration: 28344; Percent complete: 85.9%; Average loss: 0.7493\n",
            "Iteration: 28345; Percent complete: 85.9%; Average loss: 0.8059\n",
            "Iteration: 28346; Percent complete: 85.9%; Average loss: 0.9030\n",
            "Iteration: 28347; Percent complete: 85.9%; Average loss: 0.8163\n",
            "Iteration: 28348; Percent complete: 85.9%; Average loss: 0.7336\n",
            "Iteration: 28349; Percent complete: 85.9%; Average loss: 0.7642\n",
            "Iteration: 28350; Percent complete: 85.9%; Average loss: 0.9333\n",
            "Iteration: 28351; Percent complete: 85.9%; Average loss: 0.9023\n",
            "Iteration: 28352; Percent complete: 85.9%; Average loss: 1.0536\n",
            "Iteration: 28353; Percent complete: 85.9%; Average loss: 0.8363\n",
            "Iteration: 28354; Percent complete: 85.9%; Average loss: 0.7419\n",
            "Iteration: 28355; Percent complete: 85.9%; Average loss: 0.8479\n",
            "Iteration: 28356; Percent complete: 85.9%; Average loss: 0.7676\n",
            "Iteration: 28357; Percent complete: 85.9%; Average loss: 0.8976\n",
            "Iteration: 28358; Percent complete: 85.9%; Average loss: 0.6243\n",
            "Iteration: 28359; Percent complete: 85.9%; Average loss: 0.6824\n",
            "Iteration: 28360; Percent complete: 85.9%; Average loss: 0.8039\n",
            "Iteration: 28361; Percent complete: 85.9%; Average loss: 0.6773\n",
            "Iteration: 28362; Percent complete: 85.9%; Average loss: 0.7479\n",
            "Iteration: 28363; Percent complete: 85.9%; Average loss: 0.8435\n",
            "Iteration: 28364; Percent complete: 86.0%; Average loss: 0.8756\n",
            "Iteration: 28365; Percent complete: 86.0%; Average loss: 0.8120\n",
            "Iteration: 28366; Percent complete: 86.0%; Average loss: 0.8263\n",
            "Iteration: 28367; Percent complete: 86.0%; Average loss: 0.9126\n",
            "Iteration: 28368; Percent complete: 86.0%; Average loss: 1.0911\n",
            "Iteration: 28369; Percent complete: 86.0%; Average loss: 0.8301\n",
            "Iteration: 28370; Percent complete: 86.0%; Average loss: 0.8238\n",
            "Iteration: 28371; Percent complete: 86.0%; Average loss: 0.7302\n",
            "Iteration: 28372; Percent complete: 86.0%; Average loss: 0.8572\n",
            "Iteration: 28373; Percent complete: 86.0%; Average loss: 0.7509\n",
            "Iteration: 28374; Percent complete: 86.0%; Average loss: 0.7587\n",
            "Iteration: 28375; Percent complete: 86.0%; Average loss: 0.7632\n",
            "Iteration: 28376; Percent complete: 86.0%; Average loss: 0.7206\n",
            "Iteration: 28377; Percent complete: 86.0%; Average loss: 0.8769\n",
            "Iteration: 28378; Percent complete: 86.0%; Average loss: 0.7254\n",
            "Iteration: 28379; Percent complete: 86.0%; Average loss: 0.7483\n",
            "Iteration: 28380; Percent complete: 86.0%; Average loss: 0.8354\n",
            "Iteration: 28381; Percent complete: 86.0%; Average loss: 0.7065\n",
            "Iteration: 28382; Percent complete: 86.0%; Average loss: 0.7423\n",
            "Iteration: 28383; Percent complete: 86.0%; Average loss: 0.8022\n",
            "Iteration: 28384; Percent complete: 86.0%; Average loss: 1.1330\n",
            "Iteration: 28385; Percent complete: 86.0%; Average loss: 0.8625\n",
            "Iteration: 28386; Percent complete: 86.0%; Average loss: 0.9754\n",
            "Iteration: 28387; Percent complete: 86.0%; Average loss: 0.8496\n",
            "Iteration: 28388; Percent complete: 86.0%; Average loss: 0.6626\n",
            "Iteration: 28389; Percent complete: 86.0%; Average loss: 0.7249\n",
            "Iteration: 28390; Percent complete: 86.0%; Average loss: 0.9468\n",
            "Iteration: 28391; Percent complete: 86.0%; Average loss: 0.7858\n",
            "Iteration: 28392; Percent complete: 86.0%; Average loss: 0.7506\n",
            "Iteration: 28393; Percent complete: 86.0%; Average loss: 0.8253\n",
            "Iteration: 28394; Percent complete: 86.0%; Average loss: 0.6433\n",
            "Iteration: 28395; Percent complete: 86.0%; Average loss: 0.8269\n",
            "Iteration: 28396; Percent complete: 86.0%; Average loss: 0.8111\n",
            "Iteration: 28397; Percent complete: 86.1%; Average loss: 0.7278\n",
            "Iteration: 28398; Percent complete: 86.1%; Average loss: 0.8906\n",
            "Iteration: 28399; Percent complete: 86.1%; Average loss: 0.8393\n",
            "Iteration: 28400; Percent complete: 86.1%; Average loss: 0.7474\n",
            "Iteration: 28401; Percent complete: 86.1%; Average loss: 0.9698\n",
            "Iteration: 28402; Percent complete: 86.1%; Average loss: 1.0337\n",
            "Iteration: 28403; Percent complete: 86.1%; Average loss: 0.7129\n",
            "Iteration: 28404; Percent complete: 86.1%; Average loss: 0.8315\n",
            "Iteration: 28405; Percent complete: 86.1%; Average loss: 0.6961\n",
            "Iteration: 28406; Percent complete: 86.1%; Average loss: 0.8799\n",
            "Iteration: 28407; Percent complete: 86.1%; Average loss: 0.8707\n",
            "Iteration: 28408; Percent complete: 86.1%; Average loss: 0.8182\n",
            "Iteration: 28409; Percent complete: 86.1%; Average loss: 0.8877\n",
            "Iteration: 28410; Percent complete: 86.1%; Average loss: 0.7476\n",
            "Iteration: 28411; Percent complete: 86.1%; Average loss: 0.7861\n",
            "Iteration: 28412; Percent complete: 86.1%; Average loss: 0.7033\n",
            "Iteration: 28413; Percent complete: 86.1%; Average loss: 0.7949\n",
            "Iteration: 28414; Percent complete: 86.1%; Average loss: 0.8119\n",
            "Iteration: 28415; Percent complete: 86.1%; Average loss: 0.8897\n",
            "Iteration: 28416; Percent complete: 86.1%; Average loss: 0.6526\n",
            "Iteration: 28417; Percent complete: 86.1%; Average loss: 0.6585\n",
            "Iteration: 28418; Percent complete: 86.1%; Average loss: 0.7075\n",
            "Iteration: 28419; Percent complete: 86.1%; Average loss: 0.7466\n",
            "Iteration: 28420; Percent complete: 86.1%; Average loss: 0.7370\n",
            "Iteration: 28421; Percent complete: 86.1%; Average loss: 0.8268\n",
            "Iteration: 28422; Percent complete: 86.1%; Average loss: 0.7498\n",
            "Iteration: 28423; Percent complete: 86.1%; Average loss: 0.7746\n",
            "Iteration: 28424; Percent complete: 86.1%; Average loss: 0.8331\n",
            "Iteration: 28425; Percent complete: 86.1%; Average loss: 0.7429\n",
            "Iteration: 28426; Percent complete: 86.1%; Average loss: 0.7781\n",
            "Iteration: 28427; Percent complete: 86.1%; Average loss: 0.8739\n",
            "Iteration: 28428; Percent complete: 86.1%; Average loss: 0.8463\n",
            "Iteration: 28429; Percent complete: 86.1%; Average loss: 0.5830\n",
            "Iteration: 28430; Percent complete: 86.2%; Average loss: 0.8362\n",
            "Iteration: 28431; Percent complete: 86.2%; Average loss: 0.7021\n",
            "Iteration: 28432; Percent complete: 86.2%; Average loss: 0.7218\n",
            "Iteration: 28433; Percent complete: 86.2%; Average loss: 0.8709\n",
            "Iteration: 28434; Percent complete: 86.2%; Average loss: 0.7106\n",
            "Iteration: 28435; Percent complete: 86.2%; Average loss: 0.6928\n",
            "Iteration: 28436; Percent complete: 86.2%; Average loss: 0.7766\n",
            "Iteration: 28437; Percent complete: 86.2%; Average loss: 0.7438\n",
            "Iteration: 28438; Percent complete: 86.2%; Average loss: 0.7413\n",
            "Iteration: 28439; Percent complete: 86.2%; Average loss: 0.8499\n",
            "Iteration: 28440; Percent complete: 86.2%; Average loss: 0.7400\n",
            "Iteration: 28441; Percent complete: 86.2%; Average loss: 0.7020\n",
            "Iteration: 28442; Percent complete: 86.2%; Average loss: 0.6491\n",
            "Iteration: 28443; Percent complete: 86.2%; Average loss: 0.8030\n",
            "Iteration: 28444; Percent complete: 86.2%; Average loss: 0.7863\n",
            "Iteration: 28445; Percent complete: 86.2%; Average loss: 0.7800\n",
            "Iteration: 28446; Percent complete: 86.2%; Average loss: 0.7151\n",
            "Iteration: 28447; Percent complete: 86.2%; Average loss: 0.9712\n",
            "Iteration: 28448; Percent complete: 86.2%; Average loss: 0.8951\n",
            "Iteration: 28449; Percent complete: 86.2%; Average loss: 0.7495\n",
            "Iteration: 28450; Percent complete: 86.2%; Average loss: 0.7734\n",
            "Iteration: 28451; Percent complete: 86.2%; Average loss: 0.7882\n",
            "Iteration: 28452; Percent complete: 86.2%; Average loss: 0.8885\n",
            "Iteration: 28453; Percent complete: 86.2%; Average loss: 0.7571\n",
            "Iteration: 28454; Percent complete: 86.2%; Average loss: 0.8093\n",
            "Iteration: 28455; Percent complete: 86.2%; Average loss: 0.7934\n",
            "Iteration: 28456; Percent complete: 86.2%; Average loss: 0.6632\n",
            "Iteration: 28457; Percent complete: 86.2%; Average loss: 0.8454\n",
            "Iteration: 28458; Percent complete: 86.2%; Average loss: 0.6736\n",
            "Iteration: 28459; Percent complete: 86.2%; Average loss: 0.7941\n",
            "Iteration: 28460; Percent complete: 86.2%; Average loss: 0.7245\n",
            "Iteration: 28461; Percent complete: 86.2%; Average loss: 0.8117\n",
            "Iteration: 28462; Percent complete: 86.2%; Average loss: 0.7043\n",
            "Iteration: 28463; Percent complete: 86.3%; Average loss: 0.7210\n",
            "Iteration: 28464; Percent complete: 86.3%; Average loss: 0.7379\n",
            "Iteration: 28465; Percent complete: 86.3%; Average loss: 0.9034\n",
            "Iteration: 28466; Percent complete: 86.3%; Average loss: 0.8720\n",
            "Iteration: 28467; Percent complete: 86.3%; Average loss: 0.8232\n",
            "Iteration: 28468; Percent complete: 86.3%; Average loss: 0.7427\n",
            "Iteration: 28469; Percent complete: 86.3%; Average loss: 0.8591\n",
            "Iteration: 28470; Percent complete: 86.3%; Average loss: 0.8186\n",
            "Iteration: 28471; Percent complete: 86.3%; Average loss: 0.7118\n",
            "Iteration: 28472; Percent complete: 86.3%; Average loss: 0.8755\n",
            "Iteration: 28473; Percent complete: 86.3%; Average loss: 0.9297\n",
            "Iteration: 28474; Percent complete: 86.3%; Average loss: 0.8236\n",
            "Iteration: 28475; Percent complete: 86.3%; Average loss: 0.8204\n",
            "Iteration: 28476; Percent complete: 86.3%; Average loss: 0.7068\n",
            "Iteration: 28477; Percent complete: 86.3%; Average loss: 0.7477\n",
            "Iteration: 28478; Percent complete: 86.3%; Average loss: 0.7974\n",
            "Iteration: 28479; Percent complete: 86.3%; Average loss: 0.8484\n",
            "Iteration: 28480; Percent complete: 86.3%; Average loss: 0.8250\n",
            "Iteration: 28481; Percent complete: 86.3%; Average loss: 0.7587\n",
            "Iteration: 28482; Percent complete: 86.3%; Average loss: 0.7178\n",
            "Iteration: 28483; Percent complete: 86.3%; Average loss: 0.7996\n",
            "Iteration: 28484; Percent complete: 86.3%; Average loss: 0.7780\n",
            "Iteration: 28485; Percent complete: 86.3%; Average loss: 0.7940\n",
            "Iteration: 28486; Percent complete: 86.3%; Average loss: 0.8383\n",
            "Iteration: 28487; Percent complete: 86.3%; Average loss: 0.6889\n",
            "Iteration: 28488; Percent complete: 86.3%; Average loss: 0.8100\n",
            "Iteration: 28489; Percent complete: 86.3%; Average loss: 0.8022\n",
            "Iteration: 28490; Percent complete: 86.3%; Average loss: 0.8812\n",
            "Iteration: 28491; Percent complete: 86.3%; Average loss: 0.7339\n",
            "Iteration: 28492; Percent complete: 86.3%; Average loss: 0.8389\n",
            "Iteration: 28493; Percent complete: 86.3%; Average loss: 0.8600\n",
            "Iteration: 28494; Percent complete: 86.3%; Average loss: 0.7448\n",
            "Iteration: 28495; Percent complete: 86.3%; Average loss: 0.6912\n",
            "Iteration: 28496; Percent complete: 86.4%; Average loss: 0.5769\n",
            "Iteration: 28497; Percent complete: 86.4%; Average loss: 0.7851\n",
            "Iteration: 28498; Percent complete: 86.4%; Average loss: 0.7668\n",
            "Iteration: 28499; Percent complete: 86.4%; Average loss: 0.7608\n",
            "Iteration: 28500; Percent complete: 86.4%; Average loss: 0.9102\n",
            "Iteration: 28501; Percent complete: 86.4%; Average loss: 0.8206\n",
            "Iteration: 28502; Percent complete: 86.4%; Average loss: 0.6929\n",
            "Iteration: 28503; Percent complete: 86.4%; Average loss: 0.7031\n",
            "Iteration: 28504; Percent complete: 86.4%; Average loss: 0.7722\n",
            "Iteration: 28505; Percent complete: 86.4%; Average loss: 0.7597\n",
            "Iteration: 28506; Percent complete: 86.4%; Average loss: 0.6789\n",
            "Iteration: 28507; Percent complete: 86.4%; Average loss: 0.8622\n",
            "Iteration: 28508; Percent complete: 86.4%; Average loss: 0.7463\n",
            "Iteration: 28509; Percent complete: 86.4%; Average loss: 0.6165\n",
            "Iteration: 28510; Percent complete: 86.4%; Average loss: 0.8753\n",
            "Iteration: 28511; Percent complete: 86.4%; Average loss: 0.6406\n",
            "Iteration: 28512; Percent complete: 86.4%; Average loss: 0.9270\n",
            "Iteration: 28513; Percent complete: 86.4%; Average loss: 0.8218\n",
            "Iteration: 28514; Percent complete: 86.4%; Average loss: 0.6354\n",
            "Iteration: 28515; Percent complete: 86.4%; Average loss: 0.7803\n",
            "Iteration: 28516; Percent complete: 86.4%; Average loss: 0.6901\n",
            "Iteration: 28517; Percent complete: 86.4%; Average loss: 0.6867\n",
            "Iteration: 28518; Percent complete: 86.4%; Average loss: 0.8839\n",
            "Iteration: 28519; Percent complete: 86.4%; Average loss: 0.6713\n",
            "Iteration: 28520; Percent complete: 86.4%; Average loss: 0.7255\n",
            "Iteration: 28521; Percent complete: 86.4%; Average loss: 0.7306\n",
            "Iteration: 28522; Percent complete: 86.4%; Average loss: 0.6335\n",
            "Iteration: 28523; Percent complete: 86.4%; Average loss: 0.7839\n",
            "Iteration: 28524; Percent complete: 86.4%; Average loss: 0.7912\n",
            "Iteration: 28525; Percent complete: 86.4%; Average loss: 0.7441\n",
            "Iteration: 28526; Percent complete: 86.4%; Average loss: 0.7458\n",
            "Iteration: 28527; Percent complete: 86.4%; Average loss: 0.9025\n",
            "Iteration: 28528; Percent complete: 86.4%; Average loss: 0.6983\n",
            "Iteration: 28529; Percent complete: 86.5%; Average loss: 0.6048\n",
            "Iteration: 28530; Percent complete: 86.5%; Average loss: 0.8504\n",
            "Iteration: 28531; Percent complete: 86.5%; Average loss: 0.6420\n",
            "Iteration: 28532; Percent complete: 86.5%; Average loss: 0.7172\n",
            "Iteration: 28533; Percent complete: 86.5%; Average loss: 0.7323\n",
            "Iteration: 28534; Percent complete: 86.5%; Average loss: 0.7940\n",
            "Iteration: 28535; Percent complete: 86.5%; Average loss: 0.7776\n",
            "Iteration: 28536; Percent complete: 86.5%; Average loss: 0.6535\n",
            "Iteration: 28537; Percent complete: 86.5%; Average loss: 0.6992\n",
            "Iteration: 28538; Percent complete: 86.5%; Average loss: 0.7857\n",
            "Iteration: 28539; Percent complete: 86.5%; Average loss: 0.6981\n",
            "Iteration: 28540; Percent complete: 86.5%; Average loss: 0.8635\n",
            "Iteration: 28541; Percent complete: 86.5%; Average loss: 0.8696\n",
            "Iteration: 28542; Percent complete: 86.5%; Average loss: 0.6453\n",
            "Iteration: 28543; Percent complete: 86.5%; Average loss: 0.6745\n",
            "Iteration: 28544; Percent complete: 86.5%; Average loss: 0.8882\n",
            "Iteration: 28545; Percent complete: 86.5%; Average loss: 0.8670\n",
            "Iteration: 28546; Percent complete: 86.5%; Average loss: 0.6794\n",
            "Iteration: 28547; Percent complete: 86.5%; Average loss: 0.7307\n",
            "Iteration: 28548; Percent complete: 86.5%; Average loss: 0.7578\n",
            "Iteration: 28549; Percent complete: 86.5%; Average loss: 0.7238\n",
            "Iteration: 28550; Percent complete: 86.5%; Average loss: 0.7208\n",
            "Iteration: 28551; Percent complete: 86.5%; Average loss: 0.6714\n",
            "Iteration: 28552; Percent complete: 86.5%; Average loss: 0.9141\n",
            "Iteration: 28553; Percent complete: 86.5%; Average loss: 0.6612\n",
            "Iteration: 28554; Percent complete: 86.5%; Average loss: 0.7921\n",
            "Iteration: 28555; Percent complete: 86.5%; Average loss: 0.7161\n",
            "Iteration: 28556; Percent complete: 86.5%; Average loss: 0.8555\n",
            "Iteration: 28557; Percent complete: 86.5%; Average loss: 0.8565\n",
            "Iteration: 28558; Percent complete: 86.5%; Average loss: 0.7747\n",
            "Iteration: 28559; Percent complete: 86.5%; Average loss: 1.0753\n",
            "Iteration: 28560; Percent complete: 86.5%; Average loss: 0.8423\n",
            "Iteration: 28561; Percent complete: 86.5%; Average loss: 0.7284\n",
            "Iteration: 28562; Percent complete: 86.6%; Average loss: 0.6286\n",
            "Iteration: 28563; Percent complete: 86.6%; Average loss: 0.6033\n",
            "Iteration: 28564; Percent complete: 86.6%; Average loss: 0.8444\n",
            "Iteration: 28565; Percent complete: 86.6%; Average loss: 0.8490\n",
            "Iteration: 28566; Percent complete: 86.6%; Average loss: 0.8797\n",
            "Iteration: 28567; Percent complete: 86.6%; Average loss: 0.6653\n",
            "Iteration: 28568; Percent complete: 86.6%; Average loss: 0.8273\n",
            "Iteration: 28569; Percent complete: 86.6%; Average loss: 0.7786\n",
            "Iteration: 28570; Percent complete: 86.6%; Average loss: 0.7318\n",
            "Iteration: 28571; Percent complete: 86.6%; Average loss: 0.8149\n",
            "Iteration: 28572; Percent complete: 86.6%; Average loss: 0.7586\n",
            "Iteration: 28573; Percent complete: 86.6%; Average loss: 0.9059\n",
            "Iteration: 28574; Percent complete: 86.6%; Average loss: 0.9065\n",
            "Iteration: 28575; Percent complete: 86.6%; Average loss: 0.5619\n",
            "Iteration: 28576; Percent complete: 86.6%; Average loss: 0.7870\n",
            "Iteration: 28577; Percent complete: 86.6%; Average loss: 0.7350\n",
            "Iteration: 28578; Percent complete: 86.6%; Average loss: 0.8180\n",
            "Iteration: 28579; Percent complete: 86.6%; Average loss: 0.9297\n",
            "Iteration: 28580; Percent complete: 86.6%; Average loss: 0.7537\n",
            "Iteration: 28581; Percent complete: 86.6%; Average loss: 0.7847\n",
            "Iteration: 28582; Percent complete: 86.6%; Average loss: 0.6049\n",
            "Iteration: 28583; Percent complete: 86.6%; Average loss: 0.6879\n",
            "Iteration: 28584; Percent complete: 86.6%; Average loss: 0.7239\n",
            "Iteration: 28585; Percent complete: 86.6%; Average loss: 0.7028\n",
            "Iteration: 28586; Percent complete: 86.6%; Average loss: 0.7666\n",
            "Iteration: 28587; Percent complete: 86.6%; Average loss: 0.8697\n",
            "Iteration: 28588; Percent complete: 86.6%; Average loss: 0.5845\n",
            "Iteration: 28589; Percent complete: 86.6%; Average loss: 0.7988\n",
            "Iteration: 28590; Percent complete: 86.6%; Average loss: 0.7974\n",
            "Iteration: 28591; Percent complete: 86.6%; Average loss: 0.7836\n",
            "Iteration: 28592; Percent complete: 86.6%; Average loss: 0.8327\n",
            "Iteration: 28593; Percent complete: 86.6%; Average loss: 0.8602\n",
            "Iteration: 28594; Percent complete: 86.6%; Average loss: 0.7758\n",
            "Iteration: 28595; Percent complete: 86.7%; Average loss: 0.7897\n",
            "Iteration: 28596; Percent complete: 86.7%; Average loss: 0.6895\n",
            "Iteration: 28597; Percent complete: 86.7%; Average loss: 0.7502\n",
            "Iteration: 28598; Percent complete: 86.7%; Average loss: 0.8348\n",
            "Iteration: 28599; Percent complete: 86.7%; Average loss: 0.5815\n",
            "Iteration: 28600; Percent complete: 86.7%; Average loss: 0.7997\n",
            "Iteration: 28601; Percent complete: 86.7%; Average loss: 0.7208\n",
            "Iteration: 28602; Percent complete: 86.7%; Average loss: 1.0021\n",
            "Iteration: 28603; Percent complete: 86.7%; Average loss: 0.7939\n",
            "Iteration: 28604; Percent complete: 86.7%; Average loss: 0.8138\n",
            "Iteration: 28605; Percent complete: 86.7%; Average loss: 0.6266\n",
            "Iteration: 28606; Percent complete: 86.7%; Average loss: 0.8474\n",
            "Iteration: 28607; Percent complete: 86.7%; Average loss: 0.7717\n",
            "Iteration: 28608; Percent complete: 86.7%; Average loss: 0.8780\n",
            "Iteration: 28609; Percent complete: 86.7%; Average loss: 0.7893\n",
            "Iteration: 28610; Percent complete: 86.7%; Average loss: 0.7272\n",
            "Iteration: 28611; Percent complete: 86.7%; Average loss: 0.9574\n",
            "Iteration: 28612; Percent complete: 86.7%; Average loss: 0.9585\n",
            "Iteration: 28613; Percent complete: 86.7%; Average loss: 0.8021\n",
            "Iteration: 28614; Percent complete: 86.7%; Average loss: 0.8614\n",
            "Iteration: 28615; Percent complete: 86.7%; Average loss: 0.6707\n",
            "Iteration: 28616; Percent complete: 86.7%; Average loss: 0.8289\n",
            "Iteration: 28617; Percent complete: 86.7%; Average loss: 0.8218\n",
            "Iteration: 28618; Percent complete: 86.7%; Average loss: 0.6898\n",
            "Iteration: 28619; Percent complete: 86.7%; Average loss: 0.6858\n",
            "Iteration: 28620; Percent complete: 86.7%; Average loss: 0.8719\n",
            "Iteration: 28621; Percent complete: 86.7%; Average loss: 0.8434\n",
            "Iteration: 28622; Percent complete: 86.7%; Average loss: 0.8545\n",
            "Iteration: 28623; Percent complete: 86.7%; Average loss: 0.8080\n",
            "Iteration: 28624; Percent complete: 86.7%; Average loss: 0.7901\n",
            "Iteration: 28625; Percent complete: 86.7%; Average loss: 0.6959\n",
            "Iteration: 28626; Percent complete: 86.7%; Average loss: 0.7847\n",
            "Iteration: 28627; Percent complete: 86.7%; Average loss: 0.8055\n",
            "Iteration: 28628; Percent complete: 86.8%; Average loss: 0.8593\n",
            "Iteration: 28629; Percent complete: 86.8%; Average loss: 0.6238\n",
            "Iteration: 28630; Percent complete: 86.8%; Average loss: 0.6879\n",
            "Iteration: 28631; Percent complete: 86.8%; Average loss: 0.8863\n",
            "Iteration: 28632; Percent complete: 86.8%; Average loss: 0.7679\n",
            "Iteration: 28633; Percent complete: 86.8%; Average loss: 0.8361\n",
            "Iteration: 28634; Percent complete: 86.8%; Average loss: 0.9144\n",
            "Iteration: 28635; Percent complete: 86.8%; Average loss: 0.6557\n",
            "Iteration: 28636; Percent complete: 86.8%; Average loss: 0.8789\n",
            "Iteration: 28637; Percent complete: 86.8%; Average loss: 0.6308\n",
            "Iteration: 28638; Percent complete: 86.8%; Average loss: 0.8130\n",
            "Iteration: 28639; Percent complete: 86.8%; Average loss: 0.8868\n",
            "Iteration: 28640; Percent complete: 86.8%; Average loss: 0.6911\n",
            "Iteration: 28641; Percent complete: 86.8%; Average loss: 0.8977\n",
            "Iteration: 28642; Percent complete: 86.8%; Average loss: 0.7524\n",
            "Iteration: 28643; Percent complete: 86.8%; Average loss: 0.8265\n",
            "Iteration: 28644; Percent complete: 86.8%; Average loss: 0.8426\n",
            "Iteration: 28645; Percent complete: 86.8%; Average loss: 0.8288\n",
            "Iteration: 28646; Percent complete: 86.8%; Average loss: 0.8650\n",
            "Iteration: 28647; Percent complete: 86.8%; Average loss: 0.7245\n",
            "Iteration: 28648; Percent complete: 86.8%; Average loss: 0.6223\n",
            "Iteration: 28649; Percent complete: 86.8%; Average loss: 0.7719\n",
            "Iteration: 28650; Percent complete: 86.8%; Average loss: 0.9103\n",
            "Iteration: 28651; Percent complete: 86.8%; Average loss: 0.9862\n",
            "Iteration: 28652; Percent complete: 86.8%; Average loss: 0.7897\n",
            "Iteration: 28653; Percent complete: 86.8%; Average loss: 0.8890\n",
            "Iteration: 28654; Percent complete: 86.8%; Average loss: 0.8602\n",
            "Iteration: 28655; Percent complete: 86.8%; Average loss: 0.7740\n",
            "Iteration: 28656; Percent complete: 86.8%; Average loss: 0.7989\n",
            "Iteration: 28657; Percent complete: 86.8%; Average loss: 0.9183\n",
            "Iteration: 28658; Percent complete: 86.8%; Average loss: 0.6877\n",
            "Iteration: 28659; Percent complete: 86.8%; Average loss: 0.8611\n",
            "Iteration: 28660; Percent complete: 86.8%; Average loss: 0.6979\n",
            "Iteration: 28661; Percent complete: 86.9%; Average loss: 0.8391\n",
            "Iteration: 28662; Percent complete: 86.9%; Average loss: 0.6772\n",
            "Iteration: 28663; Percent complete: 86.9%; Average loss: 0.7083\n",
            "Iteration: 28664; Percent complete: 86.9%; Average loss: 0.6912\n",
            "Iteration: 28665; Percent complete: 86.9%; Average loss: 0.6406\n",
            "Iteration: 28666; Percent complete: 86.9%; Average loss: 0.7856\n",
            "Iteration: 28667; Percent complete: 86.9%; Average loss: 0.7440\n",
            "Iteration: 28668; Percent complete: 86.9%; Average loss: 0.6593\n",
            "Iteration: 28669; Percent complete: 86.9%; Average loss: 0.8393\n",
            "Iteration: 28670; Percent complete: 86.9%; Average loss: 0.6968\n",
            "Iteration: 28671; Percent complete: 86.9%; Average loss: 0.7477\n",
            "Iteration: 28672; Percent complete: 86.9%; Average loss: 0.6615\n",
            "Iteration: 28673; Percent complete: 86.9%; Average loss: 0.8306\n",
            "Iteration: 28674; Percent complete: 86.9%; Average loss: 0.9147\n",
            "Iteration: 28675; Percent complete: 86.9%; Average loss: 0.6161\n",
            "Iteration: 28676; Percent complete: 86.9%; Average loss: 0.8742\n",
            "Iteration: 28677; Percent complete: 86.9%; Average loss: 0.6725\n",
            "Iteration: 28678; Percent complete: 86.9%; Average loss: 0.9138\n",
            "Iteration: 28679; Percent complete: 86.9%; Average loss: 0.7868\n",
            "Iteration: 28680; Percent complete: 86.9%; Average loss: 0.7007\n",
            "Iteration: 28681; Percent complete: 86.9%; Average loss: 0.8865\n",
            "Iteration: 28682; Percent complete: 86.9%; Average loss: 0.6824\n",
            "Iteration: 28683; Percent complete: 86.9%; Average loss: 0.7228\n",
            "Iteration: 28684; Percent complete: 86.9%; Average loss: 0.8493\n",
            "Iteration: 28685; Percent complete: 86.9%; Average loss: 0.7970\n",
            "Iteration: 28686; Percent complete: 86.9%; Average loss: 0.6398\n",
            "Iteration: 28687; Percent complete: 86.9%; Average loss: 0.8412\n",
            "Iteration: 28688; Percent complete: 86.9%; Average loss: 0.6688\n",
            "Iteration: 28689; Percent complete: 86.9%; Average loss: 0.7609\n",
            "Iteration: 28690; Percent complete: 86.9%; Average loss: 0.6564\n",
            "Iteration: 28691; Percent complete: 86.9%; Average loss: 0.7473\n",
            "Iteration: 28692; Percent complete: 86.9%; Average loss: 0.7155\n",
            "Iteration: 28693; Percent complete: 86.9%; Average loss: 0.7753\n",
            "Iteration: 28694; Percent complete: 87.0%; Average loss: 0.8493\n",
            "Iteration: 28695; Percent complete: 87.0%; Average loss: 0.8511\n",
            "Iteration: 28696; Percent complete: 87.0%; Average loss: 0.7719\n",
            "Iteration: 28697; Percent complete: 87.0%; Average loss: 0.7440\n",
            "Iteration: 28698; Percent complete: 87.0%; Average loss: 0.7022\n",
            "Iteration: 28699; Percent complete: 87.0%; Average loss: 0.7528\n",
            "Iteration: 28700; Percent complete: 87.0%; Average loss: 0.7419\n",
            "Iteration: 28701; Percent complete: 87.0%; Average loss: 0.8885\n",
            "Iteration: 28702; Percent complete: 87.0%; Average loss: 0.7871\n",
            "Iteration: 28703; Percent complete: 87.0%; Average loss: 0.7410\n",
            "Iteration: 28704; Percent complete: 87.0%; Average loss: 0.9054\n",
            "Iteration: 28705; Percent complete: 87.0%; Average loss: 0.7736\n",
            "Iteration: 28706; Percent complete: 87.0%; Average loss: 0.6690\n",
            "Iteration: 28707; Percent complete: 87.0%; Average loss: 0.6149\n",
            "Iteration: 28708; Percent complete: 87.0%; Average loss: 0.7567\n",
            "Iteration: 28709; Percent complete: 87.0%; Average loss: 0.7417\n",
            "Iteration: 28710; Percent complete: 87.0%; Average loss: 0.6330\n",
            "Iteration: 28711; Percent complete: 87.0%; Average loss: 0.7257\n",
            "Iteration: 28712; Percent complete: 87.0%; Average loss: 0.8984\n",
            "Iteration: 28713; Percent complete: 87.0%; Average loss: 0.7132\n",
            "Iteration: 28714; Percent complete: 87.0%; Average loss: 0.9380\n",
            "Iteration: 28715; Percent complete: 87.0%; Average loss: 0.9033\n",
            "Iteration: 28716; Percent complete: 87.0%; Average loss: 0.7150\n",
            "Iteration: 28717; Percent complete: 87.0%; Average loss: 0.7537\n",
            "Iteration: 28718; Percent complete: 87.0%; Average loss: 0.7551\n",
            "Iteration: 28719; Percent complete: 87.0%; Average loss: 0.8730\n",
            "Iteration: 28720; Percent complete: 87.0%; Average loss: 0.7336\n",
            "Iteration: 28721; Percent complete: 87.0%; Average loss: 1.0131\n",
            "Iteration: 28722; Percent complete: 87.0%; Average loss: 0.8220\n",
            "Iteration: 28723; Percent complete: 87.0%; Average loss: 0.8670\n",
            "Iteration: 28724; Percent complete: 87.0%; Average loss: 0.8584\n",
            "Iteration: 28725; Percent complete: 87.0%; Average loss: 0.7920\n",
            "Iteration: 28726; Percent complete: 87.0%; Average loss: 0.7460\n",
            "Iteration: 28727; Percent complete: 87.1%; Average loss: 0.7312\n",
            "Iteration: 28728; Percent complete: 87.1%; Average loss: 0.7280\n",
            "Iteration: 28729; Percent complete: 87.1%; Average loss: 0.8724\n",
            "Iteration: 28730; Percent complete: 87.1%; Average loss: 0.6581\n",
            "Iteration: 28731; Percent complete: 87.1%; Average loss: 0.6335\n",
            "Iteration: 28732; Percent complete: 87.1%; Average loss: 0.7339\n",
            "Iteration: 28733; Percent complete: 87.1%; Average loss: 0.7343\n",
            "Iteration: 28734; Percent complete: 87.1%; Average loss: 0.9579\n",
            "Iteration: 28735; Percent complete: 87.1%; Average loss: 0.7321\n",
            "Iteration: 28736; Percent complete: 87.1%; Average loss: 0.7672\n",
            "Iteration: 28737; Percent complete: 87.1%; Average loss: 0.9170\n",
            "Iteration: 28738; Percent complete: 87.1%; Average loss: 0.8380\n",
            "Iteration: 28739; Percent complete: 87.1%; Average loss: 0.9680\n",
            "Iteration: 28740; Percent complete: 87.1%; Average loss: 0.7838\n",
            "Iteration: 28741; Percent complete: 87.1%; Average loss: 0.7418\n",
            "Iteration: 28742; Percent complete: 87.1%; Average loss: 0.8585\n",
            "Iteration: 28743; Percent complete: 87.1%; Average loss: 0.7646\n",
            "Iteration: 28744; Percent complete: 87.1%; Average loss: 0.7638\n",
            "Iteration: 28745; Percent complete: 87.1%; Average loss: 0.7786\n",
            "Iteration: 28746; Percent complete: 87.1%; Average loss: 0.9054\n",
            "Iteration: 28747; Percent complete: 87.1%; Average loss: 0.7262\n",
            "Iteration: 28748; Percent complete: 87.1%; Average loss: 0.7599\n",
            "Iteration: 28749; Percent complete: 87.1%; Average loss: 0.7015\n",
            "Iteration: 28750; Percent complete: 87.1%; Average loss: 0.6523\n",
            "Iteration: 28751; Percent complete: 87.1%; Average loss: 0.8253\n",
            "Iteration: 28752; Percent complete: 87.1%; Average loss: 0.8387\n",
            "Iteration: 28753; Percent complete: 87.1%; Average loss: 0.6921\n",
            "Iteration: 28754; Percent complete: 87.1%; Average loss: 0.8231\n",
            "Iteration: 28755; Percent complete: 87.1%; Average loss: 0.5137\n",
            "Iteration: 28756; Percent complete: 87.1%; Average loss: 0.6813\n",
            "Iteration: 28757; Percent complete: 87.1%; Average loss: 0.8386\n",
            "Iteration: 28758; Percent complete: 87.1%; Average loss: 0.9943\n",
            "Iteration: 28759; Percent complete: 87.1%; Average loss: 0.6022\n",
            "Iteration: 28760; Percent complete: 87.2%; Average loss: 0.8396\n",
            "Iteration: 28761; Percent complete: 87.2%; Average loss: 0.7824\n",
            "Iteration: 28762; Percent complete: 87.2%; Average loss: 0.9435\n",
            "Iteration: 28763; Percent complete: 87.2%; Average loss: 0.7620\n",
            "Iteration: 28764; Percent complete: 87.2%; Average loss: 0.7279\n",
            "Iteration: 28765; Percent complete: 87.2%; Average loss: 0.7637\n",
            "Iteration: 28766; Percent complete: 87.2%; Average loss: 0.7019\n",
            "Iteration: 28767; Percent complete: 87.2%; Average loss: 0.6669\n",
            "Iteration: 28768; Percent complete: 87.2%; Average loss: 0.7898\n",
            "Iteration: 28769; Percent complete: 87.2%; Average loss: 0.8340\n",
            "Iteration: 28770; Percent complete: 87.2%; Average loss: 0.6592\n",
            "Iteration: 28771; Percent complete: 87.2%; Average loss: 0.7070\n",
            "Iteration: 28772; Percent complete: 87.2%; Average loss: 0.6413\n",
            "Iteration: 28773; Percent complete: 87.2%; Average loss: 0.7372\n",
            "Iteration: 28774; Percent complete: 87.2%; Average loss: 0.8131\n",
            "Iteration: 28775; Percent complete: 87.2%; Average loss: 0.6037\n",
            "Iteration: 28776; Percent complete: 87.2%; Average loss: 0.6996\n",
            "Iteration: 28777; Percent complete: 87.2%; Average loss: 0.7280\n",
            "Iteration: 28778; Percent complete: 87.2%; Average loss: 0.7210\n",
            "Iteration: 28779; Percent complete: 87.2%; Average loss: 0.7615\n",
            "Iteration: 28780; Percent complete: 87.2%; Average loss: 0.8993\n",
            "Iteration: 28781; Percent complete: 87.2%; Average loss: 0.7917\n",
            "Iteration: 28782; Percent complete: 87.2%; Average loss: 0.7164\n",
            "Iteration: 28783; Percent complete: 87.2%; Average loss: 0.8423\n",
            "Iteration: 28784; Percent complete: 87.2%; Average loss: 0.9567\n",
            "Iteration: 28785; Percent complete: 87.2%; Average loss: 0.8150\n",
            "Iteration: 28786; Percent complete: 87.2%; Average loss: 0.7576\n",
            "Iteration: 28787; Percent complete: 87.2%; Average loss: 0.8342\n",
            "Iteration: 28788; Percent complete: 87.2%; Average loss: 0.5872\n",
            "Iteration: 28789; Percent complete: 87.2%; Average loss: 0.6807\n",
            "Iteration: 28790; Percent complete: 87.2%; Average loss: 0.6671\n",
            "Iteration: 28791; Percent complete: 87.2%; Average loss: 0.6494\n",
            "Iteration: 28792; Percent complete: 87.2%; Average loss: 0.6558\n",
            "Iteration: 28793; Percent complete: 87.3%; Average loss: 0.8824\n",
            "Iteration: 28794; Percent complete: 87.3%; Average loss: 0.7026\n",
            "Iteration: 28795; Percent complete: 87.3%; Average loss: 0.7447\n",
            "Iteration: 28796; Percent complete: 87.3%; Average loss: 0.7020\n",
            "Iteration: 28797; Percent complete: 87.3%; Average loss: 0.6580\n",
            "Iteration: 28798; Percent complete: 87.3%; Average loss: 0.8936\n",
            "Iteration: 28799; Percent complete: 87.3%; Average loss: 0.8047\n",
            "Iteration: 28800; Percent complete: 87.3%; Average loss: 0.8721\n",
            "Iteration: 28801; Percent complete: 87.3%; Average loss: 0.7567\n",
            "Iteration: 28802; Percent complete: 87.3%; Average loss: 0.6957\n",
            "Iteration: 28803; Percent complete: 87.3%; Average loss: 0.6372\n",
            "Iteration: 28804; Percent complete: 87.3%; Average loss: 0.7872\n",
            "Iteration: 28805; Percent complete: 87.3%; Average loss: 0.6636\n",
            "Iteration: 28806; Percent complete: 87.3%; Average loss: 0.7738\n",
            "Iteration: 28807; Percent complete: 87.3%; Average loss: 0.8020\n",
            "Iteration: 28808; Percent complete: 87.3%; Average loss: 0.8260\n",
            "Iteration: 28809; Percent complete: 87.3%; Average loss: 0.7844\n",
            "Iteration: 28810; Percent complete: 87.3%; Average loss: 0.7059\n",
            "Iteration: 28811; Percent complete: 87.3%; Average loss: 0.7540\n",
            "Iteration: 28812; Percent complete: 87.3%; Average loss: 0.8233\n",
            "Iteration: 28813; Percent complete: 87.3%; Average loss: 0.8124\n",
            "Iteration: 28814; Percent complete: 87.3%; Average loss: 0.8420\n",
            "Iteration: 28815; Percent complete: 87.3%; Average loss: 0.7525\n",
            "Iteration: 28816; Percent complete: 87.3%; Average loss: 0.7138\n",
            "Iteration: 28817; Percent complete: 87.3%; Average loss: 0.8944\n",
            "Iteration: 28818; Percent complete: 87.3%; Average loss: 0.6957\n",
            "Iteration: 28819; Percent complete: 87.3%; Average loss: 0.5855\n",
            "Iteration: 28820; Percent complete: 87.3%; Average loss: 0.7663\n",
            "Iteration: 28821; Percent complete: 87.3%; Average loss: 0.6925\n",
            "Iteration: 28822; Percent complete: 87.3%; Average loss: 0.6958\n",
            "Iteration: 28823; Percent complete: 87.3%; Average loss: 0.6417\n",
            "Iteration: 28824; Percent complete: 87.3%; Average loss: 0.6222\n",
            "Iteration: 28825; Percent complete: 87.3%; Average loss: 0.7323\n",
            "Iteration: 28826; Percent complete: 87.4%; Average loss: 0.7276\n",
            "Iteration: 28827; Percent complete: 87.4%; Average loss: 0.7301\n",
            "Iteration: 28828; Percent complete: 87.4%; Average loss: 0.6626\n",
            "Iteration: 28829; Percent complete: 87.4%; Average loss: 0.7281\n",
            "Iteration: 28830; Percent complete: 87.4%; Average loss: 0.7546\n",
            "Iteration: 28831; Percent complete: 87.4%; Average loss: 0.6703\n",
            "Iteration: 28832; Percent complete: 87.4%; Average loss: 0.8141\n",
            "Iteration: 28833; Percent complete: 87.4%; Average loss: 0.7787\n",
            "Iteration: 28834; Percent complete: 87.4%; Average loss: 0.7905\n",
            "Iteration: 28835; Percent complete: 87.4%; Average loss: 0.6277\n",
            "Iteration: 28836; Percent complete: 87.4%; Average loss: 0.7670\n",
            "Iteration: 28837; Percent complete: 87.4%; Average loss: 0.7297\n",
            "Iteration: 28838; Percent complete: 87.4%; Average loss: 0.8620\n",
            "Iteration: 28839; Percent complete: 87.4%; Average loss: 0.8035\n",
            "Iteration: 28840; Percent complete: 87.4%; Average loss: 0.8328\n",
            "Iteration: 28841; Percent complete: 87.4%; Average loss: 0.7477\n",
            "Iteration: 28842; Percent complete: 87.4%; Average loss: 0.7523\n",
            "Iteration: 28843; Percent complete: 87.4%; Average loss: 0.8085\n",
            "Iteration: 28844; Percent complete: 87.4%; Average loss: 0.6088\n",
            "Iteration: 28845; Percent complete: 87.4%; Average loss: 0.7998\n",
            "Iteration: 28846; Percent complete: 87.4%; Average loss: 0.7400\n",
            "Iteration: 28847; Percent complete: 87.4%; Average loss: 0.6686\n",
            "Iteration: 28848; Percent complete: 87.4%; Average loss: 0.5969\n",
            "Iteration: 28849; Percent complete: 87.4%; Average loss: 0.8517\n",
            "Iteration: 28850; Percent complete: 87.4%; Average loss: 0.8808\n",
            "Iteration: 28851; Percent complete: 87.4%; Average loss: 0.6651\n",
            "Iteration: 28852; Percent complete: 87.4%; Average loss: 0.5999\n",
            "Iteration: 28853; Percent complete: 87.4%; Average loss: 0.8684\n",
            "Iteration: 28854; Percent complete: 87.4%; Average loss: 0.6882\n",
            "Iteration: 28855; Percent complete: 87.4%; Average loss: 0.6482\n",
            "Iteration: 28856; Percent complete: 87.4%; Average loss: 0.6940\n",
            "Iteration: 28857; Percent complete: 87.4%; Average loss: 0.7073\n",
            "Iteration: 28858; Percent complete: 87.4%; Average loss: 0.7995\n",
            "Iteration: 28859; Percent complete: 87.5%; Average loss: 0.7519\n",
            "Iteration: 28860; Percent complete: 87.5%; Average loss: 0.8728\n",
            "Iteration: 28861; Percent complete: 87.5%; Average loss: 0.6912\n",
            "Iteration: 28862; Percent complete: 87.5%; Average loss: 0.7243\n",
            "Iteration: 28863; Percent complete: 87.5%; Average loss: 0.8754\n",
            "Iteration: 28864; Percent complete: 87.5%; Average loss: 0.7923\n",
            "Iteration: 28865; Percent complete: 87.5%; Average loss: 0.6894\n",
            "Iteration: 28866; Percent complete: 87.5%; Average loss: 0.7362\n",
            "Iteration: 28867; Percent complete: 87.5%; Average loss: 0.7844\n",
            "Iteration: 28868; Percent complete: 87.5%; Average loss: 0.7227\n",
            "Iteration: 28869; Percent complete: 87.5%; Average loss: 0.8058\n",
            "Iteration: 28870; Percent complete: 87.5%; Average loss: 0.7396\n",
            "Iteration: 28871; Percent complete: 87.5%; Average loss: 0.6358\n",
            "Iteration: 28872; Percent complete: 87.5%; Average loss: 0.8023\n",
            "Iteration: 28873; Percent complete: 87.5%; Average loss: 0.8124\n",
            "Iteration: 28874; Percent complete: 87.5%; Average loss: 0.8608\n",
            "Iteration: 28875; Percent complete: 87.5%; Average loss: 0.9415\n",
            "Iteration: 28876; Percent complete: 87.5%; Average loss: 0.6952\n",
            "Iteration: 28877; Percent complete: 87.5%; Average loss: 0.7974\n",
            "Iteration: 28878; Percent complete: 87.5%; Average loss: 0.7613\n",
            "Iteration: 28879; Percent complete: 87.5%; Average loss: 0.8278\n",
            "Iteration: 28880; Percent complete: 87.5%; Average loss: 0.5741\n",
            "Iteration: 28881; Percent complete: 87.5%; Average loss: 0.7195\n",
            "Iteration: 28882; Percent complete: 87.5%; Average loss: 0.6851\n",
            "Iteration: 28883; Percent complete: 87.5%; Average loss: 0.6576\n",
            "Iteration: 28884; Percent complete: 87.5%; Average loss: 0.8927\n",
            "Iteration: 28885; Percent complete: 87.5%; Average loss: 0.7029\n",
            "Iteration: 28886; Percent complete: 87.5%; Average loss: 0.8111\n",
            "Iteration: 28887; Percent complete: 87.5%; Average loss: 0.8481\n",
            "Iteration: 28888; Percent complete: 87.5%; Average loss: 0.7486\n",
            "Iteration: 28889; Percent complete: 87.5%; Average loss: 0.9014\n",
            "Iteration: 28890; Percent complete: 87.5%; Average loss: 0.7344\n",
            "Iteration: 28891; Percent complete: 87.5%; Average loss: 0.6782\n",
            "Iteration: 28892; Percent complete: 87.6%; Average loss: 0.8165\n",
            "Iteration: 28893; Percent complete: 87.6%; Average loss: 0.6281\n",
            "Iteration: 28894; Percent complete: 87.6%; Average loss: 0.8690\n",
            "Iteration: 28895; Percent complete: 87.6%; Average loss: 0.7940\n",
            "Iteration: 28896; Percent complete: 87.6%; Average loss: 0.7863\n",
            "Iteration: 28897; Percent complete: 87.6%; Average loss: 0.7229\n",
            "Iteration: 28898; Percent complete: 87.6%; Average loss: 0.9227\n",
            "Iteration: 28899; Percent complete: 87.6%; Average loss: 0.9378\n",
            "Iteration: 28900; Percent complete: 87.6%; Average loss: 0.6422\n",
            "Iteration: 28901; Percent complete: 87.6%; Average loss: 0.7497\n",
            "Iteration: 28902; Percent complete: 87.6%; Average loss: 0.6622\n",
            "Iteration: 28903; Percent complete: 87.6%; Average loss: 0.7164\n",
            "Iteration: 28904; Percent complete: 87.6%; Average loss: 0.7990\n",
            "Iteration: 28905; Percent complete: 87.6%; Average loss: 0.7848\n",
            "Iteration: 28906; Percent complete: 87.6%; Average loss: 0.8833\n",
            "Iteration: 28907; Percent complete: 87.6%; Average loss: 0.6871\n",
            "Iteration: 28908; Percent complete: 87.6%; Average loss: 0.8812\n",
            "Iteration: 28909; Percent complete: 87.6%; Average loss: 0.8420\n",
            "Iteration: 28910; Percent complete: 87.6%; Average loss: 0.6781\n",
            "Iteration: 28911; Percent complete: 87.6%; Average loss: 0.6899\n",
            "Iteration: 28912; Percent complete: 87.6%; Average loss: 0.8426\n",
            "Iteration: 28913; Percent complete: 87.6%; Average loss: 0.8214\n",
            "Iteration: 28914; Percent complete: 87.6%; Average loss: 0.8855\n",
            "Iteration: 28915; Percent complete: 87.6%; Average loss: 0.7535\n",
            "Iteration: 28916; Percent complete: 87.6%; Average loss: 0.7869\n",
            "Iteration: 28917; Percent complete: 87.6%; Average loss: 0.7652\n",
            "Iteration: 28918; Percent complete: 87.6%; Average loss: 0.8131\n",
            "Iteration: 28919; Percent complete: 87.6%; Average loss: 0.6433\n",
            "Iteration: 28920; Percent complete: 87.6%; Average loss: 0.7132\n",
            "Iteration: 28921; Percent complete: 87.6%; Average loss: 0.8475\n",
            "Iteration: 28922; Percent complete: 87.6%; Average loss: 0.7840\n",
            "Iteration: 28923; Percent complete: 87.6%; Average loss: 0.8426\n",
            "Iteration: 28924; Percent complete: 87.6%; Average loss: 0.8269\n",
            "Iteration: 28925; Percent complete: 87.7%; Average loss: 0.7946\n",
            "Iteration: 28926; Percent complete: 87.7%; Average loss: 0.6970\n",
            "Iteration: 28927; Percent complete: 87.7%; Average loss: 0.7721\n",
            "Iteration: 28928; Percent complete: 87.7%; Average loss: 0.7795\n",
            "Iteration: 28929; Percent complete: 87.7%; Average loss: 0.7943\n",
            "Iteration: 28930; Percent complete: 87.7%; Average loss: 0.9684\n",
            "Iteration: 28931; Percent complete: 87.7%; Average loss: 0.7534\n",
            "Iteration: 28932; Percent complete: 87.7%; Average loss: 0.7705\n",
            "Iteration: 28933; Percent complete: 87.7%; Average loss: 1.0417\n",
            "Iteration: 28934; Percent complete: 87.7%; Average loss: 0.6897\n",
            "Iteration: 28935; Percent complete: 87.7%; Average loss: 0.7593\n",
            "Iteration: 28936; Percent complete: 87.7%; Average loss: 0.6688\n",
            "Iteration: 28937; Percent complete: 87.7%; Average loss: 0.8597\n",
            "Iteration: 28938; Percent complete: 87.7%; Average loss: 0.7191\n",
            "Iteration: 28939; Percent complete: 87.7%; Average loss: 0.6874\n",
            "Iteration: 28940; Percent complete: 87.7%; Average loss: 0.7966\n",
            "Iteration: 28941; Percent complete: 87.7%; Average loss: 0.7802\n",
            "Iteration: 28942; Percent complete: 87.7%; Average loss: 0.7676\n",
            "Iteration: 28943; Percent complete: 87.7%; Average loss: 0.7021\n",
            "Iteration: 28944; Percent complete: 87.7%; Average loss: 0.7346\n",
            "Iteration: 28945; Percent complete: 87.7%; Average loss: 0.7381\n",
            "Iteration: 28946; Percent complete: 87.7%; Average loss: 0.6553\n",
            "Iteration: 28947; Percent complete: 87.7%; Average loss: 0.6469\n",
            "Iteration: 28948; Percent complete: 87.7%; Average loss: 0.7181\n",
            "Iteration: 28949; Percent complete: 87.7%; Average loss: 0.7558\n",
            "Iteration: 28950; Percent complete: 87.7%; Average loss: 0.7526\n",
            "Iteration: 28951; Percent complete: 87.7%; Average loss: 0.7544\n",
            "Iteration: 28952; Percent complete: 87.7%; Average loss: 0.7731\n",
            "Iteration: 28953; Percent complete: 87.7%; Average loss: 0.7012\n",
            "Iteration: 28954; Percent complete: 87.7%; Average loss: 0.7312\n",
            "Iteration: 28955; Percent complete: 87.7%; Average loss: 0.6125\n",
            "Iteration: 28956; Percent complete: 87.7%; Average loss: 0.8828\n",
            "Iteration: 28957; Percent complete: 87.7%; Average loss: 0.6637\n",
            "Iteration: 28958; Percent complete: 87.8%; Average loss: 0.7199\n",
            "Iteration: 28959; Percent complete: 87.8%; Average loss: 0.8108\n",
            "Iteration: 28960; Percent complete: 87.8%; Average loss: 0.7368\n",
            "Iteration: 28961; Percent complete: 87.8%; Average loss: 0.8139\n",
            "Iteration: 28962; Percent complete: 87.8%; Average loss: 0.8291\n",
            "Iteration: 28963; Percent complete: 87.8%; Average loss: 0.7622\n",
            "Iteration: 28964; Percent complete: 87.8%; Average loss: 0.6323\n",
            "Iteration: 28965; Percent complete: 87.8%; Average loss: 0.8316\n",
            "Iteration: 28966; Percent complete: 87.8%; Average loss: 0.7321\n",
            "Iteration: 28967; Percent complete: 87.8%; Average loss: 0.6817\n",
            "Iteration: 28968; Percent complete: 87.8%; Average loss: 0.6541\n",
            "Iteration: 28969; Percent complete: 87.8%; Average loss: 0.7851\n",
            "Iteration: 28970; Percent complete: 87.8%; Average loss: 0.6979\n",
            "Iteration: 28971; Percent complete: 87.8%; Average loss: 0.7229\n",
            "Iteration: 28972; Percent complete: 87.8%; Average loss: 0.8695\n",
            "Iteration: 28973; Percent complete: 87.8%; Average loss: 0.7548\n",
            "Iteration: 28974; Percent complete: 87.8%; Average loss: 0.7859\n",
            "Iteration: 28975; Percent complete: 87.8%; Average loss: 0.7136\n",
            "Iteration: 28976; Percent complete: 87.8%; Average loss: 0.6768\n",
            "Iteration: 28977; Percent complete: 87.8%; Average loss: 0.7022\n",
            "Iteration: 28978; Percent complete: 87.8%; Average loss: 0.6790\n",
            "Iteration: 28979; Percent complete: 87.8%; Average loss: 0.7089\n",
            "Iteration: 28980; Percent complete: 87.8%; Average loss: 0.8472\n",
            "Iteration: 28981; Percent complete: 87.8%; Average loss: 0.6298\n",
            "Iteration: 28982; Percent complete: 87.8%; Average loss: 0.8309\n",
            "Iteration: 28983; Percent complete: 87.8%; Average loss: 0.7298\n",
            "Iteration: 28984; Percent complete: 87.8%; Average loss: 0.8378\n",
            "Iteration: 28985; Percent complete: 87.8%; Average loss: 0.5999\n",
            "Iteration: 28986; Percent complete: 87.8%; Average loss: 0.7603\n",
            "Iteration: 28987; Percent complete: 87.8%; Average loss: 0.7165\n",
            "Iteration: 28988; Percent complete: 87.8%; Average loss: 0.8922\n",
            "Iteration: 28989; Percent complete: 87.8%; Average loss: 0.7064\n",
            "Iteration: 28990; Percent complete: 87.8%; Average loss: 0.6746\n",
            "Iteration: 28991; Percent complete: 87.9%; Average loss: 0.7354\n",
            "Iteration: 28992; Percent complete: 87.9%; Average loss: 0.7561\n",
            "Iteration: 28993; Percent complete: 87.9%; Average loss: 0.6766\n",
            "Iteration: 28994; Percent complete: 87.9%; Average loss: 0.8920\n",
            "Iteration: 28995; Percent complete: 87.9%; Average loss: 0.8844\n",
            "Iteration: 28996; Percent complete: 87.9%; Average loss: 0.9020\n",
            "Iteration: 28997; Percent complete: 87.9%; Average loss: 0.7388\n",
            "Iteration: 28998; Percent complete: 87.9%; Average loss: 0.9988\n",
            "Iteration: 28999; Percent complete: 87.9%; Average loss: 0.8398\n",
            "Iteration: 29000; Percent complete: 87.9%; Average loss: 0.6760\n",
            "Iteration: 29001; Percent complete: 87.9%; Average loss: 0.6431\n",
            "Iteration: 29002; Percent complete: 87.9%; Average loss: 0.7985\n",
            "Iteration: 29003; Percent complete: 87.9%; Average loss: 0.7933\n",
            "Iteration: 29004; Percent complete: 87.9%; Average loss: 0.7761\n",
            "Iteration: 29005; Percent complete: 87.9%; Average loss: 0.7378\n",
            "Iteration: 29006; Percent complete: 87.9%; Average loss: 0.8178\n",
            "Iteration: 29007; Percent complete: 87.9%; Average loss: 0.6961\n",
            "Iteration: 29008; Percent complete: 87.9%; Average loss: 0.7264\n",
            "Iteration: 29009; Percent complete: 87.9%; Average loss: 0.9014\n",
            "Iteration: 29010; Percent complete: 87.9%; Average loss: 0.8068\n",
            "Iteration: 29011; Percent complete: 87.9%; Average loss: 0.8765\n",
            "Iteration: 29012; Percent complete: 87.9%; Average loss: 0.8717\n",
            "Iteration: 29013; Percent complete: 87.9%; Average loss: 0.6800\n",
            "Iteration: 29014; Percent complete: 87.9%; Average loss: 0.6216\n",
            "Iteration: 29015; Percent complete: 87.9%; Average loss: 0.7395\n",
            "Iteration: 29016; Percent complete: 87.9%; Average loss: 0.5980\n",
            "Iteration: 29017; Percent complete: 87.9%; Average loss: 0.7883\n",
            "Iteration: 29018; Percent complete: 87.9%; Average loss: 0.7265\n",
            "Iteration: 29019; Percent complete: 87.9%; Average loss: 0.7516\n",
            "Iteration: 29020; Percent complete: 87.9%; Average loss: 0.8077\n",
            "Iteration: 29021; Percent complete: 87.9%; Average loss: 0.8043\n",
            "Iteration: 29022; Percent complete: 87.9%; Average loss: 0.6947\n",
            "Iteration: 29023; Percent complete: 87.9%; Average loss: 0.7598\n",
            "Iteration: 29024; Percent complete: 88.0%; Average loss: 0.6793\n",
            "Iteration: 29025; Percent complete: 88.0%; Average loss: 0.9235\n",
            "Iteration: 29026; Percent complete: 88.0%; Average loss: 0.6231\n",
            "Iteration: 29027; Percent complete: 88.0%; Average loss: 0.6970\n",
            "Iteration: 29028; Percent complete: 88.0%; Average loss: 0.8859\n",
            "Iteration: 29029; Percent complete: 88.0%; Average loss: 0.8464\n",
            "Iteration: 29030; Percent complete: 88.0%; Average loss: 0.6634\n",
            "Iteration: 29031; Percent complete: 88.0%; Average loss: 0.6747\n",
            "Iteration: 29032; Percent complete: 88.0%; Average loss: 0.6311\n",
            "Iteration: 29033; Percent complete: 88.0%; Average loss: 0.7656\n",
            "Iteration: 29034; Percent complete: 88.0%; Average loss: 0.8734\n",
            "Iteration: 29035; Percent complete: 88.0%; Average loss: 0.6819\n",
            "Iteration: 29036; Percent complete: 88.0%; Average loss: 0.6987\n",
            "Iteration: 29037; Percent complete: 88.0%; Average loss: 0.6222\n",
            "Iteration: 29038; Percent complete: 88.0%; Average loss: 0.6401\n",
            "Iteration: 29039; Percent complete: 88.0%; Average loss: 0.7044\n",
            "Iteration: 29040; Percent complete: 88.0%; Average loss: 0.6744\n",
            "Iteration: 29041; Percent complete: 88.0%; Average loss: 0.6649\n",
            "Iteration: 29042; Percent complete: 88.0%; Average loss: 0.7206\n",
            "Iteration: 29043; Percent complete: 88.0%; Average loss: 0.8354\n",
            "Iteration: 29044; Percent complete: 88.0%; Average loss: 0.7449\n",
            "Iteration: 29045; Percent complete: 88.0%; Average loss: 0.8219\n",
            "Iteration: 29046; Percent complete: 88.0%; Average loss: 0.6631\n",
            "Iteration: 29047; Percent complete: 88.0%; Average loss: 0.6953\n",
            "Iteration: 29048; Percent complete: 88.0%; Average loss: 0.8314\n",
            "Iteration: 29049; Percent complete: 88.0%; Average loss: 0.8670\n",
            "Iteration: 29050; Percent complete: 88.0%; Average loss: 0.7646\n",
            "Iteration: 29051; Percent complete: 88.0%; Average loss: 0.6935\n",
            "Iteration: 29052; Percent complete: 88.0%; Average loss: 0.9895\n",
            "Iteration: 29053; Percent complete: 88.0%; Average loss: 0.7435\n",
            "Iteration: 29054; Percent complete: 88.0%; Average loss: 0.7603\n",
            "Iteration: 29055; Percent complete: 88.0%; Average loss: 0.8513\n",
            "Iteration: 29056; Percent complete: 88.0%; Average loss: 0.7772\n",
            "Iteration: 29057; Percent complete: 88.1%; Average loss: 0.6720\n",
            "Iteration: 29058; Percent complete: 88.1%; Average loss: 0.7341\n",
            "Iteration: 29059; Percent complete: 88.1%; Average loss: 0.9915\n",
            "Iteration: 29060; Percent complete: 88.1%; Average loss: 0.7046\n",
            "Iteration: 29061; Percent complete: 88.1%; Average loss: 0.6968\n",
            "Iteration: 29062; Percent complete: 88.1%; Average loss: 0.6900\n",
            "Iteration: 29063; Percent complete: 88.1%; Average loss: 0.8523\n",
            "Iteration: 29064; Percent complete: 88.1%; Average loss: 0.6567\n",
            "Iteration: 29065; Percent complete: 88.1%; Average loss: 0.7039\n",
            "Iteration: 29066; Percent complete: 88.1%; Average loss: 0.8950\n",
            "Iteration: 29067; Percent complete: 88.1%; Average loss: 0.6751\n",
            "Iteration: 29068; Percent complete: 88.1%; Average loss: 0.7017\n",
            "Iteration: 29069; Percent complete: 88.1%; Average loss: 0.8130\n",
            "Iteration: 29070; Percent complete: 88.1%; Average loss: 0.8418\n",
            "Iteration: 29071; Percent complete: 88.1%; Average loss: 0.7173\n",
            "Iteration: 29072; Percent complete: 88.1%; Average loss: 0.7194\n",
            "Iteration: 29073; Percent complete: 88.1%; Average loss: 0.6505\n",
            "Iteration: 29074; Percent complete: 88.1%; Average loss: 0.6681\n",
            "Iteration: 29075; Percent complete: 88.1%; Average loss: 0.7390\n",
            "Iteration: 29076; Percent complete: 88.1%; Average loss: 0.6995\n",
            "Iteration: 29077; Percent complete: 88.1%; Average loss: 0.8681\n",
            "Iteration: 29078; Percent complete: 88.1%; Average loss: 0.7015\n",
            "Iteration: 29079; Percent complete: 88.1%; Average loss: 0.6984\n",
            "Iteration: 29080; Percent complete: 88.1%; Average loss: 0.8863\n",
            "Iteration: 29081; Percent complete: 88.1%; Average loss: 0.6199\n",
            "Iteration: 29082; Percent complete: 88.1%; Average loss: 0.7453\n",
            "Iteration: 29083; Percent complete: 88.1%; Average loss: 0.6883\n",
            "Iteration: 29084; Percent complete: 88.1%; Average loss: 0.8171\n",
            "Iteration: 29085; Percent complete: 88.1%; Average loss: 0.6853\n",
            "Iteration: 29086; Percent complete: 88.1%; Average loss: 0.7692\n",
            "Iteration: 29087; Percent complete: 88.1%; Average loss: 0.6953\n",
            "Iteration: 29088; Percent complete: 88.1%; Average loss: 0.8152\n",
            "Iteration: 29089; Percent complete: 88.1%; Average loss: 0.8023\n",
            "Iteration: 29090; Percent complete: 88.2%; Average loss: 0.5992\n",
            "Iteration: 29091; Percent complete: 88.2%; Average loss: 0.5896\n",
            "Iteration: 29092; Percent complete: 88.2%; Average loss: 0.6581\n",
            "Iteration: 29093; Percent complete: 88.2%; Average loss: 0.7994\n",
            "Iteration: 29094; Percent complete: 88.2%; Average loss: 0.8036\n",
            "Iteration: 29095; Percent complete: 88.2%; Average loss: 0.8068\n",
            "Iteration: 29096; Percent complete: 88.2%; Average loss: 0.6050\n",
            "Iteration: 29097; Percent complete: 88.2%; Average loss: 0.9100\n",
            "Iteration: 29098; Percent complete: 88.2%; Average loss: 0.9062\n",
            "Iteration: 29099; Percent complete: 88.2%; Average loss: 0.7529\n",
            "Iteration: 29100; Percent complete: 88.2%; Average loss: 0.8451\n",
            "Iteration: 29101; Percent complete: 88.2%; Average loss: 0.8039\n",
            "Iteration: 29102; Percent complete: 88.2%; Average loss: 0.7645\n",
            "Iteration: 29103; Percent complete: 88.2%; Average loss: 0.6841\n",
            "Iteration: 29104; Percent complete: 88.2%; Average loss: 0.7585\n",
            "Iteration: 29105; Percent complete: 88.2%; Average loss: 0.6552\n",
            "Iteration: 29106; Percent complete: 88.2%; Average loss: 0.6494\n",
            "Iteration: 29107; Percent complete: 88.2%; Average loss: 0.6266\n",
            "Iteration: 29108; Percent complete: 88.2%; Average loss: 0.7823\n",
            "Iteration: 29109; Percent complete: 88.2%; Average loss: 0.8668\n",
            "Iteration: 29110; Percent complete: 88.2%; Average loss: 0.6591\n",
            "Iteration: 29111; Percent complete: 88.2%; Average loss: 0.6974\n",
            "Iteration: 29112; Percent complete: 88.2%; Average loss: 0.8197\n",
            "Iteration: 29113; Percent complete: 88.2%; Average loss: 0.7851\n",
            "Iteration: 29114; Percent complete: 88.2%; Average loss: 0.9489\n",
            "Iteration: 29115; Percent complete: 88.2%; Average loss: 0.7471\n",
            "Iteration: 29116; Percent complete: 88.2%; Average loss: 0.7315\n",
            "Iteration: 29117; Percent complete: 88.2%; Average loss: 0.7621\n",
            "Iteration: 29118; Percent complete: 88.2%; Average loss: 0.6877\n",
            "Iteration: 29119; Percent complete: 88.2%; Average loss: 0.9123\n",
            "Iteration: 29120; Percent complete: 88.2%; Average loss: 0.6751\n",
            "Iteration: 29121; Percent complete: 88.2%; Average loss: 0.7037\n",
            "Iteration: 29122; Percent complete: 88.2%; Average loss: 0.7094\n",
            "Iteration: 29123; Percent complete: 88.3%; Average loss: 0.6702\n",
            "Iteration: 29124; Percent complete: 88.3%; Average loss: 0.7580\n",
            "Iteration: 29125; Percent complete: 88.3%; Average loss: 0.6759\n",
            "Iteration: 29126; Percent complete: 88.3%; Average loss: 0.8061\n",
            "Iteration: 29127; Percent complete: 88.3%; Average loss: 0.7122\n",
            "Iteration: 29128; Percent complete: 88.3%; Average loss: 0.8047\n",
            "Iteration: 29129; Percent complete: 88.3%; Average loss: 0.9026\n",
            "Iteration: 29130; Percent complete: 88.3%; Average loss: 0.6738\n",
            "Iteration: 29131; Percent complete: 88.3%; Average loss: 0.8549\n",
            "Iteration: 29132; Percent complete: 88.3%; Average loss: 0.8536\n",
            "Iteration: 29133; Percent complete: 88.3%; Average loss: 0.7802\n",
            "Iteration: 29134; Percent complete: 88.3%; Average loss: 0.8086\n",
            "Iteration: 29135; Percent complete: 88.3%; Average loss: 0.8159\n",
            "Iteration: 29136; Percent complete: 88.3%; Average loss: 0.8598\n",
            "Iteration: 29137; Percent complete: 88.3%; Average loss: 0.6685\n",
            "Iteration: 29138; Percent complete: 88.3%; Average loss: 0.6990\n",
            "Iteration: 29139; Percent complete: 88.3%; Average loss: 0.7990\n",
            "Iteration: 29140; Percent complete: 88.3%; Average loss: 0.7729\n",
            "Iteration: 29141; Percent complete: 88.3%; Average loss: 0.6431\n",
            "Iteration: 29142; Percent complete: 88.3%; Average loss: 0.7807\n",
            "Iteration: 29143; Percent complete: 88.3%; Average loss: 0.7837\n",
            "Iteration: 29144; Percent complete: 88.3%; Average loss: 0.6983\n",
            "Iteration: 29145; Percent complete: 88.3%; Average loss: 0.7570\n",
            "Iteration: 29146; Percent complete: 88.3%; Average loss: 0.7228\n",
            "Iteration: 29147; Percent complete: 88.3%; Average loss: 0.7182\n",
            "Iteration: 29148; Percent complete: 88.3%; Average loss: 0.8201\n",
            "Iteration: 29149; Percent complete: 88.3%; Average loss: 0.8310\n",
            "Iteration: 29150; Percent complete: 88.3%; Average loss: 0.8653\n",
            "Iteration: 29151; Percent complete: 88.3%; Average loss: 0.8318\n",
            "Iteration: 29152; Percent complete: 88.3%; Average loss: 0.8973\n",
            "Iteration: 29153; Percent complete: 88.3%; Average loss: 0.6557\n",
            "Iteration: 29154; Percent complete: 88.3%; Average loss: 0.7573\n",
            "Iteration: 29155; Percent complete: 88.3%; Average loss: 0.6442\n",
            "Iteration: 29156; Percent complete: 88.4%; Average loss: 0.6664\n",
            "Iteration: 29157; Percent complete: 88.4%; Average loss: 0.7490\n",
            "Iteration: 29158; Percent complete: 88.4%; Average loss: 0.8299\n",
            "Iteration: 29159; Percent complete: 88.4%; Average loss: 0.7209\n",
            "Iteration: 29160; Percent complete: 88.4%; Average loss: 0.5762\n",
            "Iteration: 29161; Percent complete: 88.4%; Average loss: 0.9394\n",
            "Iteration: 29162; Percent complete: 88.4%; Average loss: 0.7614\n",
            "Iteration: 29163; Percent complete: 88.4%; Average loss: 0.7051\n",
            "Iteration: 29164; Percent complete: 88.4%; Average loss: 0.8902\n",
            "Iteration: 29165; Percent complete: 88.4%; Average loss: 0.7180\n",
            "Iteration: 29166; Percent complete: 88.4%; Average loss: 0.6072\n",
            "Iteration: 29167; Percent complete: 88.4%; Average loss: 0.7471\n",
            "Iteration: 29168; Percent complete: 88.4%; Average loss: 0.5850\n",
            "Iteration: 29169; Percent complete: 88.4%; Average loss: 0.7468\n",
            "Iteration: 29170; Percent complete: 88.4%; Average loss: 0.7585\n",
            "Iteration: 29171; Percent complete: 88.4%; Average loss: 0.5743\n",
            "Iteration: 29172; Percent complete: 88.4%; Average loss: 0.5925\n",
            "Iteration: 29173; Percent complete: 88.4%; Average loss: 0.5965\n",
            "Iteration: 29174; Percent complete: 88.4%; Average loss: 0.6557\n",
            "Iteration: 29175; Percent complete: 88.4%; Average loss: 0.7138\n",
            "Iteration: 29176; Percent complete: 88.4%; Average loss: 0.8131\n",
            "Iteration: 29177; Percent complete: 88.4%; Average loss: 0.6246\n",
            "Iteration: 29178; Percent complete: 88.4%; Average loss: 0.6650\n",
            "Iteration: 29179; Percent complete: 88.4%; Average loss: 0.9937\n",
            "Iteration: 29180; Percent complete: 88.4%; Average loss: 0.7855\n",
            "Iteration: 29181; Percent complete: 88.4%; Average loss: 0.6073\n",
            "Iteration: 29182; Percent complete: 88.4%; Average loss: 0.8834\n",
            "Iteration: 29183; Percent complete: 88.4%; Average loss: 0.7821\n",
            "Iteration: 29184; Percent complete: 88.4%; Average loss: 0.6958\n",
            "Iteration: 29185; Percent complete: 88.4%; Average loss: 0.6309\n",
            "Iteration: 29186; Percent complete: 88.4%; Average loss: 0.7519\n",
            "Iteration: 29187; Percent complete: 88.4%; Average loss: 0.7004\n",
            "Iteration: 29188; Percent complete: 88.4%; Average loss: 0.8313\n",
            "Iteration: 29189; Percent complete: 88.5%; Average loss: 0.7503\n",
            "Iteration: 29190; Percent complete: 88.5%; Average loss: 0.6991\n",
            "Iteration: 29191; Percent complete: 88.5%; Average loss: 0.7187\n",
            "Iteration: 29192; Percent complete: 88.5%; Average loss: 0.6211\n",
            "Iteration: 29193; Percent complete: 88.5%; Average loss: 1.0194\n",
            "Iteration: 29194; Percent complete: 88.5%; Average loss: 0.7661\n",
            "Iteration: 29195; Percent complete: 88.5%; Average loss: 0.7824\n",
            "Iteration: 29196; Percent complete: 88.5%; Average loss: 0.7196\n",
            "Iteration: 29197; Percent complete: 88.5%; Average loss: 0.5815\n",
            "Iteration: 29198; Percent complete: 88.5%; Average loss: 0.7291\n",
            "Iteration: 29199; Percent complete: 88.5%; Average loss: 0.7001\n",
            "Iteration: 29200; Percent complete: 88.5%; Average loss: 0.6882\n",
            "Iteration: 29201; Percent complete: 88.5%; Average loss: 0.7974\n",
            "Iteration: 29202; Percent complete: 88.5%; Average loss: 0.8668\n",
            "Iteration: 29203; Percent complete: 88.5%; Average loss: 0.7531\n",
            "Iteration: 29204; Percent complete: 88.5%; Average loss: 0.7215\n",
            "Iteration: 29205; Percent complete: 88.5%; Average loss: 0.7901\n",
            "Iteration: 29206; Percent complete: 88.5%; Average loss: 0.7190\n",
            "Iteration: 29207; Percent complete: 88.5%; Average loss: 0.6858\n",
            "Iteration: 29208; Percent complete: 88.5%; Average loss: 0.7861\n",
            "Iteration: 29209; Percent complete: 88.5%; Average loss: 0.8575\n",
            "Iteration: 29210; Percent complete: 88.5%; Average loss: 0.7551\n",
            "Iteration: 29211; Percent complete: 88.5%; Average loss: 0.7910\n",
            "Iteration: 29212; Percent complete: 88.5%; Average loss: 0.6838\n",
            "Iteration: 29213; Percent complete: 88.5%; Average loss: 0.8510\n",
            "Iteration: 29214; Percent complete: 88.5%; Average loss: 0.6652\n",
            "Iteration: 29215; Percent complete: 88.5%; Average loss: 0.8065\n",
            "Iteration: 29216; Percent complete: 88.5%; Average loss: 0.7122\n",
            "Iteration: 29217; Percent complete: 88.5%; Average loss: 0.7862\n",
            "Iteration: 29218; Percent complete: 88.5%; Average loss: 0.6434\n",
            "Iteration: 29219; Percent complete: 88.5%; Average loss: 0.6447\n",
            "Iteration: 29220; Percent complete: 88.5%; Average loss: 0.6526\n",
            "Iteration: 29221; Percent complete: 88.5%; Average loss: 0.8456\n",
            "Iteration: 29222; Percent complete: 88.6%; Average loss: 0.6232\n",
            "Iteration: 29223; Percent complete: 88.6%; Average loss: 0.8883\n",
            "Iteration: 29224; Percent complete: 88.6%; Average loss: 0.7381\n",
            "Iteration: 29225; Percent complete: 88.6%; Average loss: 0.7740\n",
            "Iteration: 29226; Percent complete: 88.6%; Average loss: 0.7031\n",
            "Iteration: 29227; Percent complete: 88.6%; Average loss: 0.7747\n",
            "Iteration: 29228; Percent complete: 88.6%; Average loss: 0.6687\n",
            "Iteration: 29229; Percent complete: 88.6%; Average loss: 0.6719\n",
            "Iteration: 29230; Percent complete: 88.6%; Average loss: 0.7377\n",
            "Iteration: 29231; Percent complete: 88.6%; Average loss: 0.6794\n",
            "Iteration: 29232; Percent complete: 88.6%; Average loss: 0.7917\n",
            "Iteration: 29233; Percent complete: 88.6%; Average loss: 0.8763\n",
            "Iteration: 29234; Percent complete: 88.6%; Average loss: 0.8886\n",
            "Iteration: 29235; Percent complete: 88.6%; Average loss: 0.6826\n",
            "Iteration: 29236; Percent complete: 88.6%; Average loss: 0.5913\n",
            "Iteration: 29237; Percent complete: 88.6%; Average loss: 0.7456\n",
            "Iteration: 29238; Percent complete: 88.6%; Average loss: 0.7868\n",
            "Iteration: 29239; Percent complete: 88.6%; Average loss: 0.7980\n",
            "Iteration: 29240; Percent complete: 88.6%; Average loss: 0.6785\n",
            "Iteration: 29241; Percent complete: 88.6%; Average loss: 0.8971\n",
            "Iteration: 29242; Percent complete: 88.6%; Average loss: 0.6795\n",
            "Iteration: 29243; Percent complete: 88.6%; Average loss: 0.8206\n",
            "Iteration: 29244; Percent complete: 88.6%; Average loss: 0.7644\n",
            "Iteration: 29245; Percent complete: 88.6%; Average loss: 0.5965\n",
            "Iteration: 29246; Percent complete: 88.6%; Average loss: 0.6581\n",
            "Iteration: 29247; Percent complete: 88.6%; Average loss: 0.8043\n",
            "Iteration: 29248; Percent complete: 88.6%; Average loss: 0.7879\n",
            "Iteration: 29249; Percent complete: 88.6%; Average loss: 0.7062\n",
            "Iteration: 29250; Percent complete: 88.6%; Average loss: 0.7335\n",
            "Iteration: 29251; Percent complete: 88.6%; Average loss: 0.7468\n",
            "Iteration: 29252; Percent complete: 88.6%; Average loss: 0.8293\n",
            "Iteration: 29253; Percent complete: 88.6%; Average loss: 0.5861\n",
            "Iteration: 29254; Percent complete: 88.6%; Average loss: 0.7859\n",
            "Iteration: 29255; Percent complete: 88.7%; Average loss: 0.7447\n",
            "Iteration: 29256; Percent complete: 88.7%; Average loss: 0.5579\n",
            "Iteration: 29257; Percent complete: 88.7%; Average loss: 0.6737\n",
            "Iteration: 29258; Percent complete: 88.7%; Average loss: 0.6750\n",
            "Iteration: 29259; Percent complete: 88.7%; Average loss: 0.6924\n",
            "Iteration: 29260; Percent complete: 88.7%; Average loss: 0.8615\n",
            "Iteration: 29261; Percent complete: 88.7%; Average loss: 0.7439\n",
            "Iteration: 29262; Percent complete: 88.7%; Average loss: 0.6743\n",
            "Iteration: 29263; Percent complete: 88.7%; Average loss: 0.6998\n",
            "Iteration: 29264; Percent complete: 88.7%; Average loss: 0.8276\n",
            "Iteration: 29265; Percent complete: 88.7%; Average loss: 0.7685\n",
            "Iteration: 29266; Percent complete: 88.7%; Average loss: 0.8141\n",
            "Iteration: 29267; Percent complete: 88.7%; Average loss: 0.6011\n",
            "Iteration: 29268; Percent complete: 88.7%; Average loss: 0.7630\n",
            "Iteration: 29269; Percent complete: 88.7%; Average loss: 0.6975\n",
            "Iteration: 29270; Percent complete: 88.7%; Average loss: 0.7592\n",
            "Iteration: 29271; Percent complete: 88.7%; Average loss: 0.8503\n",
            "Iteration: 29272; Percent complete: 88.7%; Average loss: 0.9486\n",
            "Iteration: 29273; Percent complete: 88.7%; Average loss: 0.8221\n",
            "Iteration: 29274; Percent complete: 88.7%; Average loss: 0.7975\n",
            "Iteration: 29275; Percent complete: 88.7%; Average loss: 0.6820\n",
            "Iteration: 29276; Percent complete: 88.7%; Average loss: 0.8751\n",
            "Iteration: 29277; Percent complete: 88.7%; Average loss: 0.7970\n",
            "Iteration: 29278; Percent complete: 88.7%; Average loss: 0.7309\n",
            "Iteration: 29279; Percent complete: 88.7%; Average loss: 0.8028\n",
            "Iteration: 29280; Percent complete: 88.7%; Average loss: 0.6119\n",
            "Iteration: 29281; Percent complete: 88.7%; Average loss: 0.7255\n",
            "Iteration: 29282; Percent complete: 88.7%; Average loss: 0.8174\n",
            "Iteration: 29283; Percent complete: 88.7%; Average loss: 0.6958\n",
            "Iteration: 29284; Percent complete: 88.7%; Average loss: 0.7766\n",
            "Iteration: 29285; Percent complete: 88.7%; Average loss: 0.7011\n",
            "Iteration: 29286; Percent complete: 88.7%; Average loss: 0.6605\n",
            "Iteration: 29287; Percent complete: 88.7%; Average loss: 0.7136\n",
            "Iteration: 29288; Percent complete: 88.8%; Average loss: 0.6385\n",
            "Iteration: 29289; Percent complete: 88.8%; Average loss: 0.7423\n",
            "Iteration: 29290; Percent complete: 88.8%; Average loss: 0.7389\n",
            "Iteration: 29291; Percent complete: 88.8%; Average loss: 0.8339\n",
            "Iteration: 29292; Percent complete: 88.8%; Average loss: 0.6634\n",
            "Iteration: 29293; Percent complete: 88.8%; Average loss: 0.7964\n",
            "Iteration: 29294; Percent complete: 88.8%; Average loss: 0.8702\n",
            "Iteration: 29295; Percent complete: 88.8%; Average loss: 0.8146\n",
            "Iteration: 29296; Percent complete: 88.8%; Average loss: 0.7160\n",
            "Iteration: 29297; Percent complete: 88.8%; Average loss: 0.6885\n",
            "Iteration: 29298; Percent complete: 88.8%; Average loss: 0.8679\n",
            "Iteration: 29299; Percent complete: 88.8%; Average loss: 0.5213\n",
            "Iteration: 29300; Percent complete: 88.8%; Average loss: 0.6911\n",
            "Iteration: 29301; Percent complete: 88.8%; Average loss: 0.9008\n",
            "Iteration: 29302; Percent complete: 88.8%; Average loss: 0.5755\n",
            "Iteration: 29303; Percent complete: 88.8%; Average loss: 0.8336\n",
            "Iteration: 29304; Percent complete: 88.8%; Average loss: 0.7992\n",
            "Iteration: 29305; Percent complete: 88.8%; Average loss: 0.6638\n",
            "Iteration: 29306; Percent complete: 88.8%; Average loss: 0.7678\n",
            "Iteration: 29307; Percent complete: 88.8%; Average loss: 0.7847\n",
            "Iteration: 29308; Percent complete: 88.8%; Average loss: 0.7992\n",
            "Iteration: 29309; Percent complete: 88.8%; Average loss: 0.7176\n",
            "Iteration: 29310; Percent complete: 88.8%; Average loss: 0.6004\n",
            "Iteration: 29311; Percent complete: 88.8%; Average loss: 0.6980\n",
            "Iteration: 29312; Percent complete: 88.8%; Average loss: 0.6975\n",
            "Iteration: 29313; Percent complete: 88.8%; Average loss: 0.7002\n",
            "Iteration: 29314; Percent complete: 88.8%; Average loss: 0.7886\n",
            "Iteration: 29315; Percent complete: 88.8%; Average loss: 0.7709\n",
            "Iteration: 29316; Percent complete: 88.8%; Average loss: 0.8304\n",
            "Iteration: 29317; Percent complete: 88.8%; Average loss: 0.6947\n",
            "Iteration: 29318; Percent complete: 88.8%; Average loss: 0.7545\n",
            "Iteration: 29319; Percent complete: 88.8%; Average loss: 0.7741\n",
            "Iteration: 29320; Percent complete: 88.8%; Average loss: 0.7824\n",
            "Iteration: 29321; Percent complete: 88.9%; Average loss: 1.0127\n",
            "Iteration: 29322; Percent complete: 88.9%; Average loss: 0.7125\n",
            "Iteration: 29323; Percent complete: 88.9%; Average loss: 0.7959\n",
            "Iteration: 29324; Percent complete: 88.9%; Average loss: 0.9988\n",
            "Iteration: 29325; Percent complete: 88.9%; Average loss: 0.6843\n",
            "Iteration: 29326; Percent complete: 88.9%; Average loss: 0.8263\n",
            "Iteration: 29327; Percent complete: 88.9%; Average loss: 0.7628\n",
            "Iteration: 29328; Percent complete: 88.9%; Average loss: 0.5909\n",
            "Iteration: 29329; Percent complete: 88.9%; Average loss: 0.6375\n",
            "Iteration: 29330; Percent complete: 88.9%; Average loss: 0.6265\n",
            "Iteration: 29331; Percent complete: 88.9%; Average loss: 0.7588\n",
            "Iteration: 29332; Percent complete: 88.9%; Average loss: 0.6682\n",
            "Iteration: 29333; Percent complete: 88.9%; Average loss: 0.7002\n",
            "Iteration: 29334; Percent complete: 88.9%; Average loss: 0.8629\n",
            "Iteration: 29335; Percent complete: 88.9%; Average loss: 0.6064\n",
            "Iteration: 29336; Percent complete: 88.9%; Average loss: 0.7213\n",
            "Iteration: 29337; Percent complete: 88.9%; Average loss: 0.6777\n",
            "Iteration: 29338; Percent complete: 88.9%; Average loss: 0.7630\n",
            "Iteration: 29339; Percent complete: 88.9%; Average loss: 0.6398\n",
            "Iteration: 29340; Percent complete: 88.9%; Average loss: 0.7081\n",
            "Iteration: 29341; Percent complete: 88.9%; Average loss: 0.8568\n",
            "Iteration: 29342; Percent complete: 88.9%; Average loss: 0.8443\n",
            "Iteration: 29343; Percent complete: 88.9%; Average loss: 0.7267\n",
            "Iteration: 29344; Percent complete: 88.9%; Average loss: 0.9856\n",
            "Iteration: 29345; Percent complete: 88.9%; Average loss: 0.6786\n",
            "Iteration: 29346; Percent complete: 88.9%; Average loss: 0.7034\n",
            "Iteration: 29347; Percent complete: 88.9%; Average loss: 0.6998\n",
            "Iteration: 29348; Percent complete: 88.9%; Average loss: 0.6674\n",
            "Iteration: 29349; Percent complete: 88.9%; Average loss: 0.8768\n",
            "Iteration: 29350; Percent complete: 88.9%; Average loss: 0.5561\n",
            "Iteration: 29351; Percent complete: 88.9%; Average loss: 0.8408\n",
            "Iteration: 29352; Percent complete: 88.9%; Average loss: 0.7630\n",
            "Iteration: 29353; Percent complete: 88.9%; Average loss: 0.7167\n",
            "Iteration: 29354; Percent complete: 89.0%; Average loss: 0.8442\n",
            "Iteration: 29355; Percent complete: 89.0%; Average loss: 0.6994\n",
            "Iteration: 29356; Percent complete: 89.0%; Average loss: 0.8026\n",
            "Iteration: 29357; Percent complete: 89.0%; Average loss: 0.7458\n",
            "Iteration: 29358; Percent complete: 89.0%; Average loss: 0.7294\n",
            "Iteration: 29359; Percent complete: 89.0%; Average loss: 0.7308\n",
            "Iteration: 29360; Percent complete: 89.0%; Average loss: 0.6430\n",
            "Iteration: 29361; Percent complete: 89.0%; Average loss: 0.8482\n",
            "Iteration: 29362; Percent complete: 89.0%; Average loss: 0.7496\n",
            "Iteration: 29363; Percent complete: 89.0%; Average loss: 0.8756\n",
            "Iteration: 29364; Percent complete: 89.0%; Average loss: 0.7489\n",
            "Iteration: 29365; Percent complete: 89.0%; Average loss: 0.7562\n",
            "Iteration: 29366; Percent complete: 89.0%; Average loss: 0.6643\n",
            "Iteration: 29367; Percent complete: 89.0%; Average loss: 0.7534\n",
            "Iteration: 29368; Percent complete: 89.0%; Average loss: 0.7199\n",
            "Iteration: 29369; Percent complete: 89.0%; Average loss: 0.8271\n",
            "Iteration: 29370; Percent complete: 89.0%; Average loss: 0.6125\n",
            "Iteration: 29371; Percent complete: 89.0%; Average loss: 0.6944\n",
            "Iteration: 29372; Percent complete: 89.0%; Average loss: 0.6865\n",
            "Iteration: 29373; Percent complete: 89.0%; Average loss: 0.6803\n",
            "Iteration: 29374; Percent complete: 89.0%; Average loss: 0.6926\n",
            "Iteration: 29375; Percent complete: 89.0%; Average loss: 0.8690\n",
            "Iteration: 29376; Percent complete: 89.0%; Average loss: 0.7051\n",
            "Iteration: 29377; Percent complete: 89.0%; Average loss: 0.6456\n",
            "Iteration: 29378; Percent complete: 89.0%; Average loss: 0.7098\n",
            "Iteration: 29379; Percent complete: 89.0%; Average loss: 0.6813\n",
            "Iteration: 29380; Percent complete: 89.0%; Average loss: 0.8164\n",
            "Iteration: 29381; Percent complete: 89.0%; Average loss: 0.8538\n",
            "Iteration: 29382; Percent complete: 89.0%; Average loss: 0.6445\n",
            "Iteration: 29383; Percent complete: 89.0%; Average loss: 0.7576\n",
            "Iteration: 29384; Percent complete: 89.0%; Average loss: 0.7773\n",
            "Iteration: 29385; Percent complete: 89.0%; Average loss: 0.7990\n",
            "Iteration: 29386; Percent complete: 89.0%; Average loss: 0.7938\n",
            "Iteration: 29387; Percent complete: 89.1%; Average loss: 0.7146\n",
            "Iteration: 29388; Percent complete: 89.1%; Average loss: 0.6697\n",
            "Iteration: 29389; Percent complete: 89.1%; Average loss: 0.7130\n",
            "Iteration: 29390; Percent complete: 89.1%; Average loss: 0.8099\n",
            "Iteration: 29391; Percent complete: 89.1%; Average loss: 0.6396\n",
            "Iteration: 29392; Percent complete: 89.1%; Average loss: 0.8085\n",
            "Iteration: 29393; Percent complete: 89.1%; Average loss: 0.6249\n",
            "Iteration: 29394; Percent complete: 89.1%; Average loss: 0.7012\n",
            "Iteration: 29395; Percent complete: 89.1%; Average loss: 0.8193\n",
            "Iteration: 29396; Percent complete: 89.1%; Average loss: 0.7551\n",
            "Iteration: 29397; Percent complete: 89.1%; Average loss: 0.6983\n",
            "Iteration: 29398; Percent complete: 89.1%; Average loss: 0.7978\n",
            "Iteration: 29399; Percent complete: 89.1%; Average loss: 0.8180\n",
            "Iteration: 29400; Percent complete: 89.1%; Average loss: 0.6368\n",
            "Iteration: 29401; Percent complete: 89.1%; Average loss: 0.7992\n",
            "Iteration: 29402; Percent complete: 89.1%; Average loss: 0.7326\n",
            "Iteration: 29403; Percent complete: 89.1%; Average loss: 0.7196\n",
            "Iteration: 29404; Percent complete: 89.1%; Average loss: 0.5959\n",
            "Iteration: 29405; Percent complete: 89.1%; Average loss: 0.6927\n",
            "Iteration: 29406; Percent complete: 89.1%; Average loss: 0.7250\n",
            "Iteration: 29407; Percent complete: 89.1%; Average loss: 0.7993\n",
            "Iteration: 29408; Percent complete: 89.1%; Average loss: 0.7401\n",
            "Iteration: 29409; Percent complete: 89.1%; Average loss: 0.7355\n",
            "Iteration: 29410; Percent complete: 89.1%; Average loss: 0.8091\n",
            "Iteration: 29411; Percent complete: 89.1%; Average loss: 0.7991\n",
            "Iteration: 29412; Percent complete: 89.1%; Average loss: 0.7228\n",
            "Iteration: 29413; Percent complete: 89.1%; Average loss: 0.7514\n",
            "Iteration: 29414; Percent complete: 89.1%; Average loss: 0.8459\n",
            "Iteration: 29415; Percent complete: 89.1%; Average loss: 0.6603\n",
            "Iteration: 29416; Percent complete: 89.1%; Average loss: 0.7408\n",
            "Iteration: 29417; Percent complete: 89.1%; Average loss: 0.8078\n",
            "Iteration: 29418; Percent complete: 89.1%; Average loss: 0.8291\n",
            "Iteration: 29419; Percent complete: 89.1%; Average loss: 0.7967\n",
            "Iteration: 29420; Percent complete: 89.2%; Average loss: 0.7083\n",
            "Iteration: 29421; Percent complete: 89.2%; Average loss: 0.8632\n",
            "Iteration: 29422; Percent complete: 89.2%; Average loss: 0.8762\n",
            "Iteration: 29423; Percent complete: 89.2%; Average loss: 0.7055\n",
            "Iteration: 29424; Percent complete: 89.2%; Average loss: 0.6006\n",
            "Iteration: 29425; Percent complete: 89.2%; Average loss: 0.6453\n",
            "Iteration: 29426; Percent complete: 89.2%; Average loss: 0.6661\n",
            "Iteration: 29427; Percent complete: 89.2%; Average loss: 0.8608\n",
            "Iteration: 29428; Percent complete: 89.2%; Average loss: 0.6798\n",
            "Iteration: 29429; Percent complete: 89.2%; Average loss: 0.8456\n",
            "Iteration: 29430; Percent complete: 89.2%; Average loss: 0.8321\n",
            "Iteration: 29431; Percent complete: 89.2%; Average loss: 0.6944\n",
            "Iteration: 29432; Percent complete: 89.2%; Average loss: 0.8003\n",
            "Iteration: 29433; Percent complete: 89.2%; Average loss: 0.7891\n",
            "Iteration: 29434; Percent complete: 89.2%; Average loss: 0.6914\n",
            "Iteration: 29435; Percent complete: 89.2%; Average loss: 0.8520\n",
            "Iteration: 29436; Percent complete: 89.2%; Average loss: 0.9013\n",
            "Iteration: 29437; Percent complete: 89.2%; Average loss: 0.8042\n",
            "Iteration: 29438; Percent complete: 89.2%; Average loss: 0.8188\n",
            "Iteration: 29439; Percent complete: 89.2%; Average loss: 0.6964\n",
            "Iteration: 29440; Percent complete: 89.2%; Average loss: 0.6620\n",
            "Iteration: 29441; Percent complete: 89.2%; Average loss: 0.8658\n",
            "Iteration: 29442; Percent complete: 89.2%; Average loss: 0.6776\n",
            "Iteration: 29443; Percent complete: 89.2%; Average loss: 0.7193\n",
            "Iteration: 29444; Percent complete: 89.2%; Average loss: 0.7647\n",
            "Iteration: 29445; Percent complete: 89.2%; Average loss: 0.6775\n",
            "Iteration: 29446; Percent complete: 89.2%; Average loss: 0.8263\n",
            "Iteration: 29447; Percent complete: 89.2%; Average loss: 0.6508\n",
            "Iteration: 29448; Percent complete: 89.2%; Average loss: 0.7523\n",
            "Iteration: 29449; Percent complete: 89.2%; Average loss: 0.5855\n",
            "Iteration: 29450; Percent complete: 89.2%; Average loss: 0.8099\n",
            "Iteration: 29451; Percent complete: 89.2%; Average loss: 0.7300\n",
            "Iteration: 29452; Percent complete: 89.2%; Average loss: 0.9107\n",
            "Iteration: 29453; Percent complete: 89.3%; Average loss: 0.8512\n",
            "Iteration: 29454; Percent complete: 89.3%; Average loss: 0.6402\n",
            "Iteration: 29455; Percent complete: 89.3%; Average loss: 0.8613\n",
            "Iteration: 29456; Percent complete: 89.3%; Average loss: 0.8113\n",
            "Iteration: 29457; Percent complete: 89.3%; Average loss: 0.8348\n",
            "Iteration: 29458; Percent complete: 89.3%; Average loss: 0.7774\n",
            "Iteration: 29459; Percent complete: 89.3%; Average loss: 0.7632\n",
            "Iteration: 29460; Percent complete: 89.3%; Average loss: 0.6874\n",
            "Iteration: 29461; Percent complete: 89.3%; Average loss: 0.8158\n",
            "Iteration: 29462; Percent complete: 89.3%; Average loss: 0.7175\n",
            "Iteration: 29463; Percent complete: 89.3%; Average loss: 0.7095\n",
            "Iteration: 29464; Percent complete: 89.3%; Average loss: 0.6922\n",
            "Iteration: 29465; Percent complete: 89.3%; Average loss: 0.6476\n",
            "Iteration: 29466; Percent complete: 89.3%; Average loss: 0.8104\n",
            "Iteration: 29467; Percent complete: 89.3%; Average loss: 0.6528\n",
            "Iteration: 29468; Percent complete: 89.3%; Average loss: 0.6897\n",
            "Iteration: 29469; Percent complete: 89.3%; Average loss: 0.7223\n",
            "Iteration: 29470; Percent complete: 89.3%; Average loss: 0.6088\n",
            "Iteration: 29471; Percent complete: 89.3%; Average loss: 0.7675\n",
            "Iteration: 29472; Percent complete: 89.3%; Average loss: 0.6172\n",
            "Iteration: 29473; Percent complete: 89.3%; Average loss: 0.7360\n",
            "Iteration: 29474; Percent complete: 89.3%; Average loss: 0.7558\n",
            "Iteration: 29475; Percent complete: 89.3%; Average loss: 0.7191\n",
            "Iteration: 29476; Percent complete: 89.3%; Average loss: 0.6773\n",
            "Iteration: 29477; Percent complete: 89.3%; Average loss: 0.6518\n",
            "Iteration: 29478; Percent complete: 89.3%; Average loss: 0.6294\n",
            "Iteration: 29479; Percent complete: 89.3%; Average loss: 0.8435\n",
            "Iteration: 29480; Percent complete: 89.3%; Average loss: 0.6786\n",
            "Iteration: 29481; Percent complete: 89.3%; Average loss: 0.6171\n",
            "Iteration: 29482; Percent complete: 89.3%; Average loss: 0.8277\n",
            "Iteration: 29483; Percent complete: 89.3%; Average loss: 0.7109\n",
            "Iteration: 29484; Percent complete: 89.3%; Average loss: 0.7908\n",
            "Iteration: 29485; Percent complete: 89.3%; Average loss: 0.7057\n",
            "Iteration: 29486; Percent complete: 89.4%; Average loss: 0.6251\n",
            "Iteration: 29487; Percent complete: 89.4%; Average loss: 0.6891\n",
            "Iteration: 29488; Percent complete: 89.4%; Average loss: 0.7689\n",
            "Iteration: 29489; Percent complete: 89.4%; Average loss: 0.6862\n",
            "Iteration: 29490; Percent complete: 89.4%; Average loss: 0.7433\n",
            "Iteration: 29491; Percent complete: 89.4%; Average loss: 0.8679\n",
            "Iteration: 29492; Percent complete: 89.4%; Average loss: 0.8211\n",
            "Iteration: 29493; Percent complete: 89.4%; Average loss: 0.7883\n",
            "Iteration: 29494; Percent complete: 89.4%; Average loss: 0.6459\n",
            "Iteration: 29495; Percent complete: 89.4%; Average loss: 0.7776\n",
            "Iteration: 29496; Percent complete: 89.4%; Average loss: 0.6425\n",
            "Iteration: 29497; Percent complete: 89.4%; Average loss: 0.8069\n",
            "Iteration: 29498; Percent complete: 89.4%; Average loss: 0.7599\n",
            "Iteration: 29499; Percent complete: 89.4%; Average loss: 0.6189\n",
            "Iteration: 29500; Percent complete: 89.4%; Average loss: 0.8664\n",
            "Iteration: 29501; Percent complete: 89.4%; Average loss: 0.7961\n",
            "Iteration: 29502; Percent complete: 89.4%; Average loss: 0.8740\n",
            "Iteration: 29503; Percent complete: 89.4%; Average loss: 0.6505\n",
            "Iteration: 29504; Percent complete: 89.4%; Average loss: 0.8888\n",
            "Iteration: 29505; Percent complete: 89.4%; Average loss: 0.7159\n",
            "Iteration: 29506; Percent complete: 89.4%; Average loss: 0.6778\n",
            "Iteration: 29507; Percent complete: 89.4%; Average loss: 0.7415\n",
            "Iteration: 29508; Percent complete: 89.4%; Average loss: 0.8142\n",
            "Iteration: 29509; Percent complete: 89.4%; Average loss: 0.7774\n",
            "Iteration: 29510; Percent complete: 89.4%; Average loss: 0.7960\n",
            "Iteration: 29511; Percent complete: 89.4%; Average loss: 0.8938\n",
            "Iteration: 29512; Percent complete: 89.4%; Average loss: 0.5556\n",
            "Iteration: 29513; Percent complete: 89.4%; Average loss: 0.8083\n",
            "Iteration: 29514; Percent complete: 89.4%; Average loss: 0.5530\n",
            "Iteration: 29515; Percent complete: 89.4%; Average loss: 0.9303\n",
            "Iteration: 29516; Percent complete: 89.4%; Average loss: 0.6982\n",
            "Iteration: 29517; Percent complete: 89.4%; Average loss: 0.7228\n",
            "Iteration: 29518; Percent complete: 89.4%; Average loss: 0.6359\n",
            "Iteration: 29519; Percent complete: 89.5%; Average loss: 0.7164\n",
            "Iteration: 29520; Percent complete: 89.5%; Average loss: 0.6971\n",
            "Iteration: 29521; Percent complete: 89.5%; Average loss: 0.8151\n",
            "Iteration: 29522; Percent complete: 89.5%; Average loss: 0.7330\n",
            "Iteration: 29523; Percent complete: 89.5%; Average loss: 0.7860\n",
            "Iteration: 29524; Percent complete: 89.5%; Average loss: 0.8351\n",
            "Iteration: 29525; Percent complete: 89.5%; Average loss: 0.6790\n",
            "Iteration: 29526; Percent complete: 89.5%; Average loss: 0.6993\n",
            "Iteration: 29527; Percent complete: 89.5%; Average loss: 0.5823\n",
            "Iteration: 29528; Percent complete: 89.5%; Average loss: 0.6765\n",
            "Iteration: 29529; Percent complete: 89.5%; Average loss: 0.8098\n",
            "Iteration: 29530; Percent complete: 89.5%; Average loss: 0.6913\n",
            "Iteration: 29531; Percent complete: 89.5%; Average loss: 0.5824\n",
            "Iteration: 29532; Percent complete: 89.5%; Average loss: 0.6161\n",
            "Iteration: 29533; Percent complete: 89.5%; Average loss: 0.8800\n",
            "Iteration: 29534; Percent complete: 89.5%; Average loss: 0.6448\n",
            "Iteration: 29535; Percent complete: 89.5%; Average loss: 0.6873\n",
            "Iteration: 29536; Percent complete: 89.5%; Average loss: 0.6694\n",
            "Iteration: 29537; Percent complete: 89.5%; Average loss: 0.9189\n",
            "Iteration: 29538; Percent complete: 89.5%; Average loss: 0.7044\n",
            "Iteration: 29539; Percent complete: 89.5%; Average loss: 0.6611\n",
            "Iteration: 29540; Percent complete: 89.5%; Average loss: 0.7110\n",
            "Iteration: 29541; Percent complete: 89.5%; Average loss: 0.7940\n",
            "Iteration: 29542; Percent complete: 89.5%; Average loss: 0.6619\n",
            "Iteration: 29543; Percent complete: 89.5%; Average loss: 0.7274\n",
            "Iteration: 29544; Percent complete: 89.5%; Average loss: 0.6810\n",
            "Iteration: 29545; Percent complete: 89.5%; Average loss: 0.6243\n",
            "Iteration: 29546; Percent complete: 89.5%; Average loss: 0.7652\n",
            "Iteration: 29547; Percent complete: 89.5%; Average loss: 0.7170\n",
            "Iteration: 29548; Percent complete: 89.5%; Average loss: 0.6502\n",
            "Iteration: 29549; Percent complete: 89.5%; Average loss: 0.9248\n",
            "Iteration: 29550; Percent complete: 89.5%; Average loss: 0.6796\n",
            "Iteration: 29551; Percent complete: 89.5%; Average loss: 0.7738\n",
            "Iteration: 29552; Percent complete: 89.6%; Average loss: 0.7531\n",
            "Iteration: 29553; Percent complete: 89.6%; Average loss: 0.8539\n",
            "Iteration: 29554; Percent complete: 89.6%; Average loss: 0.7450\n",
            "Iteration: 29555; Percent complete: 89.6%; Average loss: 0.6140\n",
            "Iteration: 29556; Percent complete: 89.6%; Average loss: 0.7988\n",
            "Iteration: 29557; Percent complete: 89.6%; Average loss: 0.7780\n",
            "Iteration: 29558; Percent complete: 89.6%; Average loss: 0.7319\n",
            "Iteration: 29559; Percent complete: 89.6%; Average loss: 0.6301\n",
            "Iteration: 29560; Percent complete: 89.6%; Average loss: 0.8545\n",
            "Iteration: 29561; Percent complete: 89.6%; Average loss: 0.7604\n",
            "Iteration: 29562; Percent complete: 89.6%; Average loss: 0.6456\n",
            "Iteration: 29563; Percent complete: 89.6%; Average loss: 0.7652\n",
            "Iteration: 29564; Percent complete: 89.6%; Average loss: 0.7599\n",
            "Iteration: 29565; Percent complete: 89.6%; Average loss: 0.8367\n",
            "Iteration: 29566; Percent complete: 89.6%; Average loss: 0.5562\n",
            "Iteration: 29567; Percent complete: 89.6%; Average loss: 0.7090\n",
            "Iteration: 29568; Percent complete: 89.6%; Average loss: 0.8919\n",
            "Iteration: 29569; Percent complete: 89.6%; Average loss: 0.5669\n",
            "Iteration: 29570; Percent complete: 89.6%; Average loss: 0.8108\n",
            "Iteration: 29571; Percent complete: 89.6%; Average loss: 0.7999\n",
            "Iteration: 29572; Percent complete: 89.6%; Average loss: 0.7308\n",
            "Iteration: 29573; Percent complete: 89.6%; Average loss: 0.6330\n",
            "Iteration: 29574; Percent complete: 89.6%; Average loss: 0.8056\n",
            "Iteration: 29575; Percent complete: 89.6%; Average loss: 0.7205\n",
            "Iteration: 29576; Percent complete: 89.6%; Average loss: 0.7378\n",
            "Iteration: 29577; Percent complete: 89.6%; Average loss: 0.6704\n",
            "Iteration: 29578; Percent complete: 89.6%; Average loss: 0.6569\n",
            "Iteration: 29579; Percent complete: 89.6%; Average loss: 0.7401\n",
            "Iteration: 29580; Percent complete: 89.6%; Average loss: 0.7680\n",
            "Iteration: 29581; Percent complete: 89.6%; Average loss: 0.7985\n",
            "Iteration: 29582; Percent complete: 89.6%; Average loss: 0.6745\n",
            "Iteration: 29583; Percent complete: 89.6%; Average loss: 0.6899\n",
            "Iteration: 29584; Percent complete: 89.6%; Average loss: 0.6036\n",
            "Iteration: 29585; Percent complete: 89.7%; Average loss: 0.6430\n",
            "Iteration: 29586; Percent complete: 89.7%; Average loss: 0.7815\n",
            "Iteration: 29587; Percent complete: 89.7%; Average loss: 0.9438\n",
            "Iteration: 29588; Percent complete: 89.7%; Average loss: 0.7089\n",
            "Iteration: 29589; Percent complete: 89.7%; Average loss: 0.7000\n",
            "Iteration: 29590; Percent complete: 89.7%; Average loss: 0.6132\n",
            "Iteration: 29591; Percent complete: 89.7%; Average loss: 0.6929\n",
            "Iteration: 29592; Percent complete: 89.7%; Average loss: 0.7149\n",
            "Iteration: 29593; Percent complete: 89.7%; Average loss: 0.6666\n",
            "Iteration: 29594; Percent complete: 89.7%; Average loss: 0.8043\n",
            "Iteration: 29595; Percent complete: 89.7%; Average loss: 0.7269\n",
            "Iteration: 29596; Percent complete: 89.7%; Average loss: 0.6239\n",
            "Iteration: 29597; Percent complete: 89.7%; Average loss: 0.7469\n",
            "Iteration: 29598; Percent complete: 89.7%; Average loss: 0.8472\n",
            "Iteration: 29599; Percent complete: 89.7%; Average loss: 0.6690\n",
            "Iteration: 29600; Percent complete: 89.7%; Average loss: 0.8550\n",
            "Iteration: 29601; Percent complete: 89.7%; Average loss: 0.7347\n",
            "Iteration: 29602; Percent complete: 89.7%; Average loss: 0.6851\n",
            "Iteration: 29603; Percent complete: 89.7%; Average loss: 0.7513\n",
            "Iteration: 29604; Percent complete: 89.7%; Average loss: 0.9907\n",
            "Iteration: 29605; Percent complete: 89.7%; Average loss: 0.7158\n",
            "Iteration: 29606; Percent complete: 89.7%; Average loss: 0.6261\n",
            "Iteration: 29607; Percent complete: 89.7%; Average loss: 0.8287\n",
            "Iteration: 29608; Percent complete: 89.7%; Average loss: 0.8160\n",
            "Iteration: 29609; Percent complete: 89.7%; Average loss: 0.9092\n",
            "Iteration: 29610; Percent complete: 89.7%; Average loss: 0.8092\n",
            "Iteration: 29611; Percent complete: 89.7%; Average loss: 0.7548\n",
            "Iteration: 29612; Percent complete: 89.7%; Average loss: 0.9026\n",
            "Iteration: 29613; Percent complete: 89.7%; Average loss: 0.7847\n",
            "Iteration: 29614; Percent complete: 89.7%; Average loss: 0.8644\n",
            "Iteration: 29615; Percent complete: 89.7%; Average loss: 0.7184\n",
            "Iteration: 29616; Percent complete: 89.7%; Average loss: 0.7667\n",
            "Iteration: 29617; Percent complete: 89.7%; Average loss: 0.6590\n",
            "Iteration: 29618; Percent complete: 89.8%; Average loss: 0.8594\n",
            "Iteration: 29619; Percent complete: 89.8%; Average loss: 0.7118\n",
            "Iteration: 29620; Percent complete: 89.8%; Average loss: 0.7453\n",
            "Iteration: 29621; Percent complete: 89.8%; Average loss: 0.8441\n",
            "Iteration: 29622; Percent complete: 89.8%; Average loss: 0.7054\n",
            "Iteration: 29623; Percent complete: 89.8%; Average loss: 0.8308\n",
            "Iteration: 29624; Percent complete: 89.8%; Average loss: 0.8405\n",
            "Iteration: 29625; Percent complete: 89.8%; Average loss: 0.7248\n",
            "Iteration: 29626; Percent complete: 89.8%; Average loss: 0.9644\n",
            "Iteration: 29627; Percent complete: 89.8%; Average loss: 0.7727\n",
            "Iteration: 29628; Percent complete: 89.8%; Average loss: 0.6064\n",
            "Iteration: 29629; Percent complete: 89.8%; Average loss: 0.6298\n",
            "Iteration: 29630; Percent complete: 89.8%; Average loss: 0.6643\n",
            "Iteration: 29631; Percent complete: 89.8%; Average loss: 0.6950\n",
            "Iteration: 29632; Percent complete: 89.8%; Average loss: 0.8630\n",
            "Iteration: 29633; Percent complete: 89.8%; Average loss: 0.6014\n",
            "Iteration: 29634; Percent complete: 89.8%; Average loss: 0.6084\n",
            "Iteration: 29635; Percent complete: 89.8%; Average loss: 0.8449\n",
            "Iteration: 29636; Percent complete: 89.8%; Average loss: 0.9180\n",
            "Iteration: 29637; Percent complete: 89.8%; Average loss: 0.7233\n",
            "Iteration: 29638; Percent complete: 89.8%; Average loss: 0.8234\n",
            "Iteration: 29639; Percent complete: 89.8%; Average loss: 0.7927\n",
            "Iteration: 29640; Percent complete: 89.8%; Average loss: 0.5690\n",
            "Iteration: 29641; Percent complete: 89.8%; Average loss: 0.6872\n",
            "Iteration: 29642; Percent complete: 89.8%; Average loss: 0.6424\n",
            "Iteration: 29643; Percent complete: 89.8%; Average loss: 0.9193\n",
            "Iteration: 29644; Percent complete: 89.8%; Average loss: 0.6181\n",
            "Iteration: 29645; Percent complete: 89.8%; Average loss: 0.5876\n",
            "Iteration: 29646; Percent complete: 89.8%; Average loss: 0.7477\n",
            "Iteration: 29647; Percent complete: 89.8%; Average loss: 0.8217\n",
            "Iteration: 29648; Percent complete: 89.8%; Average loss: 0.6671\n",
            "Iteration: 29649; Percent complete: 89.8%; Average loss: 0.6900\n",
            "Iteration: 29650; Percent complete: 89.8%; Average loss: 0.7399\n",
            "Iteration: 29651; Percent complete: 89.9%; Average loss: 0.7274\n",
            "Iteration: 29652; Percent complete: 89.9%; Average loss: 0.7208\n",
            "Iteration: 29653; Percent complete: 89.9%; Average loss: 0.8974\n",
            "Iteration: 29654; Percent complete: 89.9%; Average loss: 0.6244\n",
            "Iteration: 29655; Percent complete: 89.9%; Average loss: 0.6999\n",
            "Iteration: 29656; Percent complete: 89.9%; Average loss: 0.7245\n",
            "Iteration: 29657; Percent complete: 89.9%; Average loss: 0.7466\n",
            "Iteration: 29658; Percent complete: 89.9%; Average loss: 0.6431\n",
            "Iteration: 29659; Percent complete: 89.9%; Average loss: 0.8252\n",
            "Iteration: 29660; Percent complete: 89.9%; Average loss: 0.6517\n",
            "Iteration: 29661; Percent complete: 89.9%; Average loss: 0.6344\n",
            "Iteration: 29662; Percent complete: 89.9%; Average loss: 0.7730\n",
            "Iteration: 29663; Percent complete: 89.9%; Average loss: 0.7754\n",
            "Iteration: 29664; Percent complete: 89.9%; Average loss: 0.7516\n",
            "Iteration: 29665; Percent complete: 89.9%; Average loss: 0.6367\n",
            "Iteration: 29666; Percent complete: 89.9%; Average loss: 0.6932\n",
            "Iteration: 29667; Percent complete: 89.9%; Average loss: 0.6282\n",
            "Iteration: 29668; Percent complete: 89.9%; Average loss: 0.7566\n",
            "Iteration: 29669; Percent complete: 89.9%; Average loss: 0.6059\n",
            "Iteration: 29670; Percent complete: 89.9%; Average loss: 0.6169\n",
            "Iteration: 29671; Percent complete: 89.9%; Average loss: 0.7273\n",
            "Iteration: 29672; Percent complete: 89.9%; Average loss: 0.8439\n",
            "Iteration: 29673; Percent complete: 89.9%; Average loss: 0.7088\n",
            "Iteration: 29674; Percent complete: 89.9%; Average loss: 0.7923\n",
            "Iteration: 29675; Percent complete: 89.9%; Average loss: 0.7463\n",
            "Iteration: 29676; Percent complete: 89.9%; Average loss: 0.8364\n",
            "Iteration: 29677; Percent complete: 89.9%; Average loss: 0.7420\n",
            "Iteration: 29678; Percent complete: 89.9%; Average loss: 0.8428\n",
            "Iteration: 29679; Percent complete: 89.9%; Average loss: 0.6639\n",
            "Iteration: 29680; Percent complete: 89.9%; Average loss: 0.8957\n",
            "Iteration: 29681; Percent complete: 89.9%; Average loss: 0.6711\n",
            "Iteration: 29682; Percent complete: 89.9%; Average loss: 0.8106\n",
            "Iteration: 29683; Percent complete: 89.9%; Average loss: 0.7987\n",
            "Iteration: 29684; Percent complete: 90.0%; Average loss: 0.7386\n",
            "Iteration: 29685; Percent complete: 90.0%; Average loss: 0.6421\n",
            "Iteration: 29686; Percent complete: 90.0%; Average loss: 0.7193\n",
            "Iteration: 29687; Percent complete: 90.0%; Average loss: 0.6225\n",
            "Iteration: 29688; Percent complete: 90.0%; Average loss: 0.7895\n",
            "Iteration: 29689; Percent complete: 90.0%; Average loss: 0.6155\n",
            "Iteration: 29690; Percent complete: 90.0%; Average loss: 0.6785\n",
            "Iteration: 29691; Percent complete: 90.0%; Average loss: 0.7106\n",
            "Iteration: 29692; Percent complete: 90.0%; Average loss: 0.5861\n",
            "Iteration: 29693; Percent complete: 90.0%; Average loss: 0.8349\n",
            "Iteration: 29694; Percent complete: 90.0%; Average loss: 0.7111\n",
            "Iteration: 29695; Percent complete: 90.0%; Average loss: 0.7981\n",
            "Iteration: 29696; Percent complete: 90.0%; Average loss: 0.7215\n",
            "Iteration: 29697; Percent complete: 90.0%; Average loss: 0.7833\n",
            "Iteration: 29698; Percent complete: 90.0%; Average loss: 0.7119\n",
            "Iteration: 29699; Percent complete: 90.0%; Average loss: 0.6941\n",
            "Iteration: 29700; Percent complete: 90.0%; Average loss: 0.8205\n",
            "Iteration: 29701; Percent complete: 90.0%; Average loss: 0.6348\n",
            "Iteration: 29702; Percent complete: 90.0%; Average loss: 0.8489\n",
            "Iteration: 29703; Percent complete: 90.0%; Average loss: 0.9265\n",
            "Iteration: 29704; Percent complete: 90.0%; Average loss: 0.8425\n",
            "Iteration: 29705; Percent complete: 90.0%; Average loss: 0.8997\n",
            "Iteration: 29706; Percent complete: 90.0%; Average loss: 0.7281\n",
            "Iteration: 29707; Percent complete: 90.0%; Average loss: 0.8678\n",
            "Iteration: 29708; Percent complete: 90.0%; Average loss: 0.6030\n",
            "Iteration: 29709; Percent complete: 90.0%; Average loss: 0.8717\n",
            "Iteration: 29710; Percent complete: 90.0%; Average loss: 0.7304\n",
            "Iteration: 29711; Percent complete: 90.0%; Average loss: 0.7647\n",
            "Iteration: 29712; Percent complete: 90.0%; Average loss: 0.8002\n",
            "Iteration: 29713; Percent complete: 90.0%; Average loss: 0.7809\n",
            "Iteration: 29714; Percent complete: 90.0%; Average loss: 0.6918\n",
            "Iteration: 29715; Percent complete: 90.0%; Average loss: 0.7090\n",
            "Iteration: 29716; Percent complete: 90.0%; Average loss: 0.5843\n",
            "Iteration: 29717; Percent complete: 90.1%; Average loss: 0.6907\n",
            "Iteration: 29718; Percent complete: 90.1%; Average loss: 0.8145\n",
            "Iteration: 29719; Percent complete: 90.1%; Average loss: 0.7317\n",
            "Iteration: 29720; Percent complete: 90.1%; Average loss: 0.6345\n",
            "Iteration: 29721; Percent complete: 90.1%; Average loss: 0.7730\n",
            "Iteration: 29722; Percent complete: 90.1%; Average loss: 0.6035\n",
            "Iteration: 29723; Percent complete: 90.1%; Average loss: 0.6762\n",
            "Iteration: 29724; Percent complete: 90.1%; Average loss: 0.8699\n",
            "Iteration: 29725; Percent complete: 90.1%; Average loss: 0.6537\n",
            "Iteration: 29726; Percent complete: 90.1%; Average loss: 0.7256\n",
            "Iteration: 29727; Percent complete: 90.1%; Average loss: 0.7870\n",
            "Iteration: 29728; Percent complete: 90.1%; Average loss: 0.7909\n",
            "Iteration: 29729; Percent complete: 90.1%; Average loss: 0.6986\n",
            "Iteration: 29730; Percent complete: 90.1%; Average loss: 0.6605\n",
            "Iteration: 29731; Percent complete: 90.1%; Average loss: 0.7990\n",
            "Iteration: 29732; Percent complete: 90.1%; Average loss: 0.7569\n",
            "Iteration: 29733; Percent complete: 90.1%; Average loss: 0.5722\n",
            "Iteration: 29734; Percent complete: 90.1%; Average loss: 0.6858\n",
            "Iteration: 29735; Percent complete: 90.1%; Average loss: 0.6536\n",
            "Iteration: 29736; Percent complete: 90.1%; Average loss: 0.6723\n",
            "Iteration: 29737; Percent complete: 90.1%; Average loss: 0.7000\n",
            "Iteration: 29738; Percent complete: 90.1%; Average loss: 0.8970\n",
            "Iteration: 29739; Percent complete: 90.1%; Average loss: 0.6551\n",
            "Iteration: 29740; Percent complete: 90.1%; Average loss: 0.7717\n",
            "Iteration: 29741; Percent complete: 90.1%; Average loss: 0.8309\n",
            "Iteration: 29742; Percent complete: 90.1%; Average loss: 0.7453\n",
            "Iteration: 29743; Percent complete: 90.1%; Average loss: 0.7777\n",
            "Iteration: 29744; Percent complete: 90.1%; Average loss: 0.7476\n",
            "Iteration: 29745; Percent complete: 90.1%; Average loss: 0.8577\n",
            "Iteration: 29746; Percent complete: 90.1%; Average loss: 0.6417\n",
            "Iteration: 29747; Percent complete: 90.1%; Average loss: 0.6938\n",
            "Iteration: 29748; Percent complete: 90.1%; Average loss: 0.7016\n",
            "Iteration: 29749; Percent complete: 90.1%; Average loss: 0.6871\n",
            "Iteration: 29750; Percent complete: 90.2%; Average loss: 0.6987\n",
            "Iteration: 29751; Percent complete: 90.2%; Average loss: 0.7367\n",
            "Iteration: 29752; Percent complete: 90.2%; Average loss: 0.8017\n",
            "Iteration: 29753; Percent complete: 90.2%; Average loss: 0.6832\n",
            "Iteration: 29754; Percent complete: 90.2%; Average loss: 0.7214\n",
            "Iteration: 29755; Percent complete: 90.2%; Average loss: 0.7140\n",
            "Iteration: 29756; Percent complete: 90.2%; Average loss: 0.6364\n",
            "Iteration: 29757; Percent complete: 90.2%; Average loss: 0.7426\n",
            "Iteration: 29758; Percent complete: 90.2%; Average loss: 0.7204\n",
            "Iteration: 29759; Percent complete: 90.2%; Average loss: 0.8153\n",
            "Iteration: 29760; Percent complete: 90.2%; Average loss: 0.7584\n",
            "Iteration: 29761; Percent complete: 90.2%; Average loss: 0.7479\n",
            "Iteration: 29762; Percent complete: 90.2%; Average loss: 0.8388\n",
            "Iteration: 29763; Percent complete: 90.2%; Average loss: 0.6039\n",
            "Iteration: 29764; Percent complete: 90.2%; Average loss: 0.7980\n",
            "Iteration: 29765; Percent complete: 90.2%; Average loss: 0.7036\n",
            "Iteration: 29766; Percent complete: 90.2%; Average loss: 0.7381\n",
            "Iteration: 29767; Percent complete: 90.2%; Average loss: 0.6664\n",
            "Iteration: 29768; Percent complete: 90.2%; Average loss: 0.7545\n",
            "Iteration: 29769; Percent complete: 90.2%; Average loss: 0.7692\n",
            "Iteration: 29770; Percent complete: 90.2%; Average loss: 0.6078\n",
            "Iteration: 29771; Percent complete: 90.2%; Average loss: 0.6901\n",
            "Iteration: 29772; Percent complete: 90.2%; Average loss: 0.7041\n",
            "Iteration: 29773; Percent complete: 90.2%; Average loss: 0.7393\n",
            "Iteration: 29774; Percent complete: 90.2%; Average loss: 0.9283\n",
            "Iteration: 29775; Percent complete: 90.2%; Average loss: 0.5952\n",
            "Iteration: 29776; Percent complete: 90.2%; Average loss: 0.7476\n",
            "Iteration: 29777; Percent complete: 90.2%; Average loss: 0.6268\n",
            "Iteration: 29778; Percent complete: 90.2%; Average loss: 0.5902\n",
            "Iteration: 29779; Percent complete: 90.2%; Average loss: 0.7377\n",
            "Iteration: 29780; Percent complete: 90.2%; Average loss: 0.6866\n",
            "Iteration: 29781; Percent complete: 90.2%; Average loss: 0.7104\n",
            "Iteration: 29782; Percent complete: 90.2%; Average loss: 0.9615\n",
            "Iteration: 29783; Percent complete: 90.3%; Average loss: 0.6908\n",
            "Iteration: 29784; Percent complete: 90.3%; Average loss: 0.6633\n",
            "Iteration: 29785; Percent complete: 90.3%; Average loss: 0.9586\n",
            "Iteration: 29786; Percent complete: 90.3%; Average loss: 0.7090\n",
            "Iteration: 29787; Percent complete: 90.3%; Average loss: 0.6638\n",
            "Iteration: 29788; Percent complete: 90.3%; Average loss: 0.7055\n",
            "Iteration: 29789; Percent complete: 90.3%; Average loss: 0.7066\n",
            "Iteration: 29790; Percent complete: 90.3%; Average loss: 0.6080\n",
            "Iteration: 29791; Percent complete: 90.3%; Average loss: 0.6955\n",
            "Iteration: 29792; Percent complete: 90.3%; Average loss: 0.6762\n",
            "Iteration: 29793; Percent complete: 90.3%; Average loss: 0.6802\n",
            "Iteration: 29794; Percent complete: 90.3%; Average loss: 0.7637\n",
            "Iteration: 29795; Percent complete: 90.3%; Average loss: 0.6201\n",
            "Iteration: 29796; Percent complete: 90.3%; Average loss: 0.8098\n",
            "Iteration: 29797; Percent complete: 90.3%; Average loss: 0.6069\n",
            "Iteration: 29798; Percent complete: 90.3%; Average loss: 0.7576\n",
            "Iteration: 29799; Percent complete: 90.3%; Average loss: 0.6246\n",
            "Iteration: 29800; Percent complete: 90.3%; Average loss: 0.6925\n",
            "Iteration: 29801; Percent complete: 90.3%; Average loss: 0.5898\n",
            "Iteration: 29802; Percent complete: 90.3%; Average loss: 0.6716\n",
            "Iteration: 29803; Percent complete: 90.3%; Average loss: 0.6159\n",
            "Iteration: 29804; Percent complete: 90.3%; Average loss: 0.6064\n",
            "Iteration: 29805; Percent complete: 90.3%; Average loss: 0.7750\n",
            "Iteration: 29806; Percent complete: 90.3%; Average loss: 0.8545\n",
            "Iteration: 29807; Percent complete: 90.3%; Average loss: 0.8467\n",
            "Iteration: 29808; Percent complete: 90.3%; Average loss: 0.7991\n",
            "Iteration: 29809; Percent complete: 90.3%; Average loss: 0.8050\n",
            "Iteration: 29810; Percent complete: 90.3%; Average loss: 0.8274\n",
            "Iteration: 29811; Percent complete: 90.3%; Average loss: 0.6761\n",
            "Iteration: 29812; Percent complete: 90.3%; Average loss: 0.8254\n",
            "Iteration: 29813; Percent complete: 90.3%; Average loss: 0.9634\n",
            "Iteration: 29814; Percent complete: 90.3%; Average loss: 0.8214\n",
            "Iteration: 29815; Percent complete: 90.3%; Average loss: 0.7517\n",
            "Iteration: 29816; Percent complete: 90.4%; Average loss: 0.6688\n",
            "Iteration: 29817; Percent complete: 90.4%; Average loss: 0.8484\n",
            "Iteration: 29818; Percent complete: 90.4%; Average loss: 0.5878\n",
            "Iteration: 29819; Percent complete: 90.4%; Average loss: 0.7111\n",
            "Iteration: 29820; Percent complete: 90.4%; Average loss: 0.6626\n",
            "Iteration: 29821; Percent complete: 90.4%; Average loss: 0.7343\n",
            "Iteration: 29822; Percent complete: 90.4%; Average loss: 0.7083\n",
            "Iteration: 29823; Percent complete: 90.4%; Average loss: 0.7560\n",
            "Iteration: 29824; Percent complete: 90.4%; Average loss: 0.8839\n",
            "Iteration: 29825; Percent complete: 90.4%; Average loss: 0.9063\n",
            "Iteration: 29826; Percent complete: 90.4%; Average loss: 0.6429\n",
            "Iteration: 29827; Percent complete: 90.4%; Average loss: 0.6167\n",
            "Iteration: 29828; Percent complete: 90.4%; Average loss: 0.7621\n",
            "Iteration: 29829; Percent complete: 90.4%; Average loss: 0.8740\n",
            "Iteration: 29830; Percent complete: 90.4%; Average loss: 0.9407\n",
            "Iteration: 29831; Percent complete: 90.4%; Average loss: 0.7567\n",
            "Iteration: 29832; Percent complete: 90.4%; Average loss: 0.7877\n",
            "Iteration: 29833; Percent complete: 90.4%; Average loss: 0.8136\n",
            "Iteration: 29834; Percent complete: 90.4%; Average loss: 0.7197\n",
            "Iteration: 29835; Percent complete: 90.4%; Average loss: 0.6532\n",
            "Iteration: 29836; Percent complete: 90.4%; Average loss: 0.8215\n",
            "Iteration: 29837; Percent complete: 90.4%; Average loss: 0.6736\n",
            "Iteration: 29838; Percent complete: 90.4%; Average loss: 0.7191\n",
            "Iteration: 29839; Percent complete: 90.4%; Average loss: 0.9689\n",
            "Iteration: 29840; Percent complete: 90.4%; Average loss: 0.6908\n",
            "Iteration: 29841; Percent complete: 90.4%; Average loss: 0.7794\n",
            "Iteration: 29842; Percent complete: 90.4%; Average loss: 0.7652\n",
            "Iteration: 29843; Percent complete: 90.4%; Average loss: 0.5493\n",
            "Iteration: 29844; Percent complete: 90.4%; Average loss: 0.7092\n",
            "Iteration: 29845; Percent complete: 90.4%; Average loss: 0.8155\n",
            "Iteration: 29846; Percent complete: 90.4%; Average loss: 0.8229\n",
            "Iteration: 29847; Percent complete: 90.4%; Average loss: 0.6180\n",
            "Iteration: 29848; Percent complete: 90.4%; Average loss: 0.7788\n",
            "Iteration: 29849; Percent complete: 90.5%; Average loss: 0.6855\n",
            "Iteration: 29850; Percent complete: 90.5%; Average loss: 0.5773\n",
            "Iteration: 29851; Percent complete: 90.5%; Average loss: 0.7551\n",
            "Iteration: 29852; Percent complete: 90.5%; Average loss: 0.6039\n",
            "Iteration: 29853; Percent complete: 90.5%; Average loss: 0.7394\n",
            "Iteration: 29854; Percent complete: 90.5%; Average loss: 0.6996\n",
            "Iteration: 29855; Percent complete: 90.5%; Average loss: 0.9097\n",
            "Iteration: 29856; Percent complete: 90.5%; Average loss: 0.6814\n",
            "Iteration: 29857; Percent complete: 90.5%; Average loss: 0.7207\n",
            "Iteration: 29858; Percent complete: 90.5%; Average loss: 0.8545\n",
            "Iteration: 29859; Percent complete: 90.5%; Average loss: 0.7149\n",
            "Iteration: 29860; Percent complete: 90.5%; Average loss: 0.6961\n",
            "Iteration: 29861; Percent complete: 90.5%; Average loss: 0.6747\n",
            "Iteration: 29862; Percent complete: 90.5%; Average loss: 0.8490\n",
            "Iteration: 29863; Percent complete: 90.5%; Average loss: 0.6618\n",
            "Iteration: 29864; Percent complete: 90.5%; Average loss: 0.8649\n",
            "Iteration: 29865; Percent complete: 90.5%; Average loss: 0.5853\n",
            "Iteration: 29866; Percent complete: 90.5%; Average loss: 0.8050\n",
            "Iteration: 29867; Percent complete: 90.5%; Average loss: 0.7225\n",
            "Iteration: 29868; Percent complete: 90.5%; Average loss: 0.6425\n",
            "Iteration: 29869; Percent complete: 90.5%; Average loss: 0.7636\n",
            "Iteration: 29870; Percent complete: 90.5%; Average loss: 0.8948\n",
            "Iteration: 29871; Percent complete: 90.5%; Average loss: 0.6278\n",
            "Iteration: 29872; Percent complete: 90.5%; Average loss: 0.7996\n",
            "Iteration: 29873; Percent complete: 90.5%; Average loss: 0.7609\n",
            "Iteration: 29874; Percent complete: 90.5%; Average loss: 0.5454\n",
            "Iteration: 29875; Percent complete: 90.5%; Average loss: 0.6739\n",
            "Iteration: 29876; Percent complete: 90.5%; Average loss: 0.7072\n",
            "Iteration: 29877; Percent complete: 90.5%; Average loss: 0.8371\n",
            "Iteration: 29878; Percent complete: 90.5%; Average loss: 0.6592\n",
            "Iteration: 29879; Percent complete: 90.5%; Average loss: 0.7358\n",
            "Iteration: 29880; Percent complete: 90.5%; Average loss: 0.7086\n",
            "Iteration: 29881; Percent complete: 90.5%; Average loss: 0.6527\n",
            "Iteration: 29882; Percent complete: 90.6%; Average loss: 0.7735\n",
            "Iteration: 29883; Percent complete: 90.6%; Average loss: 0.7133\n",
            "Iteration: 29884; Percent complete: 90.6%; Average loss: 0.6850\n",
            "Iteration: 29885; Percent complete: 90.6%; Average loss: 0.6956\n",
            "Iteration: 29886; Percent complete: 90.6%; Average loss: 0.6516\n",
            "Iteration: 29887; Percent complete: 90.6%; Average loss: 0.8573\n",
            "Iteration: 29888; Percent complete: 90.6%; Average loss: 0.7322\n",
            "Iteration: 29889; Percent complete: 90.6%; Average loss: 0.7385\n",
            "Iteration: 29890; Percent complete: 90.6%; Average loss: 0.8815\n",
            "Iteration: 29891; Percent complete: 90.6%; Average loss: 0.6906\n",
            "Iteration: 29892; Percent complete: 90.6%; Average loss: 0.7328\n",
            "Iteration: 29893; Percent complete: 90.6%; Average loss: 0.8269\n",
            "Iteration: 29894; Percent complete: 90.6%; Average loss: 0.7193\n",
            "Iteration: 29895; Percent complete: 90.6%; Average loss: 0.6412\n",
            "Iteration: 29896; Percent complete: 90.6%; Average loss: 0.7866\n",
            "Iteration: 29897; Percent complete: 90.6%; Average loss: 0.7162\n",
            "Iteration: 29898; Percent complete: 90.6%; Average loss: 0.7335\n",
            "Iteration: 29899; Percent complete: 90.6%; Average loss: 0.7775\n",
            "Iteration: 29900; Percent complete: 90.6%; Average loss: 0.7713\n",
            "Iteration: 29901; Percent complete: 90.6%; Average loss: 0.7510\n",
            "Iteration: 29902; Percent complete: 90.6%; Average loss: 0.7233\n",
            "Iteration: 29903; Percent complete: 90.6%; Average loss: 0.7451\n",
            "Iteration: 29904; Percent complete: 90.6%; Average loss: 0.6521\n",
            "Iteration: 29905; Percent complete: 90.6%; Average loss: 0.6399\n",
            "Iteration: 29906; Percent complete: 90.6%; Average loss: 0.7580\n",
            "Iteration: 29907; Percent complete: 90.6%; Average loss: 0.6951\n",
            "Iteration: 29908; Percent complete: 90.6%; Average loss: 0.5499\n",
            "Iteration: 29909; Percent complete: 90.6%; Average loss: 0.6292\n",
            "Iteration: 29910; Percent complete: 90.6%; Average loss: 0.9307\n",
            "Iteration: 29911; Percent complete: 90.6%; Average loss: 0.6719\n",
            "Iteration: 29912; Percent complete: 90.6%; Average loss: 0.6102\n",
            "Iteration: 29913; Percent complete: 90.6%; Average loss: 0.8531\n",
            "Iteration: 29914; Percent complete: 90.6%; Average loss: 0.7279\n",
            "Iteration: 29915; Percent complete: 90.7%; Average loss: 0.7343\n",
            "Iteration: 29916; Percent complete: 90.7%; Average loss: 0.6841\n",
            "Iteration: 29917; Percent complete: 90.7%; Average loss: 0.5611\n",
            "Iteration: 29918; Percent complete: 90.7%; Average loss: 0.8179\n",
            "Iteration: 29919; Percent complete: 90.7%; Average loss: 0.6680\n",
            "Iteration: 29920; Percent complete: 90.7%; Average loss: 0.6597\n",
            "Iteration: 29921; Percent complete: 90.7%; Average loss: 0.8143\n",
            "Iteration: 29922; Percent complete: 90.7%; Average loss: 0.6679\n",
            "Iteration: 29923; Percent complete: 90.7%; Average loss: 0.6792\n",
            "Iteration: 29924; Percent complete: 90.7%; Average loss: 0.8077\n",
            "Iteration: 29925; Percent complete: 90.7%; Average loss: 0.7453\n",
            "Iteration: 29926; Percent complete: 90.7%; Average loss: 0.7008\n",
            "Iteration: 29927; Percent complete: 90.7%; Average loss: 0.6849\n",
            "Iteration: 29928; Percent complete: 90.7%; Average loss: 0.7107\n",
            "Iteration: 29929; Percent complete: 90.7%; Average loss: 0.6862\n",
            "Iteration: 29930; Percent complete: 90.7%; Average loss: 0.8466\n",
            "Iteration: 29931; Percent complete: 90.7%; Average loss: 0.6890\n",
            "Iteration: 29932; Percent complete: 90.7%; Average loss: 0.8117\n",
            "Iteration: 29933; Percent complete: 90.7%; Average loss: 0.7212\n",
            "Iteration: 29934; Percent complete: 90.7%; Average loss: 0.7865\n",
            "Iteration: 29935; Percent complete: 90.7%; Average loss: 0.8066\n",
            "Iteration: 29936; Percent complete: 90.7%; Average loss: 0.6667\n",
            "Iteration: 29937; Percent complete: 90.7%; Average loss: 0.8461\n",
            "Iteration: 29938; Percent complete: 90.7%; Average loss: 0.6976\n",
            "Iteration: 29939; Percent complete: 90.7%; Average loss: 0.7988\n",
            "Iteration: 29940; Percent complete: 90.7%; Average loss: 0.8861\n",
            "Iteration: 29941; Percent complete: 90.7%; Average loss: 0.7866\n",
            "Iteration: 29942; Percent complete: 90.7%; Average loss: 0.6811\n",
            "Iteration: 29943; Percent complete: 90.7%; Average loss: 0.7962\n",
            "Iteration: 29944; Percent complete: 90.7%; Average loss: 0.7479\n",
            "Iteration: 29945; Percent complete: 90.7%; Average loss: 0.8819\n",
            "Iteration: 29946; Percent complete: 90.7%; Average loss: 0.8060\n",
            "Iteration: 29947; Percent complete: 90.7%; Average loss: 0.8331\n",
            "Iteration: 29948; Percent complete: 90.8%; Average loss: 0.6028\n",
            "Iteration: 29949; Percent complete: 90.8%; Average loss: 0.7980\n",
            "Iteration: 29950; Percent complete: 90.8%; Average loss: 0.6612\n",
            "Iteration: 29951; Percent complete: 90.8%; Average loss: 0.7204\n",
            "Iteration: 29952; Percent complete: 90.8%; Average loss: 0.7848\n",
            "Iteration: 29953; Percent complete: 90.8%; Average loss: 0.8558\n",
            "Iteration: 29954; Percent complete: 90.8%; Average loss: 0.8624\n",
            "Iteration: 29955; Percent complete: 90.8%; Average loss: 0.6894\n",
            "Iteration: 29956; Percent complete: 90.8%; Average loss: 0.7188\n",
            "Iteration: 29957; Percent complete: 90.8%; Average loss: 0.7485\n",
            "Iteration: 29958; Percent complete: 90.8%; Average loss: 0.8123\n",
            "Iteration: 29959; Percent complete: 90.8%; Average loss: 0.7105\n",
            "Iteration: 29960; Percent complete: 90.8%; Average loss: 0.7019\n",
            "Iteration: 29961; Percent complete: 90.8%; Average loss: 0.7847\n",
            "Iteration: 29962; Percent complete: 90.8%; Average loss: 0.8358\n",
            "Iteration: 29963; Percent complete: 90.8%; Average loss: 0.7313\n",
            "Iteration: 29964; Percent complete: 90.8%; Average loss: 0.7077\n",
            "Iteration: 29965; Percent complete: 90.8%; Average loss: 0.9164\n",
            "Iteration: 29966; Percent complete: 90.8%; Average loss: 0.7598\n",
            "Iteration: 29967; Percent complete: 90.8%; Average loss: 0.7460\n",
            "Iteration: 29968; Percent complete: 90.8%; Average loss: 0.6320\n",
            "Iteration: 29969; Percent complete: 90.8%; Average loss: 0.6536\n",
            "Iteration: 29970; Percent complete: 90.8%; Average loss: 0.6865\n",
            "Iteration: 29971; Percent complete: 90.8%; Average loss: 0.7598\n",
            "Iteration: 29972; Percent complete: 90.8%; Average loss: 0.6110\n",
            "Iteration: 29973; Percent complete: 90.8%; Average loss: 0.7432\n",
            "Iteration: 29974; Percent complete: 90.8%; Average loss: 0.5295\n",
            "Iteration: 29975; Percent complete: 90.8%; Average loss: 0.7436\n",
            "Iteration: 29976; Percent complete: 90.8%; Average loss: 0.8803\n",
            "Iteration: 29977; Percent complete: 90.8%; Average loss: 0.6864\n",
            "Iteration: 29978; Percent complete: 90.8%; Average loss: 0.5141\n",
            "Iteration: 29979; Percent complete: 90.8%; Average loss: 0.6903\n",
            "Iteration: 29980; Percent complete: 90.8%; Average loss: 0.8847\n",
            "Iteration: 29981; Percent complete: 90.9%; Average loss: 0.7142\n",
            "Iteration: 29982; Percent complete: 90.9%; Average loss: 0.8170\n",
            "Iteration: 29983; Percent complete: 90.9%; Average loss: 0.7174\n",
            "Iteration: 29984; Percent complete: 90.9%; Average loss: 0.7545\n",
            "Iteration: 29985; Percent complete: 90.9%; Average loss: 0.7538\n",
            "Iteration: 29986; Percent complete: 90.9%; Average loss: 0.7380\n",
            "Iteration: 29987; Percent complete: 90.9%; Average loss: 0.8286\n",
            "Iteration: 29988; Percent complete: 90.9%; Average loss: 0.6862\n",
            "Iteration: 29989; Percent complete: 90.9%; Average loss: 0.7519\n",
            "Iteration: 29990; Percent complete: 90.9%; Average loss: 0.8099\n",
            "Iteration: 29991; Percent complete: 90.9%; Average loss: 0.6285\n",
            "Iteration: 29992; Percent complete: 90.9%; Average loss: 0.7626\n",
            "Iteration: 29993; Percent complete: 90.9%; Average loss: 0.6741\n",
            "Iteration: 29994; Percent complete: 90.9%; Average loss: 0.8180\n",
            "Iteration: 29995; Percent complete: 90.9%; Average loss: 0.7218\n",
            "Iteration: 29996; Percent complete: 90.9%; Average loss: 0.7309\n",
            "Iteration: 29997; Percent complete: 90.9%; Average loss: 0.9348\n",
            "Iteration: 29998; Percent complete: 90.9%; Average loss: 0.6612\n",
            "Iteration: 29999; Percent complete: 90.9%; Average loss: 0.7671\n",
            "Iteration: 30000; Percent complete: 90.9%; Average loss: 0.6687\n",
            "Iteration: 30001; Percent complete: 90.9%; Average loss: 0.7354\n",
            "Iteration: 30002; Percent complete: 90.9%; Average loss: 0.6094\n",
            "Iteration: 30003; Percent complete: 90.9%; Average loss: 0.6447\n",
            "Iteration: 30004; Percent complete: 90.9%; Average loss: 0.6747\n",
            "Iteration: 30005; Percent complete: 90.9%; Average loss: 0.8085\n",
            "Iteration: 30006; Percent complete: 90.9%; Average loss: 0.6223\n",
            "Iteration: 30007; Percent complete: 90.9%; Average loss: 0.8854\n",
            "Iteration: 30008; Percent complete: 90.9%; Average loss: 0.6043\n",
            "Iteration: 30009; Percent complete: 90.9%; Average loss: 0.6038\n",
            "Iteration: 30010; Percent complete: 90.9%; Average loss: 0.6402\n",
            "Iteration: 30011; Percent complete: 90.9%; Average loss: 0.8224\n",
            "Iteration: 30012; Percent complete: 90.9%; Average loss: 0.8593\n",
            "Iteration: 30013; Percent complete: 90.9%; Average loss: 0.6796\n",
            "Iteration: 30014; Percent complete: 91.0%; Average loss: 0.6644\n",
            "Iteration: 30015; Percent complete: 91.0%; Average loss: 0.7322\n",
            "Iteration: 30016; Percent complete: 91.0%; Average loss: 0.8296\n",
            "Iteration: 30017; Percent complete: 91.0%; Average loss: 0.8892\n",
            "Iteration: 30018; Percent complete: 91.0%; Average loss: 0.7779\n",
            "Iteration: 30019; Percent complete: 91.0%; Average loss: 0.8385\n",
            "Iteration: 30020; Percent complete: 91.0%; Average loss: 0.6804\n",
            "Iteration: 30021; Percent complete: 91.0%; Average loss: 0.7867\n",
            "Iteration: 30022; Percent complete: 91.0%; Average loss: 0.6616\n",
            "Iteration: 30023; Percent complete: 91.0%; Average loss: 0.7753\n",
            "Iteration: 30024; Percent complete: 91.0%; Average loss: 0.5969\n",
            "Iteration: 30025; Percent complete: 91.0%; Average loss: 0.7635\n",
            "Iteration: 30026; Percent complete: 91.0%; Average loss: 0.8683\n",
            "Iteration: 30027; Percent complete: 91.0%; Average loss: 0.7432\n",
            "Iteration: 30028; Percent complete: 91.0%; Average loss: 0.8309\n",
            "Iteration: 30029; Percent complete: 91.0%; Average loss: 0.8523\n",
            "Iteration: 30030; Percent complete: 91.0%; Average loss: 0.6408\n",
            "Iteration: 30031; Percent complete: 91.0%; Average loss: 0.8010\n",
            "Iteration: 30032; Percent complete: 91.0%; Average loss: 0.8708\n",
            "Iteration: 30033; Percent complete: 91.0%; Average loss: 0.7369\n",
            "Iteration: 30034; Percent complete: 91.0%; Average loss: 0.6983\n",
            "Iteration: 30035; Percent complete: 91.0%; Average loss: 0.7511\n",
            "Iteration: 30036; Percent complete: 91.0%; Average loss: 0.8008\n",
            "Iteration: 30037; Percent complete: 91.0%; Average loss: 0.7081\n",
            "Iteration: 30038; Percent complete: 91.0%; Average loss: 0.7845\n",
            "Iteration: 30039; Percent complete: 91.0%; Average loss: 0.7280\n",
            "Iteration: 30040; Percent complete: 91.0%; Average loss: 0.7420\n",
            "Iteration: 30041; Percent complete: 91.0%; Average loss: 0.8005\n",
            "Iteration: 30042; Percent complete: 91.0%; Average loss: 0.8116\n",
            "Iteration: 30043; Percent complete: 91.0%; Average loss: 0.5618\n",
            "Iteration: 30044; Percent complete: 91.0%; Average loss: 0.7850\n",
            "Iteration: 30045; Percent complete: 91.0%; Average loss: 0.7926\n",
            "Iteration: 30046; Percent complete: 91.0%; Average loss: 0.6799\n",
            "Iteration: 30047; Percent complete: 91.1%; Average loss: 0.6900\n",
            "Iteration: 30048; Percent complete: 91.1%; Average loss: 0.6005\n",
            "Iteration: 30049; Percent complete: 91.1%; Average loss: 0.6755\n",
            "Iteration: 30050; Percent complete: 91.1%; Average loss: 0.6857\n",
            "Iteration: 30051; Percent complete: 91.1%; Average loss: 0.8587\n",
            "Iteration: 30052; Percent complete: 91.1%; Average loss: 0.7644\n",
            "Iteration: 30053; Percent complete: 91.1%; Average loss: 0.7649\n",
            "Iteration: 30054; Percent complete: 91.1%; Average loss: 0.9530\n",
            "Iteration: 30055; Percent complete: 91.1%; Average loss: 0.8694\n",
            "Iteration: 30056; Percent complete: 91.1%; Average loss: 0.7564\n",
            "Iteration: 30057; Percent complete: 91.1%; Average loss: 0.7056\n",
            "Iteration: 30058; Percent complete: 91.1%; Average loss: 0.7975\n",
            "Iteration: 30059; Percent complete: 91.1%; Average loss: 0.6894\n",
            "Iteration: 30060; Percent complete: 91.1%; Average loss: 0.6368\n",
            "Iteration: 30061; Percent complete: 91.1%; Average loss: 0.8042\n",
            "Iteration: 30062; Percent complete: 91.1%; Average loss: 0.6637\n",
            "Iteration: 30063; Percent complete: 91.1%; Average loss: 0.7397\n",
            "Iteration: 30064; Percent complete: 91.1%; Average loss: 0.7071\n",
            "Iteration: 30065; Percent complete: 91.1%; Average loss: 0.6664\n",
            "Iteration: 30066; Percent complete: 91.1%; Average loss: 0.7140\n",
            "Iteration: 30067; Percent complete: 91.1%; Average loss: 0.7268\n",
            "Iteration: 30068; Percent complete: 91.1%; Average loss: 0.7267\n",
            "Iteration: 30069; Percent complete: 91.1%; Average loss: 0.8359\n",
            "Iteration: 30070; Percent complete: 91.1%; Average loss: 0.6530\n",
            "Iteration: 30071; Percent complete: 91.1%; Average loss: 0.7116\n",
            "Iteration: 30072; Percent complete: 91.1%; Average loss: 0.8968\n",
            "Iteration: 30073; Percent complete: 91.1%; Average loss: 0.7996\n",
            "Iteration: 30074; Percent complete: 91.1%; Average loss: 0.7479\n",
            "Iteration: 30075; Percent complete: 91.1%; Average loss: 0.7617\n",
            "Iteration: 30076; Percent complete: 91.1%; Average loss: 0.7170\n",
            "Iteration: 30077; Percent complete: 91.1%; Average loss: 0.7158\n",
            "Iteration: 30078; Percent complete: 91.1%; Average loss: 0.7705\n",
            "Iteration: 30079; Percent complete: 91.1%; Average loss: 0.7307\n",
            "Iteration: 30080; Percent complete: 91.2%; Average loss: 0.8437\n",
            "Iteration: 30081; Percent complete: 91.2%; Average loss: 0.7945\n",
            "Iteration: 30082; Percent complete: 91.2%; Average loss: 0.8020\n",
            "Iteration: 30083; Percent complete: 91.2%; Average loss: 0.7623\n",
            "Iteration: 30084; Percent complete: 91.2%; Average loss: 0.8045\n",
            "Iteration: 30085; Percent complete: 91.2%; Average loss: 0.7943\n",
            "Iteration: 30086; Percent complete: 91.2%; Average loss: 0.7953\n",
            "Iteration: 30087; Percent complete: 91.2%; Average loss: 0.7115\n",
            "Iteration: 30088; Percent complete: 91.2%; Average loss: 0.6639\n",
            "Iteration: 30089; Percent complete: 91.2%; Average loss: 0.6446\n",
            "Iteration: 30090; Percent complete: 91.2%; Average loss: 0.7061\n",
            "Iteration: 30091; Percent complete: 91.2%; Average loss: 0.8267\n",
            "Iteration: 30092; Percent complete: 91.2%; Average loss: 0.5888\n",
            "Iteration: 30093; Percent complete: 91.2%; Average loss: 0.7674\n",
            "Iteration: 30094; Percent complete: 91.2%; Average loss: 0.9126\n",
            "Iteration: 30095; Percent complete: 91.2%; Average loss: 0.7336\n",
            "Iteration: 30096; Percent complete: 91.2%; Average loss: 0.7151\n",
            "Iteration: 30097; Percent complete: 91.2%; Average loss: 0.8043\n",
            "Iteration: 30098; Percent complete: 91.2%; Average loss: 0.7084\n",
            "Iteration: 30099; Percent complete: 91.2%; Average loss: 0.8316\n",
            "Iteration: 30100; Percent complete: 91.2%; Average loss: 0.7840\n",
            "Iteration: 30101; Percent complete: 91.2%; Average loss: 0.7227\n",
            "Iteration: 30102; Percent complete: 91.2%; Average loss: 0.8835\n",
            "Iteration: 30103; Percent complete: 91.2%; Average loss: 0.8188\n",
            "Iteration: 30104; Percent complete: 91.2%; Average loss: 0.6401\n",
            "Iteration: 30105; Percent complete: 91.2%; Average loss: 0.7863\n",
            "Iteration: 30106; Percent complete: 91.2%; Average loss: 0.7971\n",
            "Iteration: 30107; Percent complete: 91.2%; Average loss: 0.8154\n",
            "Iteration: 30108; Percent complete: 91.2%; Average loss: 0.6938\n",
            "Iteration: 30109; Percent complete: 91.2%; Average loss: 0.7100\n",
            "Iteration: 30110; Percent complete: 91.2%; Average loss: 0.5449\n",
            "Iteration: 30111; Percent complete: 91.2%; Average loss: 0.6953\n",
            "Iteration: 30112; Percent complete: 91.2%; Average loss: 0.7396\n",
            "Iteration: 30113; Percent complete: 91.3%; Average loss: 0.7794\n",
            "Iteration: 30114; Percent complete: 91.3%; Average loss: 0.5756\n",
            "Iteration: 30115; Percent complete: 91.3%; Average loss: 0.7066\n",
            "Iteration: 30116; Percent complete: 91.3%; Average loss: 0.5892\n",
            "Iteration: 30117; Percent complete: 91.3%; Average loss: 0.7780\n",
            "Iteration: 30118; Percent complete: 91.3%; Average loss: 0.7484\n",
            "Iteration: 30119; Percent complete: 91.3%; Average loss: 0.6740\n",
            "Iteration: 30120; Percent complete: 91.3%; Average loss: 0.8687\n",
            "Iteration: 30121; Percent complete: 91.3%; Average loss: 0.7171\n",
            "Iteration: 30122; Percent complete: 91.3%; Average loss: 0.7618\n",
            "Iteration: 30123; Percent complete: 91.3%; Average loss: 0.5905\n",
            "Iteration: 30124; Percent complete: 91.3%; Average loss: 0.7389\n",
            "Iteration: 30125; Percent complete: 91.3%; Average loss: 0.9425\n",
            "Iteration: 30126; Percent complete: 91.3%; Average loss: 0.6996\n",
            "Iteration: 30127; Percent complete: 91.3%; Average loss: 0.6750\n",
            "Iteration: 30128; Percent complete: 91.3%; Average loss: 0.8685\n",
            "Iteration: 30129; Percent complete: 91.3%; Average loss: 0.6598\n",
            "Iteration: 30130; Percent complete: 91.3%; Average loss: 0.8086\n",
            "Iteration: 30131; Percent complete: 91.3%; Average loss: 0.7597\n",
            "Iteration: 30132; Percent complete: 91.3%; Average loss: 0.7012\n",
            "Iteration: 30133; Percent complete: 91.3%; Average loss: 0.7729\n",
            "Iteration: 30134; Percent complete: 91.3%; Average loss: 0.7186\n",
            "Iteration: 30135; Percent complete: 91.3%; Average loss: 0.8280\n",
            "Iteration: 30136; Percent complete: 91.3%; Average loss: 0.8728\n",
            "Iteration: 30137; Percent complete: 91.3%; Average loss: 0.8074\n",
            "Iteration: 30138; Percent complete: 91.3%; Average loss: 0.6698\n",
            "Iteration: 30139; Percent complete: 91.3%; Average loss: 0.9563\n",
            "Iteration: 30140; Percent complete: 91.3%; Average loss: 0.7967\n",
            "Iteration: 30141; Percent complete: 91.3%; Average loss: 0.6546\n",
            "Iteration: 30142; Percent complete: 91.3%; Average loss: 0.7604\n",
            "Iteration: 30143; Percent complete: 91.3%; Average loss: 0.7629\n",
            "Iteration: 30144; Percent complete: 91.3%; Average loss: 0.8584\n",
            "Iteration: 30145; Percent complete: 91.3%; Average loss: 0.5627\n",
            "Iteration: 30146; Percent complete: 91.4%; Average loss: 0.7217\n",
            "Iteration: 30147; Percent complete: 91.4%; Average loss: 0.5129\n",
            "Iteration: 30148; Percent complete: 91.4%; Average loss: 0.9427\n",
            "Iteration: 30149; Percent complete: 91.4%; Average loss: 0.8098\n",
            "Iteration: 30150; Percent complete: 91.4%; Average loss: 0.8229\n",
            "Iteration: 30151; Percent complete: 91.4%; Average loss: 0.7276\n",
            "Iteration: 30152; Percent complete: 91.4%; Average loss: 0.7611\n",
            "Iteration: 30153; Percent complete: 91.4%; Average loss: 0.7780\n",
            "Iteration: 30154; Percent complete: 91.4%; Average loss: 0.7570\n",
            "Iteration: 30155; Percent complete: 91.4%; Average loss: 0.5533\n",
            "Iteration: 30156; Percent complete: 91.4%; Average loss: 0.7758\n",
            "Iteration: 30157; Percent complete: 91.4%; Average loss: 0.5816\n",
            "Iteration: 30158; Percent complete: 91.4%; Average loss: 0.6771\n",
            "Iteration: 30159; Percent complete: 91.4%; Average loss: 0.7919\n",
            "Iteration: 30160; Percent complete: 91.4%; Average loss: 0.6493\n",
            "Iteration: 30161; Percent complete: 91.4%; Average loss: 0.7250\n",
            "Iteration: 30162; Percent complete: 91.4%; Average loss: 0.6873\n",
            "Iteration: 30163; Percent complete: 91.4%; Average loss: 0.7603\n",
            "Iteration: 30164; Percent complete: 91.4%; Average loss: 0.8486\n",
            "Iteration: 30165; Percent complete: 91.4%; Average loss: 0.7242\n",
            "Iteration: 30166; Percent complete: 91.4%; Average loss: 0.7443\n",
            "Iteration: 30167; Percent complete: 91.4%; Average loss: 0.7300\n",
            "Iteration: 30168; Percent complete: 91.4%; Average loss: 0.6440\n",
            "Iteration: 30169; Percent complete: 91.4%; Average loss: 0.7216\n",
            "Iteration: 30170; Percent complete: 91.4%; Average loss: 0.6518\n",
            "Iteration: 30171; Percent complete: 91.4%; Average loss: 0.6512\n",
            "Iteration: 30172; Percent complete: 91.4%; Average loss: 0.6109\n",
            "Iteration: 30173; Percent complete: 91.4%; Average loss: 0.7653\n",
            "Iteration: 30174; Percent complete: 91.4%; Average loss: 0.7746\n",
            "Iteration: 30175; Percent complete: 91.4%; Average loss: 0.7829\n",
            "Iteration: 30176; Percent complete: 91.4%; Average loss: 0.8554\n",
            "Iteration: 30177; Percent complete: 91.4%; Average loss: 0.7431\n",
            "Iteration: 30178; Percent complete: 91.4%; Average loss: 0.5853\n",
            "Iteration: 30179; Percent complete: 91.5%; Average loss: 0.8947\n",
            "Iteration: 30180; Percent complete: 91.5%; Average loss: 0.6330\n",
            "Iteration: 30181; Percent complete: 91.5%; Average loss: 0.6804\n",
            "Iteration: 30182; Percent complete: 91.5%; Average loss: 0.5983\n",
            "Iteration: 30183; Percent complete: 91.5%; Average loss: 0.7426\n",
            "Iteration: 30184; Percent complete: 91.5%; Average loss: 0.7666\n",
            "Iteration: 30185; Percent complete: 91.5%; Average loss: 0.7252\n",
            "Iteration: 30186; Percent complete: 91.5%; Average loss: 0.5850\n",
            "Iteration: 30187; Percent complete: 91.5%; Average loss: 0.5409\n",
            "Iteration: 30188; Percent complete: 91.5%; Average loss: 0.7867\n",
            "Iteration: 30189; Percent complete: 91.5%; Average loss: 0.7740\n",
            "Iteration: 30190; Percent complete: 91.5%; Average loss: 0.7593\n",
            "Iteration: 30191; Percent complete: 91.5%; Average loss: 0.7425\n",
            "Iteration: 30192; Percent complete: 91.5%; Average loss: 0.7588\n",
            "Iteration: 30193; Percent complete: 91.5%; Average loss: 0.7000\n",
            "Iteration: 30194; Percent complete: 91.5%; Average loss: 0.7128\n",
            "Iteration: 30195; Percent complete: 91.5%; Average loss: 0.7314\n",
            "Iteration: 30196; Percent complete: 91.5%; Average loss: 0.8839\n",
            "Iteration: 30197; Percent complete: 91.5%; Average loss: 0.7229\n",
            "Iteration: 30198; Percent complete: 91.5%; Average loss: 0.7143\n",
            "Iteration: 30199; Percent complete: 91.5%; Average loss: 0.6454\n",
            "Iteration: 30200; Percent complete: 91.5%; Average loss: 0.6292\n",
            "Iteration: 30201; Percent complete: 91.5%; Average loss: 0.6788\n",
            "Iteration: 30202; Percent complete: 91.5%; Average loss: 0.7400\n",
            "Iteration: 30203; Percent complete: 91.5%; Average loss: 0.7569\n",
            "Iteration: 30204; Percent complete: 91.5%; Average loss: 0.6788\n",
            "Iteration: 30205; Percent complete: 91.5%; Average loss: 0.6202\n",
            "Iteration: 30206; Percent complete: 91.5%; Average loss: 0.7931\n",
            "Iteration: 30207; Percent complete: 91.5%; Average loss: 0.8897\n",
            "Iteration: 30208; Percent complete: 91.5%; Average loss: 0.6800\n",
            "Iteration: 30209; Percent complete: 91.5%; Average loss: 0.5810\n",
            "Iteration: 30210; Percent complete: 91.5%; Average loss: 0.6940\n",
            "Iteration: 30211; Percent complete: 91.5%; Average loss: 0.6639\n",
            "Iteration: 30212; Percent complete: 91.6%; Average loss: 0.7688\n",
            "Iteration: 30213; Percent complete: 91.6%; Average loss: 0.6897\n",
            "Iteration: 30214; Percent complete: 91.6%; Average loss: 0.7475\n",
            "Iteration: 30215; Percent complete: 91.6%; Average loss: 0.6759\n",
            "Iteration: 30216; Percent complete: 91.6%; Average loss: 0.7508\n",
            "Iteration: 30217; Percent complete: 91.6%; Average loss: 0.7281\n",
            "Iteration: 30218; Percent complete: 91.6%; Average loss: 0.6661\n",
            "Iteration: 30219; Percent complete: 91.6%; Average loss: 0.7719\n",
            "Iteration: 30220; Percent complete: 91.6%; Average loss: 0.7695\n",
            "Iteration: 30221; Percent complete: 91.6%; Average loss: 0.8111\n",
            "Iteration: 30222; Percent complete: 91.6%; Average loss: 0.7308\n",
            "Iteration: 30223; Percent complete: 91.6%; Average loss: 0.8305\n",
            "Iteration: 30224; Percent complete: 91.6%; Average loss: 0.7534\n",
            "Iteration: 30225; Percent complete: 91.6%; Average loss: 0.6813\n",
            "Iteration: 30226; Percent complete: 91.6%; Average loss: 0.8191\n",
            "Iteration: 30227; Percent complete: 91.6%; Average loss: 0.7719\n",
            "Iteration: 30228; Percent complete: 91.6%; Average loss: 0.7977\n",
            "Iteration: 30229; Percent complete: 91.6%; Average loss: 0.6932\n",
            "Iteration: 30230; Percent complete: 91.6%; Average loss: 0.6098\n",
            "Iteration: 30231; Percent complete: 91.6%; Average loss: 0.6675\n",
            "Iteration: 30232; Percent complete: 91.6%; Average loss: 0.7162\n",
            "Iteration: 30233; Percent complete: 91.6%; Average loss: 0.8290\n",
            "Iteration: 30234; Percent complete: 91.6%; Average loss: 0.6721\n",
            "Iteration: 30235; Percent complete: 91.6%; Average loss: 0.7154\n",
            "Iteration: 30236; Percent complete: 91.6%; Average loss: 0.7764\n",
            "Iteration: 30237; Percent complete: 91.6%; Average loss: 0.8971\n",
            "Iteration: 30238; Percent complete: 91.6%; Average loss: 0.7458\n",
            "Iteration: 30239; Percent complete: 91.6%; Average loss: 0.7660\n",
            "Iteration: 30240; Percent complete: 91.6%; Average loss: 0.7370\n",
            "Iteration: 30241; Percent complete: 91.6%; Average loss: 0.6874\n",
            "Iteration: 30242; Percent complete: 91.6%; Average loss: 0.7897\n",
            "Iteration: 30243; Percent complete: 91.6%; Average loss: 0.7678\n",
            "Iteration: 30244; Percent complete: 91.6%; Average loss: 0.7797\n",
            "Iteration: 30245; Percent complete: 91.7%; Average loss: 0.6703\n",
            "Iteration: 30246; Percent complete: 91.7%; Average loss: 0.7895\n",
            "Iteration: 30247; Percent complete: 91.7%; Average loss: 0.6365\n",
            "Iteration: 30248; Percent complete: 91.7%; Average loss: 0.7355\n",
            "Iteration: 30249; Percent complete: 91.7%; Average loss: 0.6749\n",
            "Iteration: 30250; Percent complete: 91.7%; Average loss: 0.7929\n",
            "Iteration: 30251; Percent complete: 91.7%; Average loss: 0.7116\n",
            "Iteration: 30252; Percent complete: 91.7%; Average loss: 0.7592\n",
            "Iteration: 30253; Percent complete: 91.7%; Average loss: 0.8494\n",
            "Iteration: 30254; Percent complete: 91.7%; Average loss: 0.7548\n",
            "Iteration: 30255; Percent complete: 91.7%; Average loss: 0.7885\n",
            "Iteration: 30256; Percent complete: 91.7%; Average loss: 0.7799\n",
            "Iteration: 30257; Percent complete: 91.7%; Average loss: 0.7409\n",
            "Iteration: 30258; Percent complete: 91.7%; Average loss: 0.8680\n",
            "Iteration: 30259; Percent complete: 91.7%; Average loss: 0.5700\n",
            "Iteration: 30260; Percent complete: 91.7%; Average loss: 0.7244\n",
            "Iteration: 30261; Percent complete: 91.7%; Average loss: 0.6097\n",
            "Iteration: 30262; Percent complete: 91.7%; Average loss: 0.6887\n",
            "Iteration: 30263; Percent complete: 91.7%; Average loss: 0.6398\n",
            "Iteration: 30264; Percent complete: 91.7%; Average loss: 0.7471\n",
            "Iteration: 30265; Percent complete: 91.7%; Average loss: 0.8509\n",
            "Iteration: 30266; Percent complete: 91.7%; Average loss: 0.7393\n",
            "Iteration: 30267; Percent complete: 91.7%; Average loss: 0.8476\n",
            "Iteration: 30268; Percent complete: 91.7%; Average loss: 0.6594\n",
            "Iteration: 30269; Percent complete: 91.7%; Average loss: 0.6732\n",
            "Iteration: 30270; Percent complete: 91.7%; Average loss: 0.7733\n",
            "Iteration: 30271; Percent complete: 91.7%; Average loss: 0.6316\n",
            "Iteration: 30272; Percent complete: 91.7%; Average loss: 0.6917\n",
            "Iteration: 30273; Percent complete: 91.7%; Average loss: 0.5634\n",
            "Iteration: 30274; Percent complete: 91.7%; Average loss: 0.6153\n",
            "Iteration: 30275; Percent complete: 91.7%; Average loss: 0.8050\n",
            "Iteration: 30276; Percent complete: 91.7%; Average loss: 0.7851\n",
            "Iteration: 30277; Percent complete: 91.7%; Average loss: 0.8044\n",
            "Iteration: 30278; Percent complete: 91.8%; Average loss: 0.6025\n",
            "Iteration: 30279; Percent complete: 91.8%; Average loss: 0.6214\n",
            "Iteration: 30280; Percent complete: 91.8%; Average loss: 0.8215\n",
            "Iteration: 30281; Percent complete: 91.8%; Average loss: 0.6418\n",
            "Iteration: 30282; Percent complete: 91.8%; Average loss: 0.7037\n",
            "Iteration: 30283; Percent complete: 91.8%; Average loss: 0.7672\n",
            "Iteration: 30284; Percent complete: 91.8%; Average loss: 0.9012\n",
            "Iteration: 30285; Percent complete: 91.8%; Average loss: 0.7065\n",
            "Iteration: 30286; Percent complete: 91.8%; Average loss: 0.9428\n",
            "Iteration: 30287; Percent complete: 91.8%; Average loss: 0.7026\n",
            "Iteration: 30288; Percent complete: 91.8%; Average loss: 0.8416\n",
            "Iteration: 30289; Percent complete: 91.8%; Average loss: 0.7465\n",
            "Iteration: 30290; Percent complete: 91.8%; Average loss: 0.9569\n",
            "Iteration: 30291; Percent complete: 91.8%; Average loss: 0.7386\n",
            "Iteration: 30292; Percent complete: 91.8%; Average loss: 0.7327\n",
            "Iteration: 30293; Percent complete: 91.8%; Average loss: 0.6685\n",
            "Iteration: 30294; Percent complete: 91.8%; Average loss: 0.8308\n",
            "Iteration: 30295; Percent complete: 91.8%; Average loss: 0.8797\n",
            "Iteration: 30296; Percent complete: 91.8%; Average loss: 0.7556\n",
            "Iteration: 30297; Percent complete: 91.8%; Average loss: 0.6912\n",
            "Iteration: 30298; Percent complete: 91.8%; Average loss: 0.7250\n",
            "Iteration: 30299; Percent complete: 91.8%; Average loss: 0.8349\n",
            "Iteration: 30300; Percent complete: 91.8%; Average loss: 0.6635\n",
            "Iteration: 30301; Percent complete: 91.8%; Average loss: 0.6815\n",
            "Iteration: 30302; Percent complete: 91.8%; Average loss: 0.6114\n",
            "Iteration: 30303; Percent complete: 91.8%; Average loss: 0.8122\n",
            "Iteration: 30304; Percent complete: 91.8%; Average loss: 0.6417\n",
            "Iteration: 30305; Percent complete: 91.8%; Average loss: 0.6063\n",
            "Iteration: 30306; Percent complete: 91.8%; Average loss: 0.6883\n",
            "Iteration: 30307; Percent complete: 91.8%; Average loss: 0.7017\n",
            "Iteration: 30308; Percent complete: 91.8%; Average loss: 0.6537\n",
            "Iteration: 30309; Percent complete: 91.8%; Average loss: 0.6866\n",
            "Iteration: 30310; Percent complete: 91.8%; Average loss: 0.6168\n",
            "Iteration: 30311; Percent complete: 91.9%; Average loss: 0.6836\n",
            "Iteration: 30312; Percent complete: 91.9%; Average loss: 0.9047\n",
            "Iteration: 30313; Percent complete: 91.9%; Average loss: 0.7675\n",
            "Iteration: 30314; Percent complete: 91.9%; Average loss: 0.5657\n",
            "Iteration: 30315; Percent complete: 91.9%; Average loss: 0.6648\n",
            "Iteration: 30316; Percent complete: 91.9%; Average loss: 0.8864\n",
            "Iteration: 30317; Percent complete: 91.9%; Average loss: 0.7230\n",
            "Iteration: 30318; Percent complete: 91.9%; Average loss: 0.9668\n",
            "Iteration: 30319; Percent complete: 91.9%; Average loss: 0.6741\n",
            "Iteration: 30320; Percent complete: 91.9%; Average loss: 0.5487\n",
            "Iteration: 30321; Percent complete: 91.9%; Average loss: 0.5886\n",
            "Iteration: 30322; Percent complete: 91.9%; Average loss: 0.7551\n",
            "Iteration: 30323; Percent complete: 91.9%; Average loss: 0.8580\n",
            "Iteration: 30324; Percent complete: 91.9%; Average loss: 0.7512\n",
            "Iteration: 30325; Percent complete: 91.9%; Average loss: 0.6987\n",
            "Iteration: 30326; Percent complete: 91.9%; Average loss: 0.6457\n",
            "Iteration: 30327; Percent complete: 91.9%; Average loss: 0.7992\n",
            "Iteration: 30328; Percent complete: 91.9%; Average loss: 0.8425\n",
            "Iteration: 30329; Percent complete: 91.9%; Average loss: 0.6628\n",
            "Iteration: 30330; Percent complete: 91.9%; Average loss: 0.5341\n",
            "Iteration: 30331; Percent complete: 91.9%; Average loss: 0.4963\n",
            "Iteration: 30332; Percent complete: 91.9%; Average loss: 0.8499\n",
            "Iteration: 30333; Percent complete: 91.9%; Average loss: 0.6736\n",
            "Iteration: 30334; Percent complete: 91.9%; Average loss: 0.6750\n",
            "Iteration: 30335; Percent complete: 91.9%; Average loss: 0.6448\n",
            "Iteration: 30336; Percent complete: 91.9%; Average loss: 0.6801\n",
            "Iteration: 30337; Percent complete: 91.9%; Average loss: 0.7455\n",
            "Iteration: 30338; Percent complete: 91.9%; Average loss: 0.8114\n",
            "Iteration: 30339; Percent complete: 91.9%; Average loss: 0.5944\n",
            "Iteration: 30340; Percent complete: 91.9%; Average loss: 0.6715\n",
            "Iteration: 30341; Percent complete: 91.9%; Average loss: 0.7271\n",
            "Iteration: 30342; Percent complete: 91.9%; Average loss: 0.5145\n",
            "Iteration: 30343; Percent complete: 91.9%; Average loss: 0.6200\n",
            "Iteration: 30344; Percent complete: 92.0%; Average loss: 0.7442\n",
            "Iteration: 30345; Percent complete: 92.0%; Average loss: 0.6964\n",
            "Iteration: 30346; Percent complete: 92.0%; Average loss: 0.7499\n",
            "Iteration: 30347; Percent complete: 92.0%; Average loss: 0.7254\n",
            "Iteration: 30348; Percent complete: 92.0%; Average loss: 0.7870\n",
            "Iteration: 30349; Percent complete: 92.0%; Average loss: 0.7105\n",
            "Iteration: 30350; Percent complete: 92.0%; Average loss: 0.7136\n",
            "Iteration: 30351; Percent complete: 92.0%; Average loss: 0.8205\n",
            "Iteration: 30352; Percent complete: 92.0%; Average loss: 0.6807\n",
            "Iteration: 30353; Percent complete: 92.0%; Average loss: 0.6825\n",
            "Iteration: 30354; Percent complete: 92.0%; Average loss: 0.6243\n",
            "Iteration: 30355; Percent complete: 92.0%; Average loss: 0.7453\n",
            "Iteration: 30356; Percent complete: 92.0%; Average loss: 0.7594\n",
            "Iteration: 30357; Percent complete: 92.0%; Average loss: 0.7284\n",
            "Iteration: 30358; Percent complete: 92.0%; Average loss: 0.7152\n",
            "Iteration: 30359; Percent complete: 92.0%; Average loss: 0.9069\n",
            "Iteration: 30360; Percent complete: 92.0%; Average loss: 0.5695\n",
            "Iteration: 30361; Percent complete: 92.0%; Average loss: 0.7525\n",
            "Iteration: 30362; Percent complete: 92.0%; Average loss: 0.7600\n",
            "Iteration: 30363; Percent complete: 92.0%; Average loss: 0.8708\n",
            "Iteration: 30364; Percent complete: 92.0%; Average loss: 0.7791\n",
            "Iteration: 30365; Percent complete: 92.0%; Average loss: 0.7288\n",
            "Iteration: 30366; Percent complete: 92.0%; Average loss: 0.7596\n",
            "Iteration: 30367; Percent complete: 92.0%; Average loss: 0.8257\n",
            "Iteration: 30368; Percent complete: 92.0%; Average loss: 0.7010\n",
            "Iteration: 30369; Percent complete: 92.0%; Average loss: 0.6674\n",
            "Iteration: 30370; Percent complete: 92.0%; Average loss: 0.9000\n",
            "Iteration: 30371; Percent complete: 92.0%; Average loss: 0.6687\n",
            "Iteration: 30372; Percent complete: 92.0%; Average loss: 0.8279\n",
            "Iteration: 30373; Percent complete: 92.0%; Average loss: 0.7492\n",
            "Iteration: 30374; Percent complete: 92.0%; Average loss: 0.8345\n",
            "Iteration: 30375; Percent complete: 92.0%; Average loss: 0.8079\n",
            "Iteration: 30376; Percent complete: 92.0%; Average loss: 0.7145\n",
            "Iteration: 30377; Percent complete: 92.1%; Average loss: 0.8169\n",
            "Iteration: 30378; Percent complete: 92.1%; Average loss: 0.7728\n",
            "Iteration: 30379; Percent complete: 92.1%; Average loss: 0.7669\n",
            "Iteration: 30380; Percent complete: 92.1%; Average loss: 0.6785\n",
            "Iteration: 30381; Percent complete: 92.1%; Average loss: 0.6153\n",
            "Iteration: 30382; Percent complete: 92.1%; Average loss: 0.7160\n",
            "Iteration: 30383; Percent complete: 92.1%; Average loss: 0.7199\n",
            "Iteration: 30384; Percent complete: 92.1%; Average loss: 0.7274\n",
            "Iteration: 30385; Percent complete: 92.1%; Average loss: 0.6646\n",
            "Iteration: 30386; Percent complete: 92.1%; Average loss: 0.9133\n",
            "Iteration: 30387; Percent complete: 92.1%; Average loss: 0.7104\n",
            "Iteration: 30388; Percent complete: 92.1%; Average loss: 0.7859\n",
            "Iteration: 30389; Percent complete: 92.1%; Average loss: 0.6682\n",
            "Iteration: 30390; Percent complete: 92.1%; Average loss: 0.8169\n",
            "Iteration: 30391; Percent complete: 92.1%; Average loss: 0.7388\n",
            "Iteration: 30392; Percent complete: 92.1%; Average loss: 0.7350\n",
            "Iteration: 30393; Percent complete: 92.1%; Average loss: 0.6339\n",
            "Iteration: 30394; Percent complete: 92.1%; Average loss: 0.7021\n",
            "Iteration: 30395; Percent complete: 92.1%; Average loss: 0.7657\n",
            "Iteration: 30396; Percent complete: 92.1%; Average loss: 0.7740\n",
            "Iteration: 30397; Percent complete: 92.1%; Average loss: 0.5555\n",
            "Iteration: 30398; Percent complete: 92.1%; Average loss: 0.6720\n",
            "Iteration: 30399; Percent complete: 92.1%; Average loss: 0.6616\n",
            "Iteration: 30400; Percent complete: 92.1%; Average loss: 0.9060\n",
            "Iteration: 30401; Percent complete: 92.1%; Average loss: 0.6887\n",
            "Iteration: 30402; Percent complete: 92.1%; Average loss: 0.6681\n",
            "Iteration: 30403; Percent complete: 92.1%; Average loss: 0.6923\n",
            "Iteration: 30404; Percent complete: 92.1%; Average loss: 0.7290\n",
            "Iteration: 30405; Percent complete: 92.1%; Average loss: 0.7750\n",
            "Iteration: 30406; Percent complete: 92.1%; Average loss: 0.7671\n",
            "Iteration: 30407; Percent complete: 92.1%; Average loss: 0.7379\n",
            "Iteration: 30408; Percent complete: 92.1%; Average loss: 0.6723\n",
            "Iteration: 30409; Percent complete: 92.1%; Average loss: 0.5851\n",
            "Iteration: 30410; Percent complete: 92.2%; Average loss: 0.6557\n",
            "Iteration: 30411; Percent complete: 92.2%; Average loss: 0.5918\n",
            "Iteration: 30412; Percent complete: 92.2%; Average loss: 0.5040\n",
            "Iteration: 30413; Percent complete: 92.2%; Average loss: 0.6979\n",
            "Iteration: 30414; Percent complete: 92.2%; Average loss: 0.8274\n",
            "Iteration: 30415; Percent complete: 92.2%; Average loss: 0.7969\n",
            "Iteration: 30416; Percent complete: 92.2%; Average loss: 0.8348\n",
            "Iteration: 30417; Percent complete: 92.2%; Average loss: 0.7177\n",
            "Iteration: 30418; Percent complete: 92.2%; Average loss: 0.7963\n",
            "Iteration: 30419; Percent complete: 92.2%; Average loss: 0.7539\n",
            "Iteration: 30420; Percent complete: 92.2%; Average loss: 0.7395\n",
            "Iteration: 30421; Percent complete: 92.2%; Average loss: 0.7816\n",
            "Iteration: 30422; Percent complete: 92.2%; Average loss: 0.5773\n",
            "Iteration: 30423; Percent complete: 92.2%; Average loss: 0.5832\n",
            "Iteration: 30424; Percent complete: 92.2%; Average loss: 0.7597\n",
            "Iteration: 30425; Percent complete: 92.2%; Average loss: 0.7082\n",
            "Iteration: 30426; Percent complete: 92.2%; Average loss: 0.8484\n",
            "Iteration: 30427; Percent complete: 92.2%; Average loss: 0.8213\n",
            "Iteration: 30428; Percent complete: 92.2%; Average loss: 0.7251\n",
            "Iteration: 30429; Percent complete: 92.2%; Average loss: 0.6383\n",
            "Iteration: 30430; Percent complete: 92.2%; Average loss: 0.6833\n",
            "Iteration: 30431; Percent complete: 92.2%; Average loss: 0.7684\n",
            "Iteration: 30432; Percent complete: 92.2%; Average loss: 0.6997\n",
            "Iteration: 30433; Percent complete: 92.2%; Average loss: 0.8633\n",
            "Iteration: 30434; Percent complete: 92.2%; Average loss: 0.7987\n",
            "Iteration: 30435; Percent complete: 92.2%; Average loss: 0.7251\n",
            "Iteration: 30436; Percent complete: 92.2%; Average loss: 0.8505\n",
            "Iteration: 30437; Percent complete: 92.2%; Average loss: 0.5472\n",
            "Iteration: 30438; Percent complete: 92.2%; Average loss: 0.8315\n",
            "Iteration: 30439; Percent complete: 92.2%; Average loss: 0.8034\n",
            "Iteration: 30440; Percent complete: 92.2%; Average loss: 0.8110\n",
            "Iteration: 30441; Percent complete: 92.2%; Average loss: 0.7101\n",
            "Iteration: 30442; Percent complete: 92.2%; Average loss: 0.6449\n",
            "Iteration: 30443; Percent complete: 92.3%; Average loss: 0.6139\n",
            "Iteration: 30444; Percent complete: 92.3%; Average loss: 0.7899\n",
            "Iteration: 30445; Percent complete: 92.3%; Average loss: 0.6409\n",
            "Iteration: 30446; Percent complete: 92.3%; Average loss: 0.7640\n",
            "Iteration: 30447; Percent complete: 92.3%; Average loss: 0.7082\n",
            "Iteration: 30448; Percent complete: 92.3%; Average loss: 0.9081\n",
            "Iteration: 30449; Percent complete: 92.3%; Average loss: 0.6538\n",
            "Iteration: 30450; Percent complete: 92.3%; Average loss: 0.6347\n",
            "Iteration: 30451; Percent complete: 92.3%; Average loss: 0.8384\n",
            "Iteration: 30452; Percent complete: 92.3%; Average loss: 0.8206\n",
            "Iteration: 30453; Percent complete: 92.3%; Average loss: 0.6284\n",
            "Iteration: 30454; Percent complete: 92.3%; Average loss: 0.6827\n",
            "Iteration: 30455; Percent complete: 92.3%; Average loss: 0.9270\n",
            "Iteration: 30456; Percent complete: 92.3%; Average loss: 0.6777\n",
            "Iteration: 30457; Percent complete: 92.3%; Average loss: 0.6618\n",
            "Iteration: 30458; Percent complete: 92.3%; Average loss: 0.6980\n",
            "Iteration: 30459; Percent complete: 92.3%; Average loss: 0.8044\n",
            "Iteration: 30460; Percent complete: 92.3%; Average loss: 0.8377\n",
            "Iteration: 30461; Percent complete: 92.3%; Average loss: 0.6453\n",
            "Iteration: 30462; Percent complete: 92.3%; Average loss: 0.8299\n",
            "Iteration: 30463; Percent complete: 92.3%; Average loss: 0.6502\n",
            "Iteration: 30464; Percent complete: 92.3%; Average loss: 0.6746\n",
            "Iteration: 30465; Percent complete: 92.3%; Average loss: 0.4850\n",
            "Iteration: 30466; Percent complete: 92.3%; Average loss: 0.7401\n",
            "Iteration: 30467; Percent complete: 92.3%; Average loss: 0.5040\n",
            "Iteration: 30468; Percent complete: 92.3%; Average loss: 0.8238\n",
            "Iteration: 30469; Percent complete: 92.3%; Average loss: 0.6688\n",
            "Iteration: 30470; Percent complete: 92.3%; Average loss: 0.6859\n",
            "Iteration: 30471; Percent complete: 92.3%; Average loss: 0.5951\n",
            "Iteration: 30472; Percent complete: 92.3%; Average loss: 0.7375\n",
            "Iteration: 30473; Percent complete: 92.3%; Average loss: 0.7671\n",
            "Iteration: 30474; Percent complete: 92.3%; Average loss: 0.7822\n",
            "Iteration: 30475; Percent complete: 92.3%; Average loss: 0.8519\n",
            "Iteration: 30476; Percent complete: 92.4%; Average loss: 0.7327\n",
            "Iteration: 30477; Percent complete: 92.4%; Average loss: 0.6763\n",
            "Iteration: 30478; Percent complete: 92.4%; Average loss: 0.5156\n",
            "Iteration: 30479; Percent complete: 92.4%; Average loss: 0.7942\n",
            "Iteration: 30480; Percent complete: 92.4%; Average loss: 0.6196\n",
            "Iteration: 30481; Percent complete: 92.4%; Average loss: 0.6959\n",
            "Iteration: 30482; Percent complete: 92.4%; Average loss: 0.7756\n",
            "Iteration: 30483; Percent complete: 92.4%; Average loss: 0.7952\n",
            "Iteration: 30484; Percent complete: 92.4%; Average loss: 0.6978\n",
            "Iteration: 30485; Percent complete: 92.4%; Average loss: 0.5519\n",
            "Iteration: 30486; Percent complete: 92.4%; Average loss: 0.7247\n",
            "Iteration: 30487; Percent complete: 92.4%; Average loss: 0.6949\n",
            "Iteration: 30488; Percent complete: 92.4%; Average loss: 0.7281\n",
            "Iteration: 30489; Percent complete: 92.4%; Average loss: 0.6376\n",
            "Iteration: 30490; Percent complete: 92.4%; Average loss: 0.8280\n",
            "Iteration: 30491; Percent complete: 92.4%; Average loss: 0.6743\n",
            "Iteration: 30492; Percent complete: 92.4%; Average loss: 0.7616\n",
            "Iteration: 30493; Percent complete: 92.4%; Average loss: 0.6921\n",
            "Iteration: 30494; Percent complete: 92.4%; Average loss: 0.7757\n",
            "Iteration: 30495; Percent complete: 92.4%; Average loss: 0.7568\n",
            "Iteration: 30496; Percent complete: 92.4%; Average loss: 0.9130\n",
            "Iteration: 30497; Percent complete: 92.4%; Average loss: 0.7529\n",
            "Iteration: 30498; Percent complete: 92.4%; Average loss: 0.7035\n",
            "Iteration: 30499; Percent complete: 92.4%; Average loss: 0.6183\n",
            "Iteration: 30500; Percent complete: 92.4%; Average loss: 0.7320\n",
            "Iteration: 30501; Percent complete: 92.4%; Average loss: 0.6966\n",
            "Iteration: 30502; Percent complete: 92.4%; Average loss: 0.6616\n",
            "Iteration: 30503; Percent complete: 92.4%; Average loss: 0.8381\n",
            "Iteration: 30504; Percent complete: 92.4%; Average loss: 0.7632\n",
            "Iteration: 30505; Percent complete: 92.4%; Average loss: 0.7260\n",
            "Iteration: 30506; Percent complete: 92.4%; Average loss: 0.6628\n",
            "Iteration: 30507; Percent complete: 92.4%; Average loss: 0.6976\n",
            "Iteration: 30508; Percent complete: 92.4%; Average loss: 0.6420\n",
            "Iteration: 30509; Percent complete: 92.5%; Average loss: 0.6260\n",
            "Iteration: 30510; Percent complete: 92.5%; Average loss: 0.8316\n",
            "Iteration: 30511; Percent complete: 92.5%; Average loss: 0.6489\n",
            "Iteration: 30512; Percent complete: 92.5%; Average loss: 0.8385\n",
            "Iteration: 30513; Percent complete: 92.5%; Average loss: 0.5152\n",
            "Iteration: 30514; Percent complete: 92.5%; Average loss: 0.7674\n",
            "Iteration: 30515; Percent complete: 92.5%; Average loss: 0.6005\n",
            "Iteration: 30516; Percent complete: 92.5%; Average loss: 0.6884\n",
            "Iteration: 30517; Percent complete: 92.5%; Average loss: 0.7313\n",
            "Iteration: 30518; Percent complete: 92.5%; Average loss: 0.6644\n",
            "Iteration: 30519; Percent complete: 92.5%; Average loss: 0.7589\n",
            "Iteration: 30520; Percent complete: 92.5%; Average loss: 0.8531\n",
            "Iteration: 30521; Percent complete: 92.5%; Average loss: 0.8154\n",
            "Iteration: 30522; Percent complete: 92.5%; Average loss: 0.8796\n",
            "Iteration: 30523; Percent complete: 92.5%; Average loss: 0.7697\n",
            "Iteration: 30524; Percent complete: 92.5%; Average loss: 0.8084\n",
            "Iteration: 30525; Percent complete: 92.5%; Average loss: 0.7322\n",
            "Iteration: 30526; Percent complete: 92.5%; Average loss: 0.9560\n",
            "Iteration: 30527; Percent complete: 92.5%; Average loss: 0.6127\n",
            "Iteration: 30528; Percent complete: 92.5%; Average loss: 0.8465\n",
            "Iteration: 30529; Percent complete: 92.5%; Average loss: 0.8678\n",
            "Iteration: 30530; Percent complete: 92.5%; Average loss: 0.7486\n",
            "Iteration: 30531; Percent complete: 92.5%; Average loss: 0.7398\n",
            "Iteration: 30532; Percent complete: 92.5%; Average loss: 0.7427\n",
            "Iteration: 30533; Percent complete: 92.5%; Average loss: 0.5796\n",
            "Iteration: 30534; Percent complete: 92.5%; Average loss: 0.6875\n",
            "Iteration: 30535; Percent complete: 92.5%; Average loss: 0.7662\n",
            "Iteration: 30536; Percent complete: 92.5%; Average loss: 0.6231\n",
            "Iteration: 30537; Percent complete: 92.5%; Average loss: 0.7276\n",
            "Iteration: 30538; Percent complete: 92.5%; Average loss: 0.6258\n",
            "Iteration: 30539; Percent complete: 92.5%; Average loss: 0.5703\n",
            "Iteration: 30540; Percent complete: 92.5%; Average loss: 0.7638\n",
            "Iteration: 30541; Percent complete: 92.5%; Average loss: 0.6919\n",
            "Iteration: 30542; Percent complete: 92.6%; Average loss: 0.6837\n",
            "Iteration: 30543; Percent complete: 92.6%; Average loss: 0.7556\n",
            "Iteration: 30544; Percent complete: 92.6%; Average loss: 0.7517\n",
            "Iteration: 30545; Percent complete: 92.6%; Average loss: 0.6664\n",
            "Iteration: 30546; Percent complete: 92.6%; Average loss: 0.6161\n",
            "Iteration: 30547; Percent complete: 92.6%; Average loss: 0.8887\n",
            "Iteration: 30548; Percent complete: 92.6%; Average loss: 0.6140\n",
            "Iteration: 30549; Percent complete: 92.6%; Average loss: 0.8565\n",
            "Iteration: 30550; Percent complete: 92.6%; Average loss: 0.7663\n",
            "Iteration: 30551; Percent complete: 92.6%; Average loss: 0.6242\n",
            "Iteration: 30552; Percent complete: 92.6%; Average loss: 0.7025\n",
            "Iteration: 30553; Percent complete: 92.6%; Average loss: 0.5998\n",
            "Iteration: 30554; Percent complete: 92.6%; Average loss: 0.6571\n",
            "Iteration: 30555; Percent complete: 92.6%; Average loss: 0.7627\n",
            "Iteration: 30556; Percent complete: 92.6%; Average loss: 0.6553\n",
            "Iteration: 30557; Percent complete: 92.6%; Average loss: 0.6502\n",
            "Iteration: 30558; Percent complete: 92.6%; Average loss: 0.7786\n",
            "Iteration: 30559; Percent complete: 92.6%; Average loss: 0.6336\n",
            "Iteration: 30560; Percent complete: 92.6%; Average loss: 0.7201\n",
            "Iteration: 30561; Percent complete: 92.6%; Average loss: 0.9456\n",
            "Iteration: 30562; Percent complete: 92.6%; Average loss: 0.5732\n",
            "Iteration: 30563; Percent complete: 92.6%; Average loss: 0.7381\n",
            "Iteration: 30564; Percent complete: 92.6%; Average loss: 0.7032\n",
            "Iteration: 30565; Percent complete: 92.6%; Average loss: 0.7869\n",
            "Iteration: 30566; Percent complete: 92.6%; Average loss: 0.7541\n",
            "Iteration: 30567; Percent complete: 92.6%; Average loss: 0.8310\n",
            "Iteration: 30568; Percent complete: 92.6%; Average loss: 0.7947\n",
            "Iteration: 30569; Percent complete: 92.6%; Average loss: 0.7526\n",
            "Iteration: 30570; Percent complete: 92.6%; Average loss: 0.6783\n",
            "Iteration: 30571; Percent complete: 92.6%; Average loss: 0.6844\n",
            "Iteration: 30572; Percent complete: 92.6%; Average loss: 0.8690\n",
            "Iteration: 30573; Percent complete: 92.6%; Average loss: 0.6258\n",
            "Iteration: 30574; Percent complete: 92.6%; Average loss: 0.7808\n",
            "Iteration: 30575; Percent complete: 92.7%; Average loss: 0.8058\n",
            "Iteration: 30576; Percent complete: 92.7%; Average loss: 0.7389\n",
            "Iteration: 30577; Percent complete: 92.7%; Average loss: 0.6623\n",
            "Iteration: 30578; Percent complete: 92.7%; Average loss: 0.7692\n",
            "Iteration: 30579; Percent complete: 92.7%; Average loss: 0.7735\n",
            "Iteration: 30580; Percent complete: 92.7%; Average loss: 0.6704\n",
            "Iteration: 30581; Percent complete: 92.7%; Average loss: 0.8169\n",
            "Iteration: 30582; Percent complete: 92.7%; Average loss: 0.8175\n",
            "Iteration: 30583; Percent complete: 92.7%; Average loss: 0.6652\n",
            "Iteration: 30584; Percent complete: 92.7%; Average loss: 0.7533\n",
            "Iteration: 30585; Percent complete: 92.7%; Average loss: 0.7213\n",
            "Iteration: 30586; Percent complete: 92.7%; Average loss: 0.6439\n",
            "Iteration: 30587; Percent complete: 92.7%; Average loss: 0.6856\n",
            "Iteration: 30588; Percent complete: 92.7%; Average loss: 0.7368\n",
            "Iteration: 30589; Percent complete: 92.7%; Average loss: 0.7723\n",
            "Iteration: 30590; Percent complete: 92.7%; Average loss: 0.8006\n",
            "Iteration: 30591; Percent complete: 92.7%; Average loss: 0.8062\n",
            "Iteration: 30592; Percent complete: 92.7%; Average loss: 0.6680\n",
            "Iteration: 30593; Percent complete: 92.7%; Average loss: 0.7623\n",
            "Iteration: 30594; Percent complete: 92.7%; Average loss: 0.6504\n",
            "Iteration: 30595; Percent complete: 92.7%; Average loss: 0.7257\n",
            "Iteration: 30596; Percent complete: 92.7%; Average loss: 0.6627\n",
            "Iteration: 30597; Percent complete: 92.7%; Average loss: 0.7849\n",
            "Iteration: 30598; Percent complete: 92.7%; Average loss: 0.8003\n",
            "Iteration: 30599; Percent complete: 92.7%; Average loss: 0.8584\n",
            "Iteration: 30600; Percent complete: 92.7%; Average loss: 0.7131\n",
            "Iteration: 30601; Percent complete: 92.7%; Average loss: 0.7273\n",
            "Iteration: 30602; Percent complete: 92.7%; Average loss: 0.6237\n",
            "Iteration: 30603; Percent complete: 92.7%; Average loss: 0.7517\n",
            "Iteration: 30604; Percent complete: 92.7%; Average loss: 0.6746\n",
            "Iteration: 30605; Percent complete: 92.7%; Average loss: 0.9313\n",
            "Iteration: 30606; Percent complete: 92.7%; Average loss: 0.6028\n",
            "Iteration: 30607; Percent complete: 92.7%; Average loss: 0.6725\n",
            "Iteration: 30608; Percent complete: 92.8%; Average loss: 0.8177\n",
            "Iteration: 30609; Percent complete: 92.8%; Average loss: 0.6558\n",
            "Iteration: 30610; Percent complete: 92.8%; Average loss: 0.7642\n",
            "Iteration: 30611; Percent complete: 92.8%; Average loss: 0.7120\n",
            "Iteration: 30612; Percent complete: 92.8%; Average loss: 0.6323\n",
            "Iteration: 30613; Percent complete: 92.8%; Average loss: 0.8034\n",
            "Iteration: 30614; Percent complete: 92.8%; Average loss: 0.7568\n",
            "Iteration: 30615; Percent complete: 92.8%; Average loss: 0.7110\n",
            "Iteration: 30616; Percent complete: 92.8%; Average loss: 0.7923\n",
            "Iteration: 30617; Percent complete: 92.8%; Average loss: 0.8674\n",
            "Iteration: 30618; Percent complete: 92.8%; Average loss: 0.6811\n",
            "Iteration: 30619; Percent complete: 92.8%; Average loss: 0.6860\n",
            "Iteration: 30620; Percent complete: 92.8%; Average loss: 0.6774\n",
            "Iteration: 30621; Percent complete: 92.8%; Average loss: 0.7211\n",
            "Iteration: 30622; Percent complete: 92.8%; Average loss: 0.7821\n",
            "Iteration: 30623; Percent complete: 92.8%; Average loss: 0.9189\n",
            "Iteration: 30624; Percent complete: 92.8%; Average loss: 0.6175\n",
            "Iteration: 30625; Percent complete: 92.8%; Average loss: 0.7519\n",
            "Iteration: 30626; Percent complete: 92.8%; Average loss: 0.8441\n",
            "Iteration: 30627; Percent complete: 92.8%; Average loss: 0.9460\n",
            "Iteration: 30628; Percent complete: 92.8%; Average loss: 0.8684\n",
            "Iteration: 30629; Percent complete: 92.8%; Average loss: 0.6942\n",
            "Iteration: 30630; Percent complete: 92.8%; Average loss: 0.7397\n",
            "Iteration: 30631; Percent complete: 92.8%; Average loss: 0.6238\n",
            "Iteration: 30632; Percent complete: 92.8%; Average loss: 0.7909\n",
            "Iteration: 30633; Percent complete: 92.8%; Average loss: 0.6911\n",
            "Iteration: 30634; Percent complete: 92.8%; Average loss: 0.7016\n",
            "Iteration: 30635; Percent complete: 92.8%; Average loss: 0.6497\n",
            "Iteration: 30636; Percent complete: 92.8%; Average loss: 0.7925\n",
            "Iteration: 30637; Percent complete: 92.8%; Average loss: 0.8027\n",
            "Iteration: 30638; Percent complete: 92.8%; Average loss: 0.6887\n",
            "Iteration: 30639; Percent complete: 92.8%; Average loss: 0.7565\n",
            "Iteration: 30640; Percent complete: 92.8%; Average loss: 0.7355\n",
            "Iteration: 30641; Percent complete: 92.9%; Average loss: 0.7688\n",
            "Iteration: 30642; Percent complete: 92.9%; Average loss: 0.7839\n",
            "Iteration: 30643; Percent complete: 92.9%; Average loss: 0.8049\n",
            "Iteration: 30644; Percent complete: 92.9%; Average loss: 0.6780\n",
            "Iteration: 30645; Percent complete: 92.9%; Average loss: 0.6721\n",
            "Iteration: 30646; Percent complete: 92.9%; Average loss: 0.6743\n",
            "Iteration: 30647; Percent complete: 92.9%; Average loss: 0.6632\n",
            "Iteration: 30648; Percent complete: 92.9%; Average loss: 0.7068\n",
            "Iteration: 30649; Percent complete: 92.9%; Average loss: 0.8298\n",
            "Iteration: 30650; Percent complete: 92.9%; Average loss: 0.7109\n",
            "Iteration: 30651; Percent complete: 92.9%; Average loss: 0.7158\n",
            "Iteration: 30652; Percent complete: 92.9%; Average loss: 0.7084\n",
            "Iteration: 30653; Percent complete: 92.9%; Average loss: 0.6574\n",
            "Iteration: 30654; Percent complete: 92.9%; Average loss: 0.7929\n",
            "Iteration: 30655; Percent complete: 92.9%; Average loss: 0.7765\n",
            "Iteration: 30656; Percent complete: 92.9%; Average loss: 0.7196\n",
            "Iteration: 30657; Percent complete: 92.9%; Average loss: 0.7660\n",
            "Iteration: 30658; Percent complete: 92.9%; Average loss: 0.6670\n",
            "Iteration: 30659; Percent complete: 92.9%; Average loss: 0.6111\n",
            "Iteration: 30660; Percent complete: 92.9%; Average loss: 0.6608\n",
            "Iteration: 30661; Percent complete: 92.9%; Average loss: 0.8259\n",
            "Iteration: 30662; Percent complete: 92.9%; Average loss: 0.6987\n",
            "Iteration: 30663; Percent complete: 92.9%; Average loss: 0.8581\n",
            "Iteration: 30664; Percent complete: 92.9%; Average loss: 0.6696\n",
            "Iteration: 30665; Percent complete: 92.9%; Average loss: 0.7611\n",
            "Iteration: 30666; Percent complete: 92.9%; Average loss: 0.6411\n",
            "Iteration: 30667; Percent complete: 92.9%; Average loss: 0.5999\n",
            "Iteration: 30668; Percent complete: 92.9%; Average loss: 0.6749\n",
            "Iteration: 30669; Percent complete: 92.9%; Average loss: 0.7082\n",
            "Iteration: 30670; Percent complete: 92.9%; Average loss: 0.6086\n",
            "Iteration: 30671; Percent complete: 92.9%; Average loss: 0.9258\n",
            "Iteration: 30672; Percent complete: 92.9%; Average loss: 0.6466\n",
            "Iteration: 30673; Percent complete: 92.9%; Average loss: 0.7113\n",
            "Iteration: 30674; Percent complete: 93.0%; Average loss: 0.6356\n",
            "Iteration: 30675; Percent complete: 93.0%; Average loss: 0.7544\n",
            "Iteration: 30676; Percent complete: 93.0%; Average loss: 0.8211\n",
            "Iteration: 30677; Percent complete: 93.0%; Average loss: 0.6411\n",
            "Iteration: 30678; Percent complete: 93.0%; Average loss: 0.8103\n",
            "Iteration: 30679; Percent complete: 93.0%; Average loss: 0.6804\n",
            "Iteration: 30680; Percent complete: 93.0%; Average loss: 0.7241\n",
            "Iteration: 30681; Percent complete: 93.0%; Average loss: 0.6495\n",
            "Iteration: 30682; Percent complete: 93.0%; Average loss: 0.5869\n",
            "Iteration: 30683; Percent complete: 93.0%; Average loss: 0.7034\n",
            "Iteration: 30684; Percent complete: 93.0%; Average loss: 0.5506\n",
            "Iteration: 30685; Percent complete: 93.0%; Average loss: 0.6314\n",
            "Iteration: 30686; Percent complete: 93.0%; Average loss: 0.6814\n",
            "Iteration: 30687; Percent complete: 93.0%; Average loss: 0.8534\n",
            "Iteration: 30688; Percent complete: 93.0%; Average loss: 0.6524\n",
            "Iteration: 30689; Percent complete: 93.0%; Average loss: 0.8555\n",
            "Iteration: 30690; Percent complete: 93.0%; Average loss: 0.6340\n",
            "Iteration: 30691; Percent complete: 93.0%; Average loss: 0.6405\n",
            "Iteration: 30692; Percent complete: 93.0%; Average loss: 0.7861\n",
            "Iteration: 30693; Percent complete: 93.0%; Average loss: 0.6336\n",
            "Iteration: 30694; Percent complete: 93.0%; Average loss: 0.8970\n",
            "Iteration: 30695; Percent complete: 93.0%; Average loss: 0.7403\n",
            "Iteration: 30696; Percent complete: 93.0%; Average loss: 0.7305\n",
            "Iteration: 30697; Percent complete: 93.0%; Average loss: 0.5987\n",
            "Iteration: 30698; Percent complete: 93.0%; Average loss: 0.7362\n",
            "Iteration: 30699; Percent complete: 93.0%; Average loss: 0.8044\n",
            "Iteration: 30700; Percent complete: 93.0%; Average loss: 0.5431\n",
            "Iteration: 30701; Percent complete: 93.0%; Average loss: 0.5202\n",
            "Iteration: 30702; Percent complete: 93.0%; Average loss: 0.6889\n",
            "Iteration: 30703; Percent complete: 93.0%; Average loss: 0.7371\n",
            "Iteration: 30704; Percent complete: 93.0%; Average loss: 0.6639\n",
            "Iteration: 30705; Percent complete: 93.0%; Average loss: 0.5262\n",
            "Iteration: 30706; Percent complete: 93.0%; Average loss: 0.5737\n",
            "Iteration: 30707; Percent complete: 93.1%; Average loss: 0.9097\n",
            "Iteration: 30708; Percent complete: 93.1%; Average loss: 0.7890\n",
            "Iteration: 30709; Percent complete: 93.1%; Average loss: 0.7080\n",
            "Iteration: 30710; Percent complete: 93.1%; Average loss: 0.5855\n",
            "Iteration: 30711; Percent complete: 93.1%; Average loss: 0.6816\n",
            "Iteration: 30712; Percent complete: 93.1%; Average loss: 0.6796\n",
            "Iteration: 30713; Percent complete: 93.1%; Average loss: 0.6153\n",
            "Iteration: 30714; Percent complete: 93.1%; Average loss: 0.7276\n",
            "Iteration: 30715; Percent complete: 93.1%; Average loss: 0.7988\n",
            "Iteration: 30716; Percent complete: 93.1%; Average loss: 0.7130\n",
            "Iteration: 30717; Percent complete: 93.1%; Average loss: 0.7678\n",
            "Iteration: 30718; Percent complete: 93.1%; Average loss: 0.6061\n",
            "Iteration: 30719; Percent complete: 93.1%; Average loss: 0.7298\n",
            "Iteration: 30720; Percent complete: 93.1%; Average loss: 0.6898\n",
            "Iteration: 30721; Percent complete: 93.1%; Average loss: 0.7809\n",
            "Iteration: 30722; Percent complete: 93.1%; Average loss: 0.7888\n",
            "Iteration: 30723; Percent complete: 93.1%; Average loss: 0.6278\n",
            "Iteration: 30724; Percent complete: 93.1%; Average loss: 0.7216\n",
            "Iteration: 30725; Percent complete: 93.1%; Average loss: 0.6496\n",
            "Iteration: 30726; Percent complete: 93.1%; Average loss: 0.7377\n",
            "Iteration: 30727; Percent complete: 93.1%; Average loss: 0.6673\n",
            "Iteration: 30728; Percent complete: 93.1%; Average loss: 0.8328\n",
            "Iteration: 30729; Percent complete: 93.1%; Average loss: 0.8436\n",
            "Iteration: 30730; Percent complete: 93.1%; Average loss: 0.7192\n",
            "Iteration: 30731; Percent complete: 93.1%; Average loss: 0.7023\n",
            "Iteration: 30732; Percent complete: 93.1%; Average loss: 0.7363\n",
            "Iteration: 30733; Percent complete: 93.1%; Average loss: 0.7084\n",
            "Iteration: 30734; Percent complete: 93.1%; Average loss: 0.7546\n",
            "Iteration: 30735; Percent complete: 93.1%; Average loss: 0.7364\n",
            "Iteration: 30736; Percent complete: 93.1%; Average loss: 0.5463\n",
            "Iteration: 30737; Percent complete: 93.1%; Average loss: 0.7006\n",
            "Iteration: 30738; Percent complete: 93.1%; Average loss: 0.5877\n",
            "Iteration: 30739; Percent complete: 93.1%; Average loss: 0.6321\n",
            "Iteration: 30740; Percent complete: 93.2%; Average loss: 0.6971\n",
            "Iteration: 30741; Percent complete: 93.2%; Average loss: 0.7171\n",
            "Iteration: 30742; Percent complete: 93.2%; Average loss: 0.7197\n",
            "Iteration: 30743; Percent complete: 93.2%; Average loss: 0.5525\n",
            "Iteration: 30744; Percent complete: 93.2%; Average loss: 0.7314\n",
            "Iteration: 30745; Percent complete: 93.2%; Average loss: 0.8186\n",
            "Iteration: 30746; Percent complete: 93.2%; Average loss: 0.6456\n",
            "Iteration: 30747; Percent complete: 93.2%; Average loss: 0.8427\n",
            "Iteration: 30748; Percent complete: 93.2%; Average loss: 0.7475\n",
            "Iteration: 30749; Percent complete: 93.2%; Average loss: 0.6429\n",
            "Iteration: 30750; Percent complete: 93.2%; Average loss: 0.7608\n",
            "Iteration: 30751; Percent complete: 93.2%; Average loss: 0.7328\n",
            "Iteration: 30752; Percent complete: 93.2%; Average loss: 0.7670\n",
            "Iteration: 30753; Percent complete: 93.2%; Average loss: 0.7783\n",
            "Iteration: 30754; Percent complete: 93.2%; Average loss: 0.6709\n",
            "Iteration: 30755; Percent complete: 93.2%; Average loss: 0.8824\n",
            "Iteration: 30756; Percent complete: 93.2%; Average loss: 0.7429\n",
            "Iteration: 30757; Percent complete: 93.2%; Average loss: 0.7231\n",
            "Iteration: 30758; Percent complete: 93.2%; Average loss: 0.8459\n",
            "Iteration: 30759; Percent complete: 93.2%; Average loss: 0.6214\n",
            "Iteration: 30760; Percent complete: 93.2%; Average loss: 0.7252\n",
            "Iteration: 30761; Percent complete: 93.2%; Average loss: 0.6699\n",
            "Iteration: 30762; Percent complete: 93.2%; Average loss: 0.7920\n",
            "Iteration: 30763; Percent complete: 93.2%; Average loss: 0.7082\n",
            "Iteration: 30764; Percent complete: 93.2%; Average loss: 0.8396\n",
            "Iteration: 30765; Percent complete: 93.2%; Average loss: 0.5539\n",
            "Iteration: 30766; Percent complete: 93.2%; Average loss: 0.8963\n",
            "Iteration: 30767; Percent complete: 93.2%; Average loss: 0.7873\n",
            "Iteration: 30768; Percent complete: 93.2%; Average loss: 0.6584\n",
            "Iteration: 30769; Percent complete: 93.2%; Average loss: 0.6854\n",
            "Iteration: 30770; Percent complete: 93.2%; Average loss: 0.7207\n",
            "Iteration: 30771; Percent complete: 93.2%; Average loss: 0.8790\n",
            "Iteration: 30772; Percent complete: 93.2%; Average loss: 0.6274\n",
            "Iteration: 30773; Percent complete: 93.3%; Average loss: 0.8905\n",
            "Iteration: 30774; Percent complete: 93.3%; Average loss: 0.7267\n",
            "Iteration: 30775; Percent complete: 93.3%; Average loss: 0.6304\n",
            "Iteration: 30776; Percent complete: 93.3%; Average loss: 0.7335\n",
            "Iteration: 30777; Percent complete: 93.3%; Average loss: 0.6116\n",
            "Iteration: 30778; Percent complete: 93.3%; Average loss: 0.6036\n",
            "Iteration: 30779; Percent complete: 93.3%; Average loss: 0.8070\n",
            "Iteration: 30780; Percent complete: 93.3%; Average loss: 0.7558\n",
            "Iteration: 30781; Percent complete: 93.3%; Average loss: 0.7093\n",
            "Iteration: 30782; Percent complete: 93.3%; Average loss: 0.5977\n",
            "Iteration: 30783; Percent complete: 93.3%; Average loss: 0.7313\n",
            "Iteration: 30784; Percent complete: 93.3%; Average loss: 0.8293\n",
            "Iteration: 30785; Percent complete: 93.3%; Average loss: 0.6218\n",
            "Iteration: 30786; Percent complete: 93.3%; Average loss: 0.6441\n",
            "Iteration: 30787; Percent complete: 93.3%; Average loss: 0.5904\n",
            "Iteration: 30788; Percent complete: 93.3%; Average loss: 0.8238\n",
            "Iteration: 30789; Percent complete: 93.3%; Average loss: 0.7015\n",
            "Iteration: 30790; Percent complete: 93.3%; Average loss: 0.6634\n",
            "Iteration: 30791; Percent complete: 93.3%; Average loss: 0.6852\n",
            "Iteration: 30792; Percent complete: 93.3%; Average loss: 0.6715\n",
            "Iteration: 30793; Percent complete: 93.3%; Average loss: 0.7485\n",
            "Iteration: 30794; Percent complete: 93.3%; Average loss: 0.6712\n",
            "Iteration: 30795; Percent complete: 93.3%; Average loss: 0.7134\n",
            "Iteration: 30796; Percent complete: 93.3%; Average loss: 0.6629\n",
            "Iteration: 30797; Percent complete: 93.3%; Average loss: 0.7515\n",
            "Iteration: 30798; Percent complete: 93.3%; Average loss: 0.7836\n",
            "Iteration: 30799; Percent complete: 93.3%; Average loss: 0.7370\n",
            "Iteration: 30800; Percent complete: 93.3%; Average loss: 0.7236\n",
            "Iteration: 30801; Percent complete: 93.3%; Average loss: 0.6781\n",
            "Iteration: 30802; Percent complete: 93.3%; Average loss: 0.7249\n",
            "Iteration: 30803; Percent complete: 93.3%; Average loss: 0.7236\n",
            "Iteration: 30804; Percent complete: 93.3%; Average loss: 0.6439\n",
            "Iteration: 30805; Percent complete: 93.3%; Average loss: 0.6999\n",
            "Iteration: 30806; Percent complete: 93.4%; Average loss: 0.7116\n",
            "Iteration: 30807; Percent complete: 93.4%; Average loss: 0.5702\n",
            "Iteration: 30808; Percent complete: 93.4%; Average loss: 0.7148\n",
            "Iteration: 30809; Percent complete: 93.4%; Average loss: 0.7659\n",
            "Iteration: 30810; Percent complete: 93.4%; Average loss: 0.6885\n",
            "Iteration: 30811; Percent complete: 93.4%; Average loss: 0.7062\n",
            "Iteration: 30812; Percent complete: 93.4%; Average loss: 0.6580\n",
            "Iteration: 30813; Percent complete: 93.4%; Average loss: 0.6884\n",
            "Iteration: 30814; Percent complete: 93.4%; Average loss: 0.6579\n",
            "Iteration: 30815; Percent complete: 93.4%; Average loss: 0.7254\n",
            "Iteration: 30816; Percent complete: 93.4%; Average loss: 0.7601\n",
            "Iteration: 30817; Percent complete: 93.4%; Average loss: 0.6081\n",
            "Iteration: 30818; Percent complete: 93.4%; Average loss: 0.7183\n",
            "Iteration: 30819; Percent complete: 93.4%; Average loss: 0.6286\n",
            "Iteration: 30820; Percent complete: 93.4%; Average loss: 0.7969\n",
            "Iteration: 30821; Percent complete: 93.4%; Average loss: 0.6304\n",
            "Iteration: 30822; Percent complete: 93.4%; Average loss: 0.8424\n",
            "Iteration: 30823; Percent complete: 93.4%; Average loss: 0.6096\n",
            "Iteration: 30824; Percent complete: 93.4%; Average loss: 0.6419\n",
            "Iteration: 30825; Percent complete: 93.4%; Average loss: 0.7011\n",
            "Iteration: 30826; Percent complete: 93.4%; Average loss: 0.7573\n",
            "Iteration: 30827; Percent complete: 93.4%; Average loss: 0.6752\n",
            "Iteration: 30828; Percent complete: 93.4%; Average loss: 0.7032\n",
            "Iteration: 30829; Percent complete: 93.4%; Average loss: 0.7237\n",
            "Iteration: 30830; Percent complete: 93.4%; Average loss: 0.7502\n",
            "Iteration: 30831; Percent complete: 93.4%; Average loss: 0.6289\n",
            "Iteration: 30832; Percent complete: 93.4%; Average loss: 0.8189\n",
            "Iteration: 30833; Percent complete: 93.4%; Average loss: 0.8678\n",
            "Iteration: 30834; Percent complete: 93.4%; Average loss: 0.6734\n",
            "Iteration: 30835; Percent complete: 93.4%; Average loss: 0.8791\n",
            "Iteration: 30836; Percent complete: 93.4%; Average loss: 0.8834\n",
            "Iteration: 30837; Percent complete: 93.4%; Average loss: 0.5953\n",
            "Iteration: 30838; Percent complete: 93.4%; Average loss: 0.7127\n",
            "Iteration: 30839; Percent complete: 93.5%; Average loss: 0.8402\n",
            "Iteration: 30840; Percent complete: 93.5%; Average loss: 0.6918\n",
            "Iteration: 30841; Percent complete: 93.5%; Average loss: 0.6337\n",
            "Iteration: 30842; Percent complete: 93.5%; Average loss: 0.6142\n",
            "Iteration: 30843; Percent complete: 93.5%; Average loss: 0.7310\n",
            "Iteration: 30844; Percent complete: 93.5%; Average loss: 0.5866\n",
            "Iteration: 30845; Percent complete: 93.5%; Average loss: 0.7046\n",
            "Iteration: 30846; Percent complete: 93.5%; Average loss: 0.7178\n",
            "Iteration: 30847; Percent complete: 93.5%; Average loss: 0.7785\n",
            "Iteration: 30848; Percent complete: 93.5%; Average loss: 0.7367\n",
            "Iteration: 30849; Percent complete: 93.5%; Average loss: 0.6776\n",
            "Iteration: 30850; Percent complete: 93.5%; Average loss: 0.6057\n",
            "Iteration: 30851; Percent complete: 93.5%; Average loss: 0.6578\n",
            "Iteration: 30852; Percent complete: 93.5%; Average loss: 0.7872\n",
            "Iteration: 30853; Percent complete: 93.5%; Average loss: 0.9417\n",
            "Iteration: 30854; Percent complete: 93.5%; Average loss: 0.6412\n",
            "Iteration: 30855; Percent complete: 93.5%; Average loss: 0.6014\n",
            "Iteration: 30856; Percent complete: 93.5%; Average loss: 0.7421\n",
            "Iteration: 30857; Percent complete: 93.5%; Average loss: 0.6764\n",
            "Iteration: 30858; Percent complete: 93.5%; Average loss: 0.6065\n",
            "Iteration: 30859; Percent complete: 93.5%; Average loss: 0.6809\n",
            "Iteration: 30860; Percent complete: 93.5%; Average loss: 0.7943\n",
            "Iteration: 30861; Percent complete: 93.5%; Average loss: 0.5888\n",
            "Iteration: 30862; Percent complete: 93.5%; Average loss: 0.6156\n",
            "Iteration: 30863; Percent complete: 93.5%; Average loss: 0.6043\n",
            "Iteration: 30864; Percent complete: 93.5%; Average loss: 0.5570\n",
            "Iteration: 30865; Percent complete: 93.5%; Average loss: 0.7369\n",
            "Iteration: 30866; Percent complete: 93.5%; Average loss: 0.6542\n",
            "Iteration: 30867; Percent complete: 93.5%; Average loss: 0.8078\n",
            "Iteration: 30868; Percent complete: 93.5%; Average loss: 0.7932\n",
            "Iteration: 30869; Percent complete: 93.5%; Average loss: 0.7084\n",
            "Iteration: 30870; Percent complete: 93.5%; Average loss: 0.6796\n",
            "Iteration: 30871; Percent complete: 93.5%; Average loss: 0.6226\n",
            "Iteration: 30872; Percent complete: 93.6%; Average loss: 0.7183\n",
            "Iteration: 30873; Percent complete: 93.6%; Average loss: 0.6541\n",
            "Iteration: 30874; Percent complete: 93.6%; Average loss: 0.8187\n",
            "Iteration: 30875; Percent complete: 93.6%; Average loss: 0.7475\n",
            "Iteration: 30876; Percent complete: 93.6%; Average loss: 0.7975\n",
            "Iteration: 30877; Percent complete: 93.6%; Average loss: 0.6296\n",
            "Iteration: 30878; Percent complete: 93.6%; Average loss: 0.7558\n",
            "Iteration: 30879; Percent complete: 93.6%; Average loss: 0.6062\n",
            "Iteration: 30880; Percent complete: 93.6%; Average loss: 0.9342\n",
            "Iteration: 30881; Percent complete: 93.6%; Average loss: 0.8410\n",
            "Iteration: 30882; Percent complete: 93.6%; Average loss: 0.6253\n",
            "Iteration: 30883; Percent complete: 93.6%; Average loss: 0.7297\n",
            "Iteration: 30884; Percent complete: 93.6%; Average loss: 0.6227\n",
            "Iteration: 30885; Percent complete: 93.6%; Average loss: 0.6004\n",
            "Iteration: 30886; Percent complete: 93.6%; Average loss: 0.6875\n",
            "Iteration: 30887; Percent complete: 93.6%; Average loss: 0.7342\n",
            "Iteration: 30888; Percent complete: 93.6%; Average loss: 0.7324\n",
            "Iteration: 30889; Percent complete: 93.6%; Average loss: 0.8172\n",
            "Iteration: 30890; Percent complete: 93.6%; Average loss: 0.6952\n",
            "Iteration: 30891; Percent complete: 93.6%; Average loss: 0.8831\n",
            "Iteration: 30892; Percent complete: 93.6%; Average loss: 0.9145\n",
            "Iteration: 30893; Percent complete: 93.6%; Average loss: 0.7399\n",
            "Iteration: 30894; Percent complete: 93.6%; Average loss: 0.6323\n",
            "Iteration: 30895; Percent complete: 93.6%; Average loss: 0.7220\n",
            "Iteration: 30896; Percent complete: 93.6%; Average loss: 0.6086\n",
            "Iteration: 30897; Percent complete: 93.6%; Average loss: 0.7519\n",
            "Iteration: 30898; Percent complete: 93.6%; Average loss: 0.6906\n",
            "Iteration: 30899; Percent complete: 93.6%; Average loss: 0.7002\n",
            "Iteration: 30900; Percent complete: 93.6%; Average loss: 0.6008\n",
            "Iteration: 30901; Percent complete: 93.6%; Average loss: 0.8418\n",
            "Iteration: 30902; Percent complete: 93.6%; Average loss: 0.6511\n",
            "Iteration: 30903; Percent complete: 93.6%; Average loss: 0.6418\n",
            "Iteration: 30904; Percent complete: 93.6%; Average loss: 0.6716\n",
            "Iteration: 30905; Percent complete: 93.7%; Average loss: 0.6481\n",
            "Iteration: 30906; Percent complete: 93.7%; Average loss: 0.8246\n",
            "Iteration: 30907; Percent complete: 93.7%; Average loss: 0.7056\n",
            "Iteration: 30908; Percent complete: 93.7%; Average loss: 0.5052\n",
            "Iteration: 30909; Percent complete: 93.7%; Average loss: 0.6461\n",
            "Iteration: 30910; Percent complete: 93.7%; Average loss: 0.6635\n",
            "Iteration: 30911; Percent complete: 93.7%; Average loss: 0.8499\n",
            "Iteration: 30912; Percent complete: 93.7%; Average loss: 0.7398\n",
            "Iteration: 30913; Percent complete: 93.7%; Average loss: 0.6883\n",
            "Iteration: 30914; Percent complete: 93.7%; Average loss: 0.8060\n",
            "Iteration: 30915; Percent complete: 93.7%; Average loss: 0.7810\n",
            "Iteration: 30916; Percent complete: 93.7%; Average loss: 0.6192\n",
            "Iteration: 30917; Percent complete: 93.7%; Average loss: 0.7244\n",
            "Iteration: 30918; Percent complete: 93.7%; Average loss: 0.7829\n",
            "Iteration: 30919; Percent complete: 93.7%; Average loss: 0.6886\n",
            "Iteration: 30920; Percent complete: 93.7%; Average loss: 0.7657\n",
            "Iteration: 30921; Percent complete: 93.7%; Average loss: 0.7702\n",
            "Iteration: 30922; Percent complete: 93.7%; Average loss: 0.7077\n",
            "Iteration: 30923; Percent complete: 93.7%; Average loss: 0.7314\n",
            "Iteration: 30924; Percent complete: 93.7%; Average loss: 0.5925\n",
            "Iteration: 30925; Percent complete: 93.7%; Average loss: 0.6664\n",
            "Iteration: 30926; Percent complete: 93.7%; Average loss: 0.6179\n",
            "Iteration: 30927; Percent complete: 93.7%; Average loss: 0.5597\n",
            "Iteration: 30928; Percent complete: 93.7%; Average loss: 0.6099\n",
            "Iteration: 30929; Percent complete: 93.7%; Average loss: 0.7211\n",
            "Iteration: 30930; Percent complete: 93.7%; Average loss: 0.7627\n",
            "Iteration: 30931; Percent complete: 93.7%; Average loss: 0.7459\n",
            "Iteration: 30932; Percent complete: 93.7%; Average loss: 0.7359\n",
            "Iteration: 30933; Percent complete: 93.7%; Average loss: 0.7109\n",
            "Iteration: 30934; Percent complete: 93.7%; Average loss: 0.8118\n",
            "Iteration: 30935; Percent complete: 93.7%; Average loss: 0.8596\n",
            "Iteration: 30936; Percent complete: 93.7%; Average loss: 0.7553\n",
            "Iteration: 30937; Percent complete: 93.7%; Average loss: 0.6826\n",
            "Iteration: 30938; Percent complete: 93.8%; Average loss: 0.8176\n",
            "Iteration: 30939; Percent complete: 93.8%; Average loss: 0.6512\n",
            "Iteration: 30940; Percent complete: 93.8%; Average loss: 0.6659\n",
            "Iteration: 30941; Percent complete: 93.8%; Average loss: 0.6893\n",
            "Iteration: 30942; Percent complete: 93.8%; Average loss: 0.7277\n",
            "Iteration: 30943; Percent complete: 93.8%; Average loss: 0.7075\n",
            "Iteration: 30944; Percent complete: 93.8%; Average loss: 0.6282\n",
            "Iteration: 30945; Percent complete: 93.8%; Average loss: 0.9044\n",
            "Iteration: 30946; Percent complete: 93.8%; Average loss: 0.6918\n",
            "Iteration: 30947; Percent complete: 93.8%; Average loss: 0.7554\n",
            "Iteration: 30948; Percent complete: 93.8%; Average loss: 0.6582\n",
            "Iteration: 30949; Percent complete: 93.8%; Average loss: 0.7669\n",
            "Iteration: 30950; Percent complete: 93.8%; Average loss: 0.7109\n",
            "Iteration: 30951; Percent complete: 93.8%; Average loss: 0.7233\n",
            "Iteration: 30952; Percent complete: 93.8%; Average loss: 0.6237\n",
            "Iteration: 30953; Percent complete: 93.8%; Average loss: 0.6717\n",
            "Iteration: 30954; Percent complete: 93.8%; Average loss: 0.7062\n",
            "Iteration: 30955; Percent complete: 93.8%; Average loss: 0.7677\n",
            "Iteration: 30956; Percent complete: 93.8%; Average loss: 0.7692\n",
            "Iteration: 30957; Percent complete: 93.8%; Average loss: 0.6769\n",
            "Iteration: 30958; Percent complete: 93.8%; Average loss: 0.6773\n",
            "Iteration: 30959; Percent complete: 93.8%; Average loss: 0.7674\n",
            "Iteration: 30960; Percent complete: 93.8%; Average loss: 0.6890\n",
            "Iteration: 30961; Percent complete: 93.8%; Average loss: 0.5930\n",
            "Iteration: 30962; Percent complete: 93.8%; Average loss: 0.7840\n",
            "Iteration: 30963; Percent complete: 93.8%; Average loss: 0.7729\n",
            "Iteration: 30964; Percent complete: 93.8%; Average loss: 0.7552\n",
            "Iteration: 30965; Percent complete: 93.8%; Average loss: 0.7544\n",
            "Iteration: 30966; Percent complete: 93.8%; Average loss: 0.7761\n",
            "Iteration: 30967; Percent complete: 93.8%; Average loss: 0.6806\n",
            "Iteration: 30968; Percent complete: 93.8%; Average loss: 0.8348\n",
            "Iteration: 30969; Percent complete: 93.8%; Average loss: 0.7764\n",
            "Iteration: 30970; Percent complete: 93.8%; Average loss: 0.7116\n",
            "Iteration: 30971; Percent complete: 93.9%; Average loss: 0.8948\n",
            "Iteration: 30972; Percent complete: 93.9%; Average loss: 0.6198\n",
            "Iteration: 30973; Percent complete: 93.9%; Average loss: 0.6588\n",
            "Iteration: 30974; Percent complete: 93.9%; Average loss: 0.9202\n",
            "Iteration: 30975; Percent complete: 93.9%; Average loss: 0.5863\n",
            "Iteration: 30976; Percent complete: 93.9%; Average loss: 0.7079\n",
            "Iteration: 30977; Percent complete: 93.9%; Average loss: 0.7377\n",
            "Iteration: 30978; Percent complete: 93.9%; Average loss: 0.8569\n",
            "Iteration: 30979; Percent complete: 93.9%; Average loss: 0.5969\n",
            "Iteration: 30980; Percent complete: 93.9%; Average loss: 0.6959\n",
            "Iteration: 30981; Percent complete: 93.9%; Average loss: 0.8755\n",
            "Iteration: 30982; Percent complete: 93.9%; Average loss: 0.7828\n",
            "Iteration: 30983; Percent complete: 93.9%; Average loss: 0.6928\n",
            "Iteration: 30984; Percent complete: 93.9%; Average loss: 0.8057\n",
            "Iteration: 30985; Percent complete: 93.9%; Average loss: 0.7398\n",
            "Iteration: 30986; Percent complete: 93.9%; Average loss: 0.6606\n",
            "Iteration: 30987; Percent complete: 93.9%; Average loss: 0.6703\n",
            "Iteration: 30988; Percent complete: 93.9%; Average loss: 0.6817\n",
            "Iteration: 30989; Percent complete: 93.9%; Average loss: 0.6087\n",
            "Iteration: 30990; Percent complete: 93.9%; Average loss: 0.7049\n",
            "Iteration: 30991; Percent complete: 93.9%; Average loss: 0.8782\n",
            "Iteration: 30992; Percent complete: 93.9%; Average loss: 0.8157\n",
            "Iteration: 30993; Percent complete: 93.9%; Average loss: 0.7127\n",
            "Iteration: 30994; Percent complete: 93.9%; Average loss: 0.6145\n",
            "Iteration: 30995; Percent complete: 93.9%; Average loss: 0.6855\n",
            "Iteration: 30996; Percent complete: 93.9%; Average loss: 0.8132\n",
            "Iteration: 30997; Percent complete: 93.9%; Average loss: 0.7308\n",
            "Iteration: 30998; Percent complete: 93.9%; Average loss: 0.6644\n",
            "Iteration: 30999; Percent complete: 93.9%; Average loss: 0.8557\n",
            "Iteration: 31000; Percent complete: 93.9%; Average loss: 0.7850\n",
            "Iteration: 31001; Percent complete: 93.9%; Average loss: 0.7726\n",
            "Iteration: 31002; Percent complete: 93.9%; Average loss: 0.7719\n",
            "Iteration: 31003; Percent complete: 93.9%; Average loss: 0.6933\n",
            "Iteration: 31004; Percent complete: 94.0%; Average loss: 0.7408\n",
            "Iteration: 31005; Percent complete: 94.0%; Average loss: 0.6300\n",
            "Iteration: 31006; Percent complete: 94.0%; Average loss: 0.6911\n",
            "Iteration: 31007; Percent complete: 94.0%; Average loss: 0.7335\n",
            "Iteration: 31008; Percent complete: 94.0%; Average loss: 0.8851\n",
            "Iteration: 31009; Percent complete: 94.0%; Average loss: 0.6583\n",
            "Iteration: 31010; Percent complete: 94.0%; Average loss: 0.6262\n",
            "Iteration: 31011; Percent complete: 94.0%; Average loss: 0.6536\n",
            "Iteration: 31012; Percent complete: 94.0%; Average loss: 0.6616\n",
            "Iteration: 31013; Percent complete: 94.0%; Average loss: 0.5622\n",
            "Iteration: 31014; Percent complete: 94.0%; Average loss: 0.8667\n",
            "Iteration: 31015; Percent complete: 94.0%; Average loss: 0.5497\n",
            "Iteration: 31016; Percent complete: 94.0%; Average loss: 0.5829\n",
            "Iteration: 31017; Percent complete: 94.0%; Average loss: 0.6980\n",
            "Iteration: 31018; Percent complete: 94.0%; Average loss: 0.7381\n",
            "Iteration: 31019; Percent complete: 94.0%; Average loss: 0.7685\n",
            "Iteration: 31020; Percent complete: 94.0%; Average loss: 0.8367\n",
            "Iteration: 31021; Percent complete: 94.0%; Average loss: 0.6424\n",
            "Iteration: 31022; Percent complete: 94.0%; Average loss: 0.6196\n",
            "Iteration: 31023; Percent complete: 94.0%; Average loss: 0.7481\n",
            "Iteration: 31024; Percent complete: 94.0%; Average loss: 0.7449\n",
            "Iteration: 31025; Percent complete: 94.0%; Average loss: 0.6607\n",
            "Iteration: 31026; Percent complete: 94.0%; Average loss: 0.6446\n",
            "Iteration: 31027; Percent complete: 94.0%; Average loss: 0.6784\n",
            "Iteration: 31028; Percent complete: 94.0%; Average loss: 0.7647\n",
            "Iteration: 31029; Percent complete: 94.0%; Average loss: 0.7189\n",
            "Iteration: 31030; Percent complete: 94.0%; Average loss: 0.5840\n",
            "Iteration: 31031; Percent complete: 94.0%; Average loss: 0.7660\n",
            "Iteration: 31032; Percent complete: 94.0%; Average loss: 0.7322\n",
            "Iteration: 31033; Percent complete: 94.0%; Average loss: 0.6116\n",
            "Iteration: 31034; Percent complete: 94.0%; Average loss: 0.7021\n",
            "Iteration: 31035; Percent complete: 94.0%; Average loss: 0.6316\n",
            "Iteration: 31036; Percent complete: 94.0%; Average loss: 0.8300\n",
            "Iteration: 31037; Percent complete: 94.1%; Average loss: 0.6358\n",
            "Iteration: 31038; Percent complete: 94.1%; Average loss: 0.6020\n",
            "Iteration: 31039; Percent complete: 94.1%; Average loss: 0.5837\n",
            "Iteration: 31040; Percent complete: 94.1%; Average loss: 0.4952\n",
            "Iteration: 31041; Percent complete: 94.1%; Average loss: 0.6716\n",
            "Iteration: 31042; Percent complete: 94.1%; Average loss: 0.8193\n",
            "Iteration: 31043; Percent complete: 94.1%; Average loss: 0.8029\n",
            "Iteration: 31044; Percent complete: 94.1%; Average loss: 0.6954\n",
            "Iteration: 31045; Percent complete: 94.1%; Average loss: 0.7061\n",
            "Iteration: 31046; Percent complete: 94.1%; Average loss: 0.6059\n",
            "Iteration: 31047; Percent complete: 94.1%; Average loss: 0.6860\n",
            "Iteration: 31048; Percent complete: 94.1%; Average loss: 0.7067\n",
            "Iteration: 31049; Percent complete: 94.1%; Average loss: 0.5811\n",
            "Iteration: 31050; Percent complete: 94.1%; Average loss: 0.6774\n",
            "Iteration: 31051; Percent complete: 94.1%; Average loss: 0.7308\n",
            "Iteration: 31052; Percent complete: 94.1%; Average loss: 0.8668\n",
            "Iteration: 31053; Percent complete: 94.1%; Average loss: 0.8590\n",
            "Iteration: 31054; Percent complete: 94.1%; Average loss: 0.6305\n",
            "Iteration: 31055; Percent complete: 94.1%; Average loss: 0.8035\n",
            "Iteration: 31056; Percent complete: 94.1%; Average loss: 0.6590\n",
            "Iteration: 31057; Percent complete: 94.1%; Average loss: 0.6632\n",
            "Iteration: 31058; Percent complete: 94.1%; Average loss: 0.8397\n",
            "Iteration: 31059; Percent complete: 94.1%; Average loss: 0.8369\n",
            "Iteration: 31060; Percent complete: 94.1%; Average loss: 0.6190\n",
            "Iteration: 31061; Percent complete: 94.1%; Average loss: 0.9303\n",
            "Iteration: 31062; Percent complete: 94.1%; Average loss: 0.7087\n",
            "Iteration: 31063; Percent complete: 94.1%; Average loss: 0.7303\n",
            "Iteration: 31064; Percent complete: 94.1%; Average loss: 0.6458\n",
            "Iteration: 31065; Percent complete: 94.1%; Average loss: 0.5684\n",
            "Iteration: 31066; Percent complete: 94.1%; Average loss: 0.7015\n",
            "Iteration: 31067; Percent complete: 94.1%; Average loss: 0.6630\n",
            "Iteration: 31068; Percent complete: 94.1%; Average loss: 0.6123\n",
            "Iteration: 31069; Percent complete: 94.1%; Average loss: 0.7648\n",
            "Iteration: 31070; Percent complete: 94.2%; Average loss: 0.8241\n",
            "Iteration: 31071; Percent complete: 94.2%; Average loss: 0.5720\n",
            "Iteration: 31072; Percent complete: 94.2%; Average loss: 0.7499\n",
            "Iteration: 31073; Percent complete: 94.2%; Average loss: 0.7160\n",
            "Iteration: 31074; Percent complete: 94.2%; Average loss: 0.6125\n",
            "Iteration: 31075; Percent complete: 94.2%; Average loss: 0.7564\n",
            "Iteration: 31076; Percent complete: 94.2%; Average loss: 0.7275\n",
            "Iteration: 31077; Percent complete: 94.2%; Average loss: 0.8277\n",
            "Iteration: 31078; Percent complete: 94.2%; Average loss: 0.6298\n",
            "Iteration: 31079; Percent complete: 94.2%; Average loss: 0.6051\n",
            "Iteration: 31080; Percent complete: 94.2%; Average loss: 0.6736\n",
            "Iteration: 31081; Percent complete: 94.2%; Average loss: 0.5928\n",
            "Iteration: 31082; Percent complete: 94.2%; Average loss: 0.7291\n",
            "Iteration: 31083; Percent complete: 94.2%; Average loss: 0.8128\n",
            "Iteration: 31084; Percent complete: 94.2%; Average loss: 0.7048\n",
            "Iteration: 31085; Percent complete: 94.2%; Average loss: 0.7140\n",
            "Iteration: 31086; Percent complete: 94.2%; Average loss: 0.7996\n",
            "Iteration: 31087; Percent complete: 94.2%; Average loss: 0.6680\n",
            "Iteration: 31088; Percent complete: 94.2%; Average loss: 0.6042\n",
            "Iteration: 31089; Percent complete: 94.2%; Average loss: 0.7828\n",
            "Iteration: 31090; Percent complete: 94.2%; Average loss: 0.6728\n",
            "Iteration: 31091; Percent complete: 94.2%; Average loss: 0.6745\n",
            "Iteration: 31092; Percent complete: 94.2%; Average loss: 0.7284\n",
            "Iteration: 31093; Percent complete: 94.2%; Average loss: 0.6437\n",
            "Iteration: 31094; Percent complete: 94.2%; Average loss: 0.7480\n",
            "Iteration: 31095; Percent complete: 94.2%; Average loss: 0.7033\n",
            "Iteration: 31096; Percent complete: 94.2%; Average loss: 0.7075\n",
            "Iteration: 31097; Percent complete: 94.2%; Average loss: 0.8167\n",
            "Iteration: 31098; Percent complete: 94.2%; Average loss: 0.6565\n",
            "Iteration: 31099; Percent complete: 94.2%; Average loss: 0.7079\n",
            "Iteration: 31100; Percent complete: 94.2%; Average loss: 0.6568\n",
            "Iteration: 31101; Percent complete: 94.2%; Average loss: 0.6463\n",
            "Iteration: 31102; Percent complete: 94.2%; Average loss: 0.6398\n",
            "Iteration: 31103; Percent complete: 94.3%; Average loss: 0.7497\n",
            "Iteration: 31104; Percent complete: 94.3%; Average loss: 0.7152\n",
            "Iteration: 31105; Percent complete: 94.3%; Average loss: 0.5832\n",
            "Iteration: 31106; Percent complete: 94.3%; Average loss: 0.8130\n",
            "Iteration: 31107; Percent complete: 94.3%; Average loss: 0.5406\n",
            "Iteration: 31108; Percent complete: 94.3%; Average loss: 0.7309\n",
            "Iteration: 31109; Percent complete: 94.3%; Average loss: 0.7389\n",
            "Iteration: 31110; Percent complete: 94.3%; Average loss: 0.6886\n",
            "Iteration: 31111; Percent complete: 94.3%; Average loss: 0.6623\n",
            "Iteration: 31112; Percent complete: 94.3%; Average loss: 0.6510\n",
            "Iteration: 31113; Percent complete: 94.3%; Average loss: 0.7071\n",
            "Iteration: 31114; Percent complete: 94.3%; Average loss: 0.6060\n",
            "Iteration: 31115; Percent complete: 94.3%; Average loss: 0.6291\n",
            "Iteration: 31116; Percent complete: 94.3%; Average loss: 0.6766\n",
            "Iteration: 31117; Percent complete: 94.3%; Average loss: 0.6186\n",
            "Iteration: 31118; Percent complete: 94.3%; Average loss: 0.8446\n",
            "Iteration: 31119; Percent complete: 94.3%; Average loss: 0.6602\n",
            "Iteration: 31120; Percent complete: 94.3%; Average loss: 0.7388\n",
            "Iteration: 31121; Percent complete: 94.3%; Average loss: 0.6824\n",
            "Iteration: 31122; Percent complete: 94.3%; Average loss: 0.6455\n",
            "Iteration: 31123; Percent complete: 94.3%; Average loss: 0.6487\n",
            "Iteration: 31124; Percent complete: 94.3%; Average loss: 0.5350\n",
            "Iteration: 31125; Percent complete: 94.3%; Average loss: 0.7454\n",
            "Iteration: 31126; Percent complete: 94.3%; Average loss: 0.8179\n",
            "Iteration: 31127; Percent complete: 94.3%; Average loss: 0.7847\n",
            "Iteration: 31128; Percent complete: 94.3%; Average loss: 0.7334\n",
            "Iteration: 31129; Percent complete: 94.3%; Average loss: 0.6176\n",
            "Iteration: 31130; Percent complete: 94.3%; Average loss: 0.7709\n",
            "Iteration: 31131; Percent complete: 94.3%; Average loss: 0.5073\n",
            "Iteration: 31132; Percent complete: 94.3%; Average loss: 0.8096\n",
            "Iteration: 31133; Percent complete: 94.3%; Average loss: 0.6024\n",
            "Iteration: 31134; Percent complete: 94.3%; Average loss: 0.8281\n",
            "Iteration: 31135; Percent complete: 94.3%; Average loss: 0.8415\n",
            "Iteration: 31136; Percent complete: 94.4%; Average loss: 0.7011\n",
            "Iteration: 31137; Percent complete: 94.4%; Average loss: 0.5906\n",
            "Iteration: 31138; Percent complete: 94.4%; Average loss: 0.7906\n",
            "Iteration: 31139; Percent complete: 94.4%; Average loss: 0.6725\n",
            "Iteration: 31140; Percent complete: 94.4%; Average loss: 0.7775\n",
            "Iteration: 31141; Percent complete: 94.4%; Average loss: 0.7562\n",
            "Iteration: 31142; Percent complete: 94.4%; Average loss: 0.8394\n",
            "Iteration: 31143; Percent complete: 94.4%; Average loss: 0.6729\n",
            "Iteration: 31144; Percent complete: 94.4%; Average loss: 0.7283\n",
            "Iteration: 31145; Percent complete: 94.4%; Average loss: 0.6822\n",
            "Iteration: 31146; Percent complete: 94.4%; Average loss: 0.5582\n",
            "Iteration: 31147; Percent complete: 94.4%; Average loss: 0.7042\n",
            "Iteration: 31148; Percent complete: 94.4%; Average loss: 0.5986\n",
            "Iteration: 31149; Percent complete: 94.4%; Average loss: 0.6883\n",
            "Iteration: 31150; Percent complete: 94.4%; Average loss: 0.6791\n",
            "Iteration: 31151; Percent complete: 94.4%; Average loss: 0.6325\n",
            "Iteration: 31152; Percent complete: 94.4%; Average loss: 0.7008\n",
            "Iteration: 31153; Percent complete: 94.4%; Average loss: 0.6854\n",
            "Iteration: 31154; Percent complete: 94.4%; Average loss: 0.6937\n",
            "Iteration: 31155; Percent complete: 94.4%; Average loss: 0.6042\n",
            "Iteration: 31156; Percent complete: 94.4%; Average loss: 0.7488\n",
            "Iteration: 31157; Percent complete: 94.4%; Average loss: 0.7650\n",
            "Iteration: 31158; Percent complete: 94.4%; Average loss: 0.7271\n",
            "Iteration: 31159; Percent complete: 94.4%; Average loss: 0.6889\n",
            "Iteration: 31160; Percent complete: 94.4%; Average loss: 0.6666\n",
            "Iteration: 31161; Percent complete: 94.4%; Average loss: 0.6829\n",
            "Iteration: 31162; Percent complete: 94.4%; Average loss: 0.8338\n",
            "Iteration: 31163; Percent complete: 94.4%; Average loss: 0.7373\n",
            "Iteration: 31164; Percent complete: 94.4%; Average loss: 0.6280\n",
            "Iteration: 31165; Percent complete: 94.4%; Average loss: 0.6178\n",
            "Iteration: 31166; Percent complete: 94.4%; Average loss: 0.7366\n",
            "Iteration: 31167; Percent complete: 94.4%; Average loss: 0.5994\n",
            "Iteration: 31168; Percent complete: 94.4%; Average loss: 0.8219\n",
            "Iteration: 31169; Percent complete: 94.5%; Average loss: 0.7417\n",
            "Iteration: 31170; Percent complete: 94.5%; Average loss: 0.6192\n",
            "Iteration: 31171; Percent complete: 94.5%; Average loss: 0.8871\n",
            "Iteration: 31172; Percent complete: 94.5%; Average loss: 0.7442\n",
            "Iteration: 31173; Percent complete: 94.5%; Average loss: 0.6834\n",
            "Iteration: 31174; Percent complete: 94.5%; Average loss: 0.6960\n",
            "Iteration: 31175; Percent complete: 94.5%; Average loss: 0.6760\n",
            "Iteration: 31176; Percent complete: 94.5%; Average loss: 0.6140\n",
            "Iteration: 31177; Percent complete: 94.5%; Average loss: 0.7133\n",
            "Iteration: 31178; Percent complete: 94.5%; Average loss: 0.6412\n",
            "Iteration: 31179; Percent complete: 94.5%; Average loss: 0.6849\n",
            "Iteration: 31180; Percent complete: 94.5%; Average loss: 0.7211\n",
            "Iteration: 31181; Percent complete: 94.5%; Average loss: 0.5677\n",
            "Iteration: 31182; Percent complete: 94.5%; Average loss: 0.6661\n",
            "Iteration: 31183; Percent complete: 94.5%; Average loss: 0.5445\n",
            "Iteration: 31184; Percent complete: 94.5%; Average loss: 0.6139\n",
            "Iteration: 31185; Percent complete: 94.5%; Average loss: 0.8021\n",
            "Iteration: 31186; Percent complete: 94.5%; Average loss: 0.7300\n",
            "Iteration: 31187; Percent complete: 94.5%; Average loss: 0.6663\n",
            "Iteration: 31188; Percent complete: 94.5%; Average loss: 0.6821\n",
            "Iteration: 31189; Percent complete: 94.5%; Average loss: 0.6613\n",
            "Iteration: 31190; Percent complete: 94.5%; Average loss: 0.6981\n",
            "Iteration: 31191; Percent complete: 94.5%; Average loss: 0.7289\n",
            "Iteration: 31192; Percent complete: 94.5%; Average loss: 0.5709\n",
            "Iteration: 31193; Percent complete: 94.5%; Average loss: 0.9637\n",
            "Iteration: 31194; Percent complete: 94.5%; Average loss: 0.6048\n",
            "Iteration: 31195; Percent complete: 94.5%; Average loss: 0.7484\n",
            "Iteration: 31196; Percent complete: 94.5%; Average loss: 0.4887\n",
            "Iteration: 31197; Percent complete: 94.5%; Average loss: 0.6272\n",
            "Iteration: 31198; Percent complete: 94.5%; Average loss: 0.7392\n",
            "Iteration: 31199; Percent complete: 94.5%; Average loss: 1.0900\n",
            "Iteration: 31200; Percent complete: 94.5%; Average loss: 0.6660\n",
            "Iteration: 31201; Percent complete: 94.5%; Average loss: 0.8130\n",
            "Iteration: 31202; Percent complete: 94.6%; Average loss: 0.6925\n",
            "Iteration: 31203; Percent complete: 94.6%; Average loss: 0.7815\n",
            "Iteration: 31204; Percent complete: 94.6%; Average loss: 0.5144\n",
            "Iteration: 31205; Percent complete: 94.6%; Average loss: 0.5836\n",
            "Iteration: 31206; Percent complete: 94.6%; Average loss: 0.8194\n",
            "Iteration: 31207; Percent complete: 94.6%; Average loss: 0.7404\n",
            "Iteration: 31208; Percent complete: 94.6%; Average loss: 0.7246\n",
            "Iteration: 31209; Percent complete: 94.6%; Average loss: 0.6697\n",
            "Iteration: 31210; Percent complete: 94.6%; Average loss: 0.5719\n",
            "Iteration: 31211; Percent complete: 94.6%; Average loss: 0.5995\n",
            "Iteration: 31212; Percent complete: 94.6%; Average loss: 0.7152\n",
            "Iteration: 31213; Percent complete: 94.6%; Average loss: 0.7383\n",
            "Iteration: 31214; Percent complete: 94.6%; Average loss: 0.8387\n",
            "Iteration: 31215; Percent complete: 94.6%; Average loss: 0.8253\n",
            "Iteration: 31216; Percent complete: 94.6%; Average loss: 0.5737\n",
            "Iteration: 31217; Percent complete: 94.6%; Average loss: 0.6805\n",
            "Iteration: 31218; Percent complete: 94.6%; Average loss: 0.7021\n",
            "Iteration: 31219; Percent complete: 94.6%; Average loss: 0.8287\n",
            "Iteration: 31220; Percent complete: 94.6%; Average loss: 0.5784\n",
            "Iteration: 31221; Percent complete: 94.6%; Average loss: 0.7489\n",
            "Iteration: 31222; Percent complete: 94.6%; Average loss: 0.6616\n",
            "Iteration: 31223; Percent complete: 94.6%; Average loss: 0.6747\n",
            "Iteration: 31224; Percent complete: 94.6%; Average loss: 0.8082\n",
            "Iteration: 31225; Percent complete: 94.6%; Average loss: 0.6960\n",
            "Iteration: 31226; Percent complete: 94.6%; Average loss: 0.7308\n",
            "Iteration: 31227; Percent complete: 94.6%; Average loss: 0.7499\n",
            "Iteration: 31228; Percent complete: 94.6%; Average loss: 0.7064\n",
            "Iteration: 31229; Percent complete: 94.6%; Average loss: 0.7719\n",
            "Iteration: 31230; Percent complete: 94.6%; Average loss: 0.7621\n",
            "Iteration: 31231; Percent complete: 94.6%; Average loss: 0.5997\n",
            "Iteration: 31232; Percent complete: 94.6%; Average loss: 0.8527\n",
            "Iteration: 31233; Percent complete: 94.6%; Average loss: 0.5097\n",
            "Iteration: 31234; Percent complete: 94.6%; Average loss: 0.7846\n",
            "Iteration: 31235; Percent complete: 94.7%; Average loss: 0.9505\n",
            "Iteration: 31236; Percent complete: 94.7%; Average loss: 0.6173\n",
            "Iteration: 31237; Percent complete: 94.7%; Average loss: 0.5783\n",
            "Iteration: 31238; Percent complete: 94.7%; Average loss: 0.7780\n",
            "Iteration: 31239; Percent complete: 94.7%; Average loss: 0.8537\n",
            "Iteration: 31240; Percent complete: 94.7%; Average loss: 0.5847\n",
            "Iteration: 31241; Percent complete: 94.7%; Average loss: 0.6986\n",
            "Iteration: 31242; Percent complete: 94.7%; Average loss: 0.7065\n",
            "Iteration: 31243; Percent complete: 94.7%; Average loss: 0.6749\n",
            "Iteration: 31244; Percent complete: 94.7%; Average loss: 0.5984\n",
            "Iteration: 31245; Percent complete: 94.7%; Average loss: 0.8346\n",
            "Iteration: 31246; Percent complete: 94.7%; Average loss: 0.7503\n",
            "Iteration: 31247; Percent complete: 94.7%; Average loss: 0.8114\n",
            "Iteration: 31248; Percent complete: 94.7%; Average loss: 0.6500\n",
            "Iteration: 31249; Percent complete: 94.7%; Average loss: 0.8973\n",
            "Iteration: 31250; Percent complete: 94.7%; Average loss: 0.7997\n",
            "Iteration: 31251; Percent complete: 94.7%; Average loss: 0.6436\n",
            "Iteration: 31252; Percent complete: 94.7%; Average loss: 0.7187\n",
            "Iteration: 31253; Percent complete: 94.7%; Average loss: 0.6250\n",
            "Iteration: 31254; Percent complete: 94.7%; Average loss: 0.6582\n",
            "Iteration: 31255; Percent complete: 94.7%; Average loss: 0.6191\n",
            "Iteration: 31256; Percent complete: 94.7%; Average loss: 0.8050\n",
            "Iteration: 31257; Percent complete: 94.7%; Average loss: 0.7484\n",
            "Iteration: 31258; Percent complete: 94.7%; Average loss: 0.8344\n",
            "Iteration: 31259; Percent complete: 94.7%; Average loss: 0.8227\n",
            "Iteration: 31260; Percent complete: 94.7%; Average loss: 0.5699\n",
            "Iteration: 31261; Percent complete: 94.7%; Average loss: 0.5981\n",
            "Iteration: 31262; Percent complete: 94.7%; Average loss: 0.7544\n",
            "Iteration: 31263; Percent complete: 94.7%; Average loss: 0.6182\n",
            "Iteration: 31264; Percent complete: 94.7%; Average loss: 0.6869\n",
            "Iteration: 31265; Percent complete: 94.7%; Average loss: 0.6788\n",
            "Iteration: 31266; Percent complete: 94.7%; Average loss: 0.7455\n",
            "Iteration: 31267; Percent complete: 94.7%; Average loss: 0.6002\n",
            "Iteration: 31268; Percent complete: 94.8%; Average loss: 0.7095\n",
            "Iteration: 31269; Percent complete: 94.8%; Average loss: 0.7535\n",
            "Iteration: 31270; Percent complete: 94.8%; Average loss: 0.8290\n",
            "Iteration: 31271; Percent complete: 94.8%; Average loss: 0.5403\n",
            "Iteration: 31272; Percent complete: 94.8%; Average loss: 0.5996\n",
            "Iteration: 31273; Percent complete: 94.8%; Average loss: 0.7013\n",
            "Iteration: 31274; Percent complete: 94.8%; Average loss: 0.7036\n",
            "Iteration: 31275; Percent complete: 94.8%; Average loss: 0.6676\n",
            "Iteration: 31276; Percent complete: 94.8%; Average loss: 0.6655\n",
            "Iteration: 31277; Percent complete: 94.8%; Average loss: 0.6499\n",
            "Iteration: 31278; Percent complete: 94.8%; Average loss: 0.6250\n",
            "Iteration: 31279; Percent complete: 94.8%; Average loss: 0.7101\n",
            "Iteration: 31280; Percent complete: 94.8%; Average loss: 0.8330\n",
            "Iteration: 31281; Percent complete: 94.8%; Average loss: 0.6513\n",
            "Iteration: 31282; Percent complete: 94.8%; Average loss: 0.8656\n",
            "Iteration: 31283; Percent complete: 94.8%; Average loss: 0.7477\n",
            "Iteration: 31284; Percent complete: 94.8%; Average loss: 0.6034\n",
            "Iteration: 31285; Percent complete: 94.8%; Average loss: 0.5678\n",
            "Iteration: 31286; Percent complete: 94.8%; Average loss: 0.7412\n",
            "Iteration: 31287; Percent complete: 94.8%; Average loss: 0.9642\n",
            "Iteration: 31288; Percent complete: 94.8%; Average loss: 0.8033\n",
            "Iteration: 31289; Percent complete: 94.8%; Average loss: 0.6979\n",
            "Iteration: 31290; Percent complete: 94.8%; Average loss: 0.7137\n",
            "Iteration: 31291; Percent complete: 94.8%; Average loss: 0.7735\n",
            "Iteration: 31292; Percent complete: 94.8%; Average loss: 0.7098\n",
            "Iteration: 31293; Percent complete: 94.8%; Average loss: 0.8786\n",
            "Iteration: 31294; Percent complete: 94.8%; Average loss: 0.6372\n",
            "Iteration: 31295; Percent complete: 94.8%; Average loss: 0.5940\n",
            "Iteration: 31296; Percent complete: 94.8%; Average loss: 0.7234\n",
            "Iteration: 31297; Percent complete: 94.8%; Average loss: 0.8259\n",
            "Iteration: 31298; Percent complete: 94.8%; Average loss: 0.6049\n",
            "Iteration: 31299; Percent complete: 94.8%; Average loss: 0.6027\n",
            "Iteration: 31300; Percent complete: 94.8%; Average loss: 0.6626\n",
            "Iteration: 31301; Percent complete: 94.9%; Average loss: 0.7283\n",
            "Iteration: 31302; Percent complete: 94.9%; Average loss: 0.7044\n",
            "Iteration: 31303; Percent complete: 94.9%; Average loss: 0.6711\n",
            "Iteration: 31304; Percent complete: 94.9%; Average loss: 0.6646\n",
            "Iteration: 31305; Percent complete: 94.9%; Average loss: 0.7324\n",
            "Iteration: 31306; Percent complete: 94.9%; Average loss: 0.6351\n",
            "Iteration: 31307; Percent complete: 94.9%; Average loss: 0.7916\n",
            "Iteration: 31308; Percent complete: 94.9%; Average loss: 0.7120\n",
            "Iteration: 31309; Percent complete: 94.9%; Average loss: 0.6867\n",
            "Iteration: 31310; Percent complete: 94.9%; Average loss: 0.7863\n",
            "Iteration: 31311; Percent complete: 94.9%; Average loss: 0.7726\n",
            "Iteration: 31312; Percent complete: 94.9%; Average loss: 0.7532\n",
            "Iteration: 31313; Percent complete: 94.9%; Average loss: 0.8030\n",
            "Iteration: 31314; Percent complete: 94.9%; Average loss: 0.7434\n",
            "Iteration: 31315; Percent complete: 94.9%; Average loss: 0.7811\n",
            "Iteration: 31316; Percent complete: 94.9%; Average loss: 0.5597\n",
            "Iteration: 31317; Percent complete: 94.9%; Average loss: 0.6110\n",
            "Iteration: 31318; Percent complete: 94.9%; Average loss: 0.6961\n",
            "Iteration: 31319; Percent complete: 94.9%; Average loss: 0.7401\n",
            "Iteration: 31320; Percent complete: 94.9%; Average loss: 0.7733\n",
            "Iteration: 31321; Percent complete: 94.9%; Average loss: 0.6032\n",
            "Iteration: 31322; Percent complete: 94.9%; Average loss: 0.6810\n",
            "Iteration: 31323; Percent complete: 94.9%; Average loss: 0.5773\n",
            "Iteration: 31324; Percent complete: 94.9%; Average loss: 0.7880\n",
            "Iteration: 31325; Percent complete: 94.9%; Average loss: 0.6668\n",
            "Iteration: 31326; Percent complete: 94.9%; Average loss: 0.7314\n",
            "Iteration: 31327; Percent complete: 94.9%; Average loss: 0.6401\n",
            "Iteration: 31328; Percent complete: 94.9%; Average loss: 0.7150\n",
            "Iteration: 31329; Percent complete: 94.9%; Average loss: 0.7038\n",
            "Iteration: 31330; Percent complete: 94.9%; Average loss: 0.7759\n",
            "Iteration: 31331; Percent complete: 94.9%; Average loss: 0.6549\n",
            "Iteration: 31332; Percent complete: 94.9%; Average loss: 0.6518\n",
            "Iteration: 31333; Percent complete: 94.9%; Average loss: 0.6376\n",
            "Iteration: 31334; Percent complete: 95.0%; Average loss: 0.8122\n",
            "Iteration: 31335; Percent complete: 95.0%; Average loss: 0.6084\n",
            "Iteration: 31336; Percent complete: 95.0%; Average loss: 0.8247\n",
            "Iteration: 31337; Percent complete: 95.0%; Average loss: 0.5177\n",
            "Iteration: 31338; Percent complete: 95.0%; Average loss: 0.7438\n",
            "Iteration: 31339; Percent complete: 95.0%; Average loss: 0.7213\n",
            "Iteration: 31340; Percent complete: 95.0%; Average loss: 0.6659\n",
            "Iteration: 31341; Percent complete: 95.0%; Average loss: 0.6313\n",
            "Iteration: 31342; Percent complete: 95.0%; Average loss: 0.8017\n",
            "Iteration: 31343; Percent complete: 95.0%; Average loss: 0.5635\n",
            "Iteration: 31344; Percent complete: 95.0%; Average loss: 0.8146\n",
            "Iteration: 31345; Percent complete: 95.0%; Average loss: 0.6427\n",
            "Iteration: 31346; Percent complete: 95.0%; Average loss: 0.6290\n",
            "Iteration: 31347; Percent complete: 95.0%; Average loss: 0.7335\n",
            "Iteration: 31348; Percent complete: 95.0%; Average loss: 0.7321\n",
            "Iteration: 31349; Percent complete: 95.0%; Average loss: 0.5740\n",
            "Iteration: 31350; Percent complete: 95.0%; Average loss: 0.7338\n",
            "Iteration: 31351; Percent complete: 95.0%; Average loss: 0.6240\n",
            "Iteration: 31352; Percent complete: 95.0%; Average loss: 0.5767\n",
            "Iteration: 31353; Percent complete: 95.0%; Average loss: 0.7659\n",
            "Iteration: 31354; Percent complete: 95.0%; Average loss: 0.5388\n",
            "Iteration: 31355; Percent complete: 95.0%; Average loss: 0.6272\n",
            "Iteration: 31356; Percent complete: 95.0%; Average loss: 0.6811\n",
            "Iteration: 31357; Percent complete: 95.0%; Average loss: 0.7200\n",
            "Iteration: 31358; Percent complete: 95.0%; Average loss: 0.4808\n",
            "Iteration: 31359; Percent complete: 95.0%; Average loss: 0.6185\n",
            "Iteration: 31360; Percent complete: 95.0%; Average loss: 0.9380\n",
            "Iteration: 31361; Percent complete: 95.0%; Average loss: 0.7238\n",
            "Iteration: 31362; Percent complete: 95.0%; Average loss: 0.7411\n",
            "Iteration: 31363; Percent complete: 95.0%; Average loss: 0.7353\n",
            "Iteration: 31364; Percent complete: 95.0%; Average loss: 0.6976\n",
            "Iteration: 31365; Percent complete: 95.0%; Average loss: 0.6676\n",
            "Iteration: 31366; Percent complete: 95.0%; Average loss: 0.6166\n",
            "Iteration: 31367; Percent complete: 95.1%; Average loss: 0.7921\n",
            "Iteration: 31368; Percent complete: 95.1%; Average loss: 0.6937\n",
            "Iteration: 31369; Percent complete: 95.1%; Average loss: 0.7457\n",
            "Iteration: 31370; Percent complete: 95.1%; Average loss: 0.8726\n",
            "Iteration: 31371; Percent complete: 95.1%; Average loss: 0.5980\n",
            "Iteration: 31372; Percent complete: 95.1%; Average loss: 0.6372\n",
            "Iteration: 31373; Percent complete: 95.1%; Average loss: 0.8333\n",
            "Iteration: 31374; Percent complete: 95.1%; Average loss: 0.6791\n",
            "Iteration: 31375; Percent complete: 95.1%; Average loss: 0.4861\n",
            "Iteration: 31376; Percent complete: 95.1%; Average loss: 0.6283\n",
            "Iteration: 31377; Percent complete: 95.1%; Average loss: 0.7209\n",
            "Iteration: 31378; Percent complete: 95.1%; Average loss: 0.6891\n",
            "Iteration: 31379; Percent complete: 95.1%; Average loss: 0.6245\n",
            "Iteration: 31380; Percent complete: 95.1%; Average loss: 0.5764\n",
            "Iteration: 31381; Percent complete: 95.1%; Average loss: 0.6308\n",
            "Iteration: 31382; Percent complete: 95.1%; Average loss: 0.7735\n",
            "Iteration: 31383; Percent complete: 95.1%; Average loss: 0.6941\n",
            "Iteration: 31384; Percent complete: 95.1%; Average loss: 0.8115\n",
            "Iteration: 31385; Percent complete: 95.1%; Average loss: 0.6004\n",
            "Iteration: 31386; Percent complete: 95.1%; Average loss: 0.5523\n",
            "Iteration: 31387; Percent complete: 95.1%; Average loss: 0.6345\n",
            "Iteration: 31388; Percent complete: 95.1%; Average loss: 0.7845\n",
            "Iteration: 31389; Percent complete: 95.1%; Average loss: 0.5714\n",
            "Iteration: 31390; Percent complete: 95.1%; Average loss: 0.5929\n",
            "Iteration: 31391; Percent complete: 95.1%; Average loss: 0.6458\n",
            "Iteration: 31392; Percent complete: 95.1%; Average loss: 0.7618\n",
            "Iteration: 31393; Percent complete: 95.1%; Average loss: 0.7896\n",
            "Iteration: 31394; Percent complete: 95.1%; Average loss: 0.7154\n",
            "Iteration: 31395; Percent complete: 95.1%; Average loss: 0.7083\n",
            "Iteration: 31396; Percent complete: 95.1%; Average loss: 0.5561\n",
            "Iteration: 31397; Percent complete: 95.1%; Average loss: 0.6199\n",
            "Iteration: 31398; Percent complete: 95.1%; Average loss: 0.7349\n",
            "Iteration: 31399; Percent complete: 95.1%; Average loss: 0.6568\n",
            "Iteration: 31400; Percent complete: 95.2%; Average loss: 0.6674\n",
            "Iteration: 31401; Percent complete: 95.2%; Average loss: 0.5307\n",
            "Iteration: 31402; Percent complete: 95.2%; Average loss: 0.7243\n",
            "Iteration: 31403; Percent complete: 95.2%; Average loss: 0.7025\n",
            "Iteration: 31404; Percent complete: 95.2%; Average loss: 0.7536\n",
            "Iteration: 31405; Percent complete: 95.2%; Average loss: 0.5419\n",
            "Iteration: 31406; Percent complete: 95.2%; Average loss: 0.7785\n",
            "Iteration: 31407; Percent complete: 95.2%; Average loss: 0.8462\n",
            "Iteration: 31408; Percent complete: 95.2%; Average loss: 0.8575\n",
            "Iteration: 31409; Percent complete: 95.2%; Average loss: 0.6892\n",
            "Iteration: 31410; Percent complete: 95.2%; Average loss: 0.6814\n",
            "Iteration: 31411; Percent complete: 95.2%; Average loss: 0.6835\n",
            "Iteration: 31412; Percent complete: 95.2%; Average loss: 0.7667\n",
            "Iteration: 31413; Percent complete: 95.2%; Average loss: 0.7080\n",
            "Iteration: 31414; Percent complete: 95.2%; Average loss: 0.7374\n",
            "Iteration: 31415; Percent complete: 95.2%; Average loss: 0.6419\n",
            "Iteration: 31416; Percent complete: 95.2%; Average loss: 0.7551\n",
            "Iteration: 31417; Percent complete: 95.2%; Average loss: 0.7169\n",
            "Iteration: 31418; Percent complete: 95.2%; Average loss: 0.8116\n",
            "Iteration: 31419; Percent complete: 95.2%; Average loss: 0.8254\n",
            "Iteration: 31420; Percent complete: 95.2%; Average loss: 0.8703\n",
            "Iteration: 31421; Percent complete: 95.2%; Average loss: 0.5739\n",
            "Iteration: 31422; Percent complete: 95.2%; Average loss: 0.6782\n",
            "Iteration: 31423; Percent complete: 95.2%; Average loss: 0.8438\n",
            "Iteration: 31424; Percent complete: 95.2%; Average loss: 0.8275\n",
            "Iteration: 31425; Percent complete: 95.2%; Average loss: 0.5025\n",
            "Iteration: 31426; Percent complete: 95.2%; Average loss: 0.6872\n",
            "Iteration: 31427; Percent complete: 95.2%; Average loss: 0.7355\n",
            "Iteration: 31428; Percent complete: 95.2%; Average loss: 0.7258\n",
            "Iteration: 31429; Percent complete: 95.2%; Average loss: 0.7039\n",
            "Iteration: 31430; Percent complete: 95.2%; Average loss: 0.6148\n",
            "Iteration: 31431; Percent complete: 95.2%; Average loss: 0.6771\n",
            "Iteration: 31432; Percent complete: 95.2%; Average loss: 0.6309\n",
            "Iteration: 31433; Percent complete: 95.3%; Average loss: 0.6688\n",
            "Iteration: 31434; Percent complete: 95.3%; Average loss: 0.6810\n",
            "Iteration: 31435; Percent complete: 95.3%; Average loss: 0.6965\n",
            "Iteration: 31436; Percent complete: 95.3%; Average loss: 0.4896\n",
            "Iteration: 31437; Percent complete: 95.3%; Average loss: 0.5385\n",
            "Iteration: 31438; Percent complete: 95.3%; Average loss: 0.7431\n",
            "Iteration: 31439; Percent complete: 95.3%; Average loss: 0.7193\n",
            "Iteration: 31440; Percent complete: 95.3%; Average loss: 0.7323\n",
            "Iteration: 31441; Percent complete: 95.3%; Average loss: 0.7087\n",
            "Iteration: 31442; Percent complete: 95.3%; Average loss: 0.7739\n",
            "Iteration: 31443; Percent complete: 95.3%; Average loss: 0.7398\n",
            "Iteration: 31444; Percent complete: 95.3%; Average loss: 0.6043\n",
            "Iteration: 31445; Percent complete: 95.3%; Average loss: 0.6768\n",
            "Iteration: 31446; Percent complete: 95.3%; Average loss: 0.6372\n",
            "Iteration: 31447; Percent complete: 95.3%; Average loss: 0.6113\n",
            "Iteration: 31448; Percent complete: 95.3%; Average loss: 0.6762\n",
            "Iteration: 31449; Percent complete: 95.3%; Average loss: 0.7671\n",
            "Iteration: 31450; Percent complete: 95.3%; Average loss: 0.6487\n",
            "Iteration: 31451; Percent complete: 95.3%; Average loss: 0.7030\n",
            "Iteration: 31452; Percent complete: 95.3%; Average loss: 0.6808\n",
            "Iteration: 31453; Percent complete: 95.3%; Average loss: 0.6808\n",
            "Iteration: 31454; Percent complete: 95.3%; Average loss: 0.7691\n",
            "Iteration: 31455; Percent complete: 95.3%; Average loss: 0.5767\n",
            "Iteration: 31456; Percent complete: 95.3%; Average loss: 0.7635\n",
            "Iteration: 31457; Percent complete: 95.3%; Average loss: 0.6351\n",
            "Iteration: 31458; Percent complete: 95.3%; Average loss: 0.4636\n",
            "Iteration: 31459; Percent complete: 95.3%; Average loss: 0.5448\n",
            "Iteration: 31460; Percent complete: 95.3%; Average loss: 0.7558\n",
            "Iteration: 31461; Percent complete: 95.3%; Average loss: 0.6584\n",
            "Iteration: 31462; Percent complete: 95.3%; Average loss: 0.6559\n",
            "Iteration: 31463; Percent complete: 95.3%; Average loss: 0.6000\n",
            "Iteration: 31464; Percent complete: 95.3%; Average loss: 0.7056\n",
            "Iteration: 31465; Percent complete: 95.3%; Average loss: 0.7170\n",
            "Iteration: 31466; Percent complete: 95.4%; Average loss: 0.6140\n",
            "Iteration: 31467; Percent complete: 95.4%; Average loss: 0.6618\n",
            "Iteration: 31468; Percent complete: 95.4%; Average loss: 0.6361\n",
            "Iteration: 31469; Percent complete: 95.4%; Average loss: 0.8218\n",
            "Iteration: 31470; Percent complete: 95.4%; Average loss: 0.7458\n",
            "Iteration: 31471; Percent complete: 95.4%; Average loss: 0.8623\n",
            "Iteration: 31472; Percent complete: 95.4%; Average loss: 0.7071\n",
            "Iteration: 31473; Percent complete: 95.4%; Average loss: 0.6921\n",
            "Iteration: 31474; Percent complete: 95.4%; Average loss: 0.9027\n",
            "Iteration: 31475; Percent complete: 95.4%; Average loss: 0.8603\n",
            "Iteration: 31476; Percent complete: 95.4%; Average loss: 0.6224\n",
            "Iteration: 31477; Percent complete: 95.4%; Average loss: 0.7354\n",
            "Iteration: 31478; Percent complete: 95.4%; Average loss: 0.6277\n",
            "Iteration: 31479; Percent complete: 95.4%; Average loss: 0.6626\n",
            "Iteration: 31480; Percent complete: 95.4%; Average loss: 0.6854\n",
            "Iteration: 31481; Percent complete: 95.4%; Average loss: 0.7300\n",
            "Iteration: 31482; Percent complete: 95.4%; Average loss: 0.6755\n",
            "Iteration: 31483; Percent complete: 95.4%; Average loss: 0.6558\n",
            "Iteration: 31484; Percent complete: 95.4%; Average loss: 0.6751\n",
            "Iteration: 31485; Percent complete: 95.4%; Average loss: 0.7958\n",
            "Iteration: 31486; Percent complete: 95.4%; Average loss: 0.7987\n",
            "Iteration: 31487; Percent complete: 95.4%; Average loss: 0.7672\n",
            "Iteration: 31488; Percent complete: 95.4%; Average loss: 0.6855\n",
            "Iteration: 31489; Percent complete: 95.4%; Average loss: 0.6637\n",
            "Iteration: 31490; Percent complete: 95.4%; Average loss: 0.7329\n",
            "Iteration: 31491; Percent complete: 95.4%; Average loss: 0.6983\n",
            "Iteration: 31492; Percent complete: 95.4%; Average loss: 0.6476\n",
            "Iteration: 31493; Percent complete: 95.4%; Average loss: 0.6760\n",
            "Iteration: 31494; Percent complete: 95.4%; Average loss: 0.6001\n",
            "Iteration: 31495; Percent complete: 95.4%; Average loss: 0.7472\n",
            "Iteration: 31496; Percent complete: 95.4%; Average loss: 0.7156\n",
            "Iteration: 31497; Percent complete: 95.4%; Average loss: 0.6944\n",
            "Iteration: 31498; Percent complete: 95.4%; Average loss: 0.6543\n",
            "Iteration: 31499; Percent complete: 95.5%; Average loss: 0.6107\n",
            "Iteration: 31500; Percent complete: 95.5%; Average loss: 0.7508\n",
            "Iteration: 31501; Percent complete: 95.5%; Average loss: 0.6751\n",
            "Iteration: 31502; Percent complete: 95.5%; Average loss: 0.6516\n",
            "Iteration: 31503; Percent complete: 95.5%; Average loss: 0.6470\n",
            "Iteration: 31504; Percent complete: 95.5%; Average loss: 0.6750\n",
            "Iteration: 31505; Percent complete: 95.5%; Average loss: 0.5367\n",
            "Iteration: 31506; Percent complete: 95.5%; Average loss: 0.7300\n",
            "Iteration: 31507; Percent complete: 95.5%; Average loss: 0.7125\n",
            "Iteration: 31508; Percent complete: 95.5%; Average loss: 0.5209\n",
            "Iteration: 31509; Percent complete: 95.5%; Average loss: 0.6473\n",
            "Iteration: 31510; Percent complete: 95.5%; Average loss: 0.7819\n",
            "Iteration: 31511; Percent complete: 95.5%; Average loss: 0.8676\n",
            "Iteration: 31512; Percent complete: 95.5%; Average loss: 0.7063\n",
            "Iteration: 31513; Percent complete: 95.5%; Average loss: 0.6793\n",
            "Iteration: 31514; Percent complete: 95.5%; Average loss: 0.7149\n",
            "Iteration: 31515; Percent complete: 95.5%; Average loss: 0.5951\n",
            "Iteration: 31516; Percent complete: 95.5%; Average loss: 0.5920\n",
            "Iteration: 31517; Percent complete: 95.5%; Average loss: 0.6411\n",
            "Iteration: 31518; Percent complete: 95.5%; Average loss: 0.7367\n",
            "Iteration: 31519; Percent complete: 95.5%; Average loss: 0.8460\n",
            "Iteration: 31520; Percent complete: 95.5%; Average loss: 0.6561\n",
            "Iteration: 31521; Percent complete: 95.5%; Average loss: 0.6199\n",
            "Iteration: 31522; Percent complete: 95.5%; Average loss: 0.8414\n",
            "Iteration: 31523; Percent complete: 95.5%; Average loss: 0.7613\n",
            "Iteration: 31524; Percent complete: 95.5%; Average loss: 0.7477\n",
            "Iteration: 31525; Percent complete: 95.5%; Average loss: 0.6527\n",
            "Iteration: 31526; Percent complete: 95.5%; Average loss: 0.6533\n",
            "Iteration: 31527; Percent complete: 95.5%; Average loss: 0.5993\n",
            "Iteration: 31528; Percent complete: 95.5%; Average loss: 0.6992\n",
            "Iteration: 31529; Percent complete: 95.5%; Average loss: 0.7263\n",
            "Iteration: 31530; Percent complete: 95.5%; Average loss: 0.7620\n",
            "Iteration: 31531; Percent complete: 95.5%; Average loss: 0.6728\n",
            "Iteration: 31532; Percent complete: 95.6%; Average loss: 0.6034\n",
            "Iteration: 31533; Percent complete: 95.6%; Average loss: 0.6948\n",
            "Iteration: 31534; Percent complete: 95.6%; Average loss: 0.7603\n",
            "Iteration: 31535; Percent complete: 95.6%; Average loss: 0.7974\n",
            "Iteration: 31536; Percent complete: 95.6%; Average loss: 0.6106\n",
            "Iteration: 31537; Percent complete: 95.6%; Average loss: 0.6687\n",
            "Iteration: 31538; Percent complete: 95.6%; Average loss: 0.6811\n",
            "Iteration: 31539; Percent complete: 95.6%; Average loss: 0.6681\n",
            "Iteration: 31540; Percent complete: 95.6%; Average loss: 0.6472\n",
            "Iteration: 31541; Percent complete: 95.6%; Average loss: 0.7461\n",
            "Iteration: 31542; Percent complete: 95.6%; Average loss: 0.7528\n",
            "Iteration: 31543; Percent complete: 95.6%; Average loss: 0.7152\n",
            "Iteration: 31544; Percent complete: 95.6%; Average loss: 0.6959\n",
            "Iteration: 31545; Percent complete: 95.6%; Average loss: 0.5010\n",
            "Iteration: 31546; Percent complete: 95.6%; Average loss: 0.7967\n",
            "Iteration: 31547; Percent complete: 95.6%; Average loss: 0.7530\n",
            "Iteration: 31548; Percent complete: 95.6%; Average loss: 0.7588\n",
            "Iteration: 31549; Percent complete: 95.6%; Average loss: 0.7341\n",
            "Iteration: 31550; Percent complete: 95.6%; Average loss: 0.6662\n",
            "Iteration: 31551; Percent complete: 95.6%; Average loss: 0.7986\n",
            "Iteration: 31552; Percent complete: 95.6%; Average loss: 0.6864\n",
            "Iteration: 31553; Percent complete: 95.6%; Average loss: 0.6659\n",
            "Iteration: 31554; Percent complete: 95.6%; Average loss: 0.5598\n",
            "Iteration: 31555; Percent complete: 95.6%; Average loss: 0.6813\n",
            "Iteration: 31556; Percent complete: 95.6%; Average loss: 0.5852\n",
            "Iteration: 31557; Percent complete: 95.6%; Average loss: 0.6412\n",
            "Iteration: 31558; Percent complete: 95.6%; Average loss: 0.6414\n",
            "Iteration: 31559; Percent complete: 95.6%; Average loss: 0.7131\n",
            "Iteration: 31560; Percent complete: 95.6%; Average loss: 0.6737\n",
            "Iteration: 31561; Percent complete: 95.6%; Average loss: 0.7055\n",
            "Iteration: 31562; Percent complete: 95.6%; Average loss: 0.7617\n",
            "Iteration: 31563; Percent complete: 95.6%; Average loss: 0.7160\n",
            "Iteration: 31564; Percent complete: 95.6%; Average loss: 0.6549\n",
            "Iteration: 31565; Percent complete: 95.7%; Average loss: 0.6572\n",
            "Iteration: 31566; Percent complete: 95.7%; Average loss: 0.5880\n",
            "Iteration: 31567; Percent complete: 95.7%; Average loss: 0.6460\n",
            "Iteration: 31568; Percent complete: 95.7%; Average loss: 0.6422\n",
            "Iteration: 31569; Percent complete: 95.7%; Average loss: 0.7467\n",
            "Iteration: 31570; Percent complete: 95.7%; Average loss: 0.6077\n",
            "Iteration: 31571; Percent complete: 95.7%; Average loss: 0.8093\n",
            "Iteration: 31572; Percent complete: 95.7%; Average loss: 0.5915\n",
            "Iteration: 31573; Percent complete: 95.7%; Average loss: 0.6890\n",
            "Iteration: 31574; Percent complete: 95.7%; Average loss: 0.6393\n",
            "Iteration: 31575; Percent complete: 95.7%; Average loss: 0.7395\n",
            "Iteration: 31576; Percent complete: 95.7%; Average loss: 0.6000\n",
            "Iteration: 31577; Percent complete: 95.7%; Average loss: 0.6757\n",
            "Iteration: 31578; Percent complete: 95.7%; Average loss: 0.6275\n",
            "Iteration: 31579; Percent complete: 95.7%; Average loss: 0.6842\n",
            "Iteration: 31580; Percent complete: 95.7%; Average loss: 0.6039\n",
            "Iteration: 31581; Percent complete: 95.7%; Average loss: 0.8649\n",
            "Iteration: 31582; Percent complete: 95.7%; Average loss: 0.6954\n",
            "Iteration: 31583; Percent complete: 95.7%; Average loss: 0.7128\n",
            "Iteration: 31584; Percent complete: 95.7%; Average loss: 0.7655\n",
            "Iteration: 31585; Percent complete: 95.7%; Average loss: 0.8162\n",
            "Iteration: 31586; Percent complete: 95.7%; Average loss: 0.6612\n",
            "Iteration: 31587; Percent complete: 95.7%; Average loss: 0.6367\n",
            "Iteration: 31588; Percent complete: 95.7%; Average loss: 0.6319\n",
            "Iteration: 31589; Percent complete: 95.7%; Average loss: 0.6858\n",
            "Iteration: 31590; Percent complete: 95.7%; Average loss: 0.6972\n",
            "Iteration: 31591; Percent complete: 95.7%; Average loss: 0.7221\n",
            "Iteration: 31592; Percent complete: 95.7%; Average loss: 0.7263\n",
            "Iteration: 31593; Percent complete: 95.7%; Average loss: 0.6722\n",
            "Iteration: 31594; Percent complete: 95.7%; Average loss: 0.7198\n",
            "Iteration: 31595; Percent complete: 95.7%; Average loss: 0.7362\n",
            "Iteration: 31596; Percent complete: 95.7%; Average loss: 0.7785\n",
            "Iteration: 31597; Percent complete: 95.7%; Average loss: 0.7659\n",
            "Iteration: 31598; Percent complete: 95.8%; Average loss: 0.5914\n",
            "Iteration: 31599; Percent complete: 95.8%; Average loss: 0.7600\n",
            "Iteration: 31600; Percent complete: 95.8%; Average loss: 0.6223\n",
            "Iteration: 31601; Percent complete: 95.8%; Average loss: 0.6910\n",
            "Iteration: 31602; Percent complete: 95.8%; Average loss: 0.6181\n",
            "Iteration: 31603; Percent complete: 95.8%; Average loss: 0.6660\n",
            "Iteration: 31604; Percent complete: 95.8%; Average loss: 0.7252\n",
            "Iteration: 31605; Percent complete: 95.8%; Average loss: 0.6411\n",
            "Iteration: 31606; Percent complete: 95.8%; Average loss: 0.8038\n",
            "Iteration: 31607; Percent complete: 95.8%; Average loss: 0.7465\n",
            "Iteration: 31608; Percent complete: 95.8%; Average loss: 0.7653\n",
            "Iteration: 31609; Percent complete: 95.8%; Average loss: 0.6189\n",
            "Iteration: 31610; Percent complete: 95.8%; Average loss: 0.7659\n",
            "Iteration: 31611; Percent complete: 95.8%; Average loss: 0.6690\n",
            "Iteration: 31612; Percent complete: 95.8%; Average loss: 0.5984\n",
            "Iteration: 31613; Percent complete: 95.8%; Average loss: 0.7439\n",
            "Iteration: 31614; Percent complete: 95.8%; Average loss: 0.6817\n",
            "Iteration: 31615; Percent complete: 95.8%; Average loss: 0.6776\n",
            "Iteration: 31616; Percent complete: 95.8%; Average loss: 0.7318\n",
            "Iteration: 31617; Percent complete: 95.8%; Average loss: 0.6496\n",
            "Iteration: 31618; Percent complete: 95.8%; Average loss: 0.6550\n",
            "Iteration: 31619; Percent complete: 95.8%; Average loss: 0.7363\n",
            "Iteration: 31620; Percent complete: 95.8%; Average loss: 0.6042\n",
            "Iteration: 31621; Percent complete: 95.8%; Average loss: 0.6002\n",
            "Iteration: 31622; Percent complete: 95.8%; Average loss: 0.5883\n",
            "Iteration: 31623; Percent complete: 95.8%; Average loss: 0.5480\n",
            "Iteration: 31624; Percent complete: 95.8%; Average loss: 0.6094\n",
            "Iteration: 31625; Percent complete: 95.8%; Average loss: 0.7331\n",
            "Iteration: 31626; Percent complete: 95.8%; Average loss: 0.8047\n",
            "Iteration: 31627; Percent complete: 95.8%; Average loss: 0.6527\n",
            "Iteration: 31628; Percent complete: 95.8%; Average loss: 0.8553\n",
            "Iteration: 31629; Percent complete: 95.8%; Average loss: 0.6507\n",
            "Iteration: 31630; Percent complete: 95.8%; Average loss: 0.8039\n",
            "Iteration: 31631; Percent complete: 95.9%; Average loss: 0.7082\n",
            "Iteration: 31632; Percent complete: 95.9%; Average loss: 0.7072\n",
            "Iteration: 31633; Percent complete: 95.9%; Average loss: 0.6919\n",
            "Iteration: 31634; Percent complete: 95.9%; Average loss: 0.5960\n",
            "Iteration: 31635; Percent complete: 95.9%; Average loss: 0.6994\n",
            "Iteration: 31636; Percent complete: 95.9%; Average loss: 0.7134\n",
            "Iteration: 31637; Percent complete: 95.9%; Average loss: 0.7061\n",
            "Iteration: 31638; Percent complete: 95.9%; Average loss: 0.7263\n",
            "Iteration: 31639; Percent complete: 95.9%; Average loss: 0.6095\n",
            "Iteration: 31640; Percent complete: 95.9%; Average loss: 0.5817\n",
            "Iteration: 31641; Percent complete: 95.9%; Average loss: 0.6085\n",
            "Iteration: 31642; Percent complete: 95.9%; Average loss: 0.7408\n",
            "Iteration: 31643; Percent complete: 95.9%; Average loss: 0.7333\n",
            "Iteration: 31644; Percent complete: 95.9%; Average loss: 0.6018\n",
            "Iteration: 31645; Percent complete: 95.9%; Average loss: 0.5578\n",
            "Iteration: 31646; Percent complete: 95.9%; Average loss: 0.6194\n",
            "Iteration: 31647; Percent complete: 95.9%; Average loss: 0.6424\n",
            "Iteration: 31648; Percent complete: 95.9%; Average loss: 0.6179\n",
            "Iteration: 31649; Percent complete: 95.9%; Average loss: 0.5820\n",
            "Iteration: 31650; Percent complete: 95.9%; Average loss: 0.7226\n",
            "Iteration: 31651; Percent complete: 95.9%; Average loss: 0.6603\n",
            "Iteration: 31652; Percent complete: 95.9%; Average loss: 0.7039\n",
            "Iteration: 31653; Percent complete: 95.9%; Average loss: 0.5662\n",
            "Iteration: 31654; Percent complete: 95.9%; Average loss: 0.5749\n",
            "Iteration: 31655; Percent complete: 95.9%; Average loss: 0.6434\n",
            "Iteration: 31656; Percent complete: 95.9%; Average loss: 0.6379\n",
            "Iteration: 31657; Percent complete: 95.9%; Average loss: 0.8272\n",
            "Iteration: 31658; Percent complete: 95.9%; Average loss: 0.8013\n",
            "Iteration: 31659; Percent complete: 95.9%; Average loss: 0.7285\n",
            "Iteration: 31660; Percent complete: 95.9%; Average loss: 0.5139\n",
            "Iteration: 31661; Percent complete: 95.9%; Average loss: 0.7489\n",
            "Iteration: 31662; Percent complete: 95.9%; Average loss: 0.7131\n",
            "Iteration: 31663; Percent complete: 95.9%; Average loss: 0.7487\n",
            "Iteration: 31664; Percent complete: 96.0%; Average loss: 0.5632\n",
            "Iteration: 31665; Percent complete: 96.0%; Average loss: 0.6446\n",
            "Iteration: 31666; Percent complete: 96.0%; Average loss: 0.7097\n",
            "Iteration: 31667; Percent complete: 96.0%; Average loss: 0.7169\n",
            "Iteration: 31668; Percent complete: 96.0%; Average loss: 0.4664\n",
            "Iteration: 31669; Percent complete: 96.0%; Average loss: 0.7206\n",
            "Iteration: 31670; Percent complete: 96.0%; Average loss: 0.8120\n",
            "Iteration: 31671; Percent complete: 96.0%; Average loss: 0.8132\n",
            "Iteration: 31672; Percent complete: 96.0%; Average loss: 0.5631\n",
            "Iteration: 31673; Percent complete: 96.0%; Average loss: 0.8336\n",
            "Iteration: 31674; Percent complete: 96.0%; Average loss: 0.6692\n",
            "Iteration: 31675; Percent complete: 96.0%; Average loss: 0.6654\n",
            "Iteration: 31676; Percent complete: 96.0%; Average loss: 0.7103\n",
            "Iteration: 31677; Percent complete: 96.0%; Average loss: 0.7149\n",
            "Iteration: 31678; Percent complete: 96.0%; Average loss: 0.7547\n",
            "Iteration: 31679; Percent complete: 96.0%; Average loss: 0.6899\n",
            "Iteration: 31680; Percent complete: 96.0%; Average loss: 0.5365\n",
            "Iteration: 31681; Percent complete: 96.0%; Average loss: 0.5493\n",
            "Iteration: 31682; Percent complete: 96.0%; Average loss: 0.6144\n",
            "Iteration: 31683; Percent complete: 96.0%; Average loss: 0.5483\n",
            "Iteration: 31684; Percent complete: 96.0%; Average loss: 0.6943\n",
            "Iteration: 31685; Percent complete: 96.0%; Average loss: 0.9356\n",
            "Iteration: 31686; Percent complete: 96.0%; Average loss: 0.6085\n",
            "Iteration: 31687; Percent complete: 96.0%; Average loss: 0.6938\n",
            "Iteration: 31688; Percent complete: 96.0%; Average loss: 0.7305\n",
            "Iteration: 31689; Percent complete: 96.0%; Average loss: 0.6089\n",
            "Iteration: 31690; Percent complete: 96.0%; Average loss: 0.5684\n",
            "Iteration: 31691; Percent complete: 96.0%; Average loss: 0.7710\n",
            "Iteration: 31692; Percent complete: 96.0%; Average loss: 0.6789\n",
            "Iteration: 31693; Percent complete: 96.0%; Average loss: 0.8659\n",
            "Iteration: 31694; Percent complete: 96.0%; Average loss: 0.6924\n",
            "Iteration: 31695; Percent complete: 96.0%; Average loss: 0.6371\n",
            "Iteration: 31696; Percent complete: 96.0%; Average loss: 0.6012\n",
            "Iteration: 31697; Percent complete: 96.1%; Average loss: 0.6791\n",
            "Iteration: 31698; Percent complete: 96.1%; Average loss: 0.5267\n",
            "Iteration: 31699; Percent complete: 96.1%; Average loss: 0.5655\n",
            "Iteration: 31700; Percent complete: 96.1%; Average loss: 0.6780\n",
            "Iteration: 31701; Percent complete: 96.1%; Average loss: 0.6261\n",
            "Iteration: 31702; Percent complete: 96.1%; Average loss: 0.6721\n",
            "Iteration: 31703; Percent complete: 96.1%; Average loss: 0.7463\n",
            "Iteration: 31704; Percent complete: 96.1%; Average loss: 0.5915\n",
            "Iteration: 31705; Percent complete: 96.1%; Average loss: 0.6085\n",
            "Iteration: 31706; Percent complete: 96.1%; Average loss: 0.6536\n",
            "Iteration: 31707; Percent complete: 96.1%; Average loss: 0.6642\n",
            "Iteration: 31708; Percent complete: 96.1%; Average loss: 0.6433\n",
            "Iteration: 31709; Percent complete: 96.1%; Average loss: 0.5738\n",
            "Iteration: 31710; Percent complete: 96.1%; Average loss: 0.6437\n",
            "Iteration: 31711; Percent complete: 96.1%; Average loss: 0.7198\n",
            "Iteration: 31712; Percent complete: 96.1%; Average loss: 0.6338\n",
            "Iteration: 31713; Percent complete: 96.1%; Average loss: 0.6825\n",
            "Iteration: 31714; Percent complete: 96.1%; Average loss: 0.5770\n",
            "Iteration: 31715; Percent complete: 96.1%; Average loss: 0.6567\n",
            "Iteration: 31716; Percent complete: 96.1%; Average loss: 0.8020\n",
            "Iteration: 31717; Percent complete: 96.1%; Average loss: 0.6767\n",
            "Iteration: 31718; Percent complete: 96.1%; Average loss: 0.7002\n",
            "Iteration: 31719; Percent complete: 96.1%; Average loss: 0.6558\n",
            "Iteration: 31720; Percent complete: 96.1%; Average loss: 0.7734\n",
            "Iteration: 31721; Percent complete: 96.1%; Average loss: 0.7425\n",
            "Iteration: 31722; Percent complete: 96.1%; Average loss: 0.6670\n",
            "Iteration: 31723; Percent complete: 96.1%; Average loss: 0.6829\n",
            "Iteration: 31724; Percent complete: 96.1%; Average loss: 0.5140\n",
            "Iteration: 31725; Percent complete: 96.1%; Average loss: 0.7255\n",
            "Iteration: 31726; Percent complete: 96.1%; Average loss: 0.7463\n",
            "Iteration: 31727; Percent complete: 96.1%; Average loss: 0.6346\n",
            "Iteration: 31728; Percent complete: 96.1%; Average loss: 0.6625\n",
            "Iteration: 31729; Percent complete: 96.1%; Average loss: 0.7724\n",
            "Iteration: 31730; Percent complete: 96.2%; Average loss: 0.7094\n",
            "Iteration: 31731; Percent complete: 96.2%; Average loss: 0.6807\n",
            "Iteration: 31732; Percent complete: 96.2%; Average loss: 0.7327\n",
            "Iteration: 31733; Percent complete: 96.2%; Average loss: 0.6521\n",
            "Iteration: 31734; Percent complete: 96.2%; Average loss: 0.5963\n",
            "Iteration: 31735; Percent complete: 96.2%; Average loss: 0.5733\n",
            "Iteration: 31736; Percent complete: 96.2%; Average loss: 0.9346\n",
            "Iteration: 31737; Percent complete: 96.2%; Average loss: 0.6450\n",
            "Iteration: 31738; Percent complete: 96.2%; Average loss: 0.6466\n",
            "Iteration: 31739; Percent complete: 96.2%; Average loss: 0.5344\n",
            "Iteration: 31740; Percent complete: 96.2%; Average loss: 0.5795\n",
            "Iteration: 31741; Percent complete: 96.2%; Average loss: 0.7248\n",
            "Iteration: 31742; Percent complete: 96.2%; Average loss: 0.6567\n",
            "Iteration: 31743; Percent complete: 96.2%; Average loss: 0.6525\n",
            "Iteration: 31744; Percent complete: 96.2%; Average loss: 0.7189\n",
            "Iteration: 31745; Percent complete: 96.2%; Average loss: 0.8206\n",
            "Iteration: 31746; Percent complete: 96.2%; Average loss: 0.5885\n",
            "Iteration: 31747; Percent complete: 96.2%; Average loss: 0.6958\n",
            "Iteration: 31748; Percent complete: 96.2%; Average loss: 0.7174\n",
            "Iteration: 31749; Percent complete: 96.2%; Average loss: 0.6467\n",
            "Iteration: 31750; Percent complete: 96.2%; Average loss: 0.6913\n",
            "Iteration: 31751; Percent complete: 96.2%; Average loss: 0.7227\n",
            "Iteration: 31752; Percent complete: 96.2%; Average loss: 0.7844\n",
            "Iteration: 31753; Percent complete: 96.2%; Average loss: 0.7567\n",
            "Iteration: 31754; Percent complete: 96.2%; Average loss: 0.4468\n",
            "Iteration: 31755; Percent complete: 96.2%; Average loss: 0.6807\n",
            "Iteration: 31756; Percent complete: 96.2%; Average loss: 0.7056\n",
            "Iteration: 31757; Percent complete: 96.2%; Average loss: 0.4921\n",
            "Iteration: 31758; Percent complete: 96.2%; Average loss: 0.7190\n",
            "Iteration: 31759; Percent complete: 96.2%; Average loss: 0.6746\n",
            "Iteration: 31760; Percent complete: 96.2%; Average loss: 0.6027\n",
            "Iteration: 31761; Percent complete: 96.2%; Average loss: 0.5938\n",
            "Iteration: 31762; Percent complete: 96.2%; Average loss: 0.5296\n",
            "Iteration: 31763; Percent complete: 96.3%; Average loss: 0.7436\n",
            "Iteration: 31764; Percent complete: 96.3%; Average loss: 0.5993\n",
            "Iteration: 31765; Percent complete: 96.3%; Average loss: 0.6156\n",
            "Iteration: 31766; Percent complete: 96.3%; Average loss: 0.8438\n",
            "Iteration: 31767; Percent complete: 96.3%; Average loss: 0.7280\n",
            "Iteration: 31768; Percent complete: 96.3%; Average loss: 0.7842\n",
            "Iteration: 31769; Percent complete: 96.3%; Average loss: 0.7957\n",
            "Iteration: 31770; Percent complete: 96.3%; Average loss: 0.5729\n",
            "Iteration: 31771; Percent complete: 96.3%; Average loss: 0.6312\n",
            "Iteration: 31772; Percent complete: 96.3%; Average loss: 0.7102\n",
            "Iteration: 31773; Percent complete: 96.3%; Average loss: 0.7216\n",
            "Iteration: 31774; Percent complete: 96.3%; Average loss: 0.6385\n",
            "Iteration: 31775; Percent complete: 96.3%; Average loss: 0.5544\n",
            "Iteration: 31776; Percent complete: 96.3%; Average loss: 0.7680\n",
            "Iteration: 31777; Percent complete: 96.3%; Average loss: 0.7228\n",
            "Iteration: 31778; Percent complete: 96.3%; Average loss: 0.6240\n",
            "Iteration: 31779; Percent complete: 96.3%; Average loss: 0.6704\n",
            "Iteration: 31780; Percent complete: 96.3%; Average loss: 0.7129\n",
            "Iteration: 31781; Percent complete: 96.3%; Average loss: 0.6973\n",
            "Iteration: 31782; Percent complete: 96.3%; Average loss: 0.6950\n",
            "Iteration: 31783; Percent complete: 96.3%; Average loss: 0.7044\n",
            "Iteration: 31784; Percent complete: 96.3%; Average loss: 0.7144\n",
            "Iteration: 31785; Percent complete: 96.3%; Average loss: 0.7832\n",
            "Iteration: 31786; Percent complete: 96.3%; Average loss: 0.5628\n",
            "Iteration: 31787; Percent complete: 96.3%; Average loss: 0.6089\n",
            "Iteration: 31788; Percent complete: 96.3%; Average loss: 0.5839\n",
            "Iteration: 31789; Percent complete: 96.3%; Average loss: 0.6530\n",
            "Iteration: 31790; Percent complete: 96.3%; Average loss: 0.6441\n",
            "Iteration: 31791; Percent complete: 96.3%; Average loss: 0.8160\n",
            "Iteration: 31792; Percent complete: 96.3%; Average loss: 0.6337\n",
            "Iteration: 31793; Percent complete: 96.3%; Average loss: 0.6329\n",
            "Iteration: 31794; Percent complete: 96.3%; Average loss: 0.6544\n",
            "Iteration: 31795; Percent complete: 96.3%; Average loss: 0.7844\n",
            "Iteration: 31796; Percent complete: 96.4%; Average loss: 0.6276\n",
            "Iteration: 31797; Percent complete: 96.4%; Average loss: 0.6363\n",
            "Iteration: 31798; Percent complete: 96.4%; Average loss: 0.7446\n",
            "Iteration: 31799; Percent complete: 96.4%; Average loss: 0.7961\n",
            "Iteration: 31800; Percent complete: 96.4%; Average loss: 0.6355\n",
            "Iteration: 31801; Percent complete: 96.4%; Average loss: 0.6022\n",
            "Iteration: 31802; Percent complete: 96.4%; Average loss: 0.6286\n",
            "Iteration: 31803; Percent complete: 96.4%; Average loss: 0.6886\n",
            "Iteration: 31804; Percent complete: 96.4%; Average loss: 0.5385\n",
            "Iteration: 31805; Percent complete: 96.4%; Average loss: 0.8284\n",
            "Iteration: 31806; Percent complete: 96.4%; Average loss: 0.6303\n",
            "Iteration: 31807; Percent complete: 96.4%; Average loss: 0.7772\n",
            "Iteration: 31808; Percent complete: 96.4%; Average loss: 0.5649\n",
            "Iteration: 31809; Percent complete: 96.4%; Average loss: 0.7598\n",
            "Iteration: 31810; Percent complete: 96.4%; Average loss: 0.6675\n",
            "Iteration: 31811; Percent complete: 96.4%; Average loss: 0.7306\n",
            "Iteration: 31812; Percent complete: 96.4%; Average loss: 0.7657\n",
            "Iteration: 31813; Percent complete: 96.4%; Average loss: 0.6780\n",
            "Iteration: 31814; Percent complete: 96.4%; Average loss: 0.7450\n",
            "Iteration: 31815; Percent complete: 96.4%; Average loss: 0.6050\n",
            "Iteration: 31816; Percent complete: 96.4%; Average loss: 0.7107\n",
            "Iteration: 31817; Percent complete: 96.4%; Average loss: 0.7644\n",
            "Iteration: 31818; Percent complete: 96.4%; Average loss: 0.5985\n",
            "Iteration: 31819; Percent complete: 96.4%; Average loss: 0.6155\n",
            "Iteration: 31820; Percent complete: 96.4%; Average loss: 0.7129\n",
            "Iteration: 31821; Percent complete: 96.4%; Average loss: 0.5856\n",
            "Iteration: 31822; Percent complete: 96.4%; Average loss: 0.6447\n",
            "Iteration: 31823; Percent complete: 96.4%; Average loss: 0.6645\n",
            "Iteration: 31824; Percent complete: 96.4%; Average loss: 0.7610\n",
            "Iteration: 31825; Percent complete: 96.4%; Average loss: 0.6493\n",
            "Iteration: 31826; Percent complete: 96.4%; Average loss: 0.8017\n",
            "Iteration: 31827; Percent complete: 96.4%; Average loss: 0.5447\n",
            "Iteration: 31828; Percent complete: 96.4%; Average loss: 0.7593\n",
            "Iteration: 31829; Percent complete: 96.5%; Average loss: 0.7608\n",
            "Iteration: 31830; Percent complete: 96.5%; Average loss: 0.6460\n",
            "Iteration: 31831; Percent complete: 96.5%; Average loss: 0.6170\n",
            "Iteration: 31832; Percent complete: 96.5%; Average loss: 0.8534\n",
            "Iteration: 31833; Percent complete: 96.5%; Average loss: 0.6447\n",
            "Iteration: 31834; Percent complete: 96.5%; Average loss: 0.6539\n",
            "Iteration: 31835; Percent complete: 96.5%; Average loss: 0.7210\n",
            "Iteration: 31836; Percent complete: 96.5%; Average loss: 0.7016\n",
            "Iteration: 31837; Percent complete: 96.5%; Average loss: 0.6738\n",
            "Iteration: 31838; Percent complete: 96.5%; Average loss: 0.5514\n",
            "Iteration: 31839; Percent complete: 96.5%; Average loss: 0.6244\n",
            "Iteration: 31840; Percent complete: 96.5%; Average loss: 0.5412\n",
            "Iteration: 31841; Percent complete: 96.5%; Average loss: 0.6762\n",
            "Iteration: 31842; Percent complete: 96.5%; Average loss: 0.7517\n",
            "Iteration: 31843; Percent complete: 96.5%; Average loss: 0.6059\n",
            "Iteration: 31844; Percent complete: 96.5%; Average loss: 0.5802\n",
            "Iteration: 31845; Percent complete: 96.5%; Average loss: 0.6657\n",
            "Iteration: 31846; Percent complete: 96.5%; Average loss: 0.6598\n",
            "Iteration: 31847; Percent complete: 96.5%; Average loss: 0.8018\n",
            "Iteration: 31848; Percent complete: 96.5%; Average loss: 0.7254\n",
            "Iteration: 31849; Percent complete: 96.5%; Average loss: 0.6172\n",
            "Iteration: 31850; Percent complete: 96.5%; Average loss: 0.7456\n",
            "Iteration: 31851; Percent complete: 96.5%; Average loss: 0.6790\n",
            "Iteration: 31852; Percent complete: 96.5%; Average loss: 0.6366\n",
            "Iteration: 31853; Percent complete: 96.5%; Average loss: 0.7686\n",
            "Iteration: 31854; Percent complete: 96.5%; Average loss: 0.6139\n",
            "Iteration: 31855; Percent complete: 96.5%; Average loss: 0.6934\n",
            "Iteration: 31856; Percent complete: 96.5%; Average loss: 0.6600\n",
            "Iteration: 31857; Percent complete: 96.5%; Average loss: 0.6912\n",
            "Iteration: 31858; Percent complete: 96.5%; Average loss: 0.6148\n",
            "Iteration: 31859; Percent complete: 96.5%; Average loss: 0.6634\n",
            "Iteration: 31860; Percent complete: 96.5%; Average loss: 0.6580\n",
            "Iteration: 31861; Percent complete: 96.5%; Average loss: 0.6985\n",
            "Iteration: 31862; Percent complete: 96.6%; Average loss: 0.6516\n",
            "Iteration: 31863; Percent complete: 96.6%; Average loss: 0.5877\n",
            "Iteration: 31864; Percent complete: 96.6%; Average loss: 0.6890\n",
            "Iteration: 31865; Percent complete: 96.6%; Average loss: 0.8460\n",
            "Iteration: 31866; Percent complete: 96.6%; Average loss: 0.6218\n",
            "Iteration: 31867; Percent complete: 96.6%; Average loss: 0.6203\n",
            "Iteration: 31868; Percent complete: 96.6%; Average loss: 0.6621\n",
            "Iteration: 31869; Percent complete: 96.6%; Average loss: 0.7188\n",
            "Iteration: 31870; Percent complete: 96.6%; Average loss: 0.6252\n",
            "Iteration: 31871; Percent complete: 96.6%; Average loss: 0.7375\n",
            "Iteration: 31872; Percent complete: 96.6%; Average loss: 0.5858\n",
            "Iteration: 31873; Percent complete: 96.6%; Average loss: 0.6398\n",
            "Iteration: 31874; Percent complete: 96.6%; Average loss: 0.7454\n",
            "Iteration: 31875; Percent complete: 96.6%; Average loss: 0.6992\n",
            "Iteration: 31876; Percent complete: 96.6%; Average loss: 0.6744\n",
            "Iteration: 31877; Percent complete: 96.6%; Average loss: 0.7803\n",
            "Iteration: 31878; Percent complete: 96.6%; Average loss: 0.6122\n",
            "Iteration: 31879; Percent complete: 96.6%; Average loss: 0.6006\n",
            "Iteration: 31880; Percent complete: 96.6%; Average loss: 0.6707\n",
            "Iteration: 31881; Percent complete: 96.6%; Average loss: 0.7630\n",
            "Iteration: 31882; Percent complete: 96.6%; Average loss: 0.5459\n",
            "Iteration: 31883; Percent complete: 96.6%; Average loss: 0.6478\n",
            "Iteration: 31884; Percent complete: 96.6%; Average loss: 0.6197\n",
            "Iteration: 31885; Percent complete: 96.6%; Average loss: 0.6973\n",
            "Iteration: 31886; Percent complete: 96.6%; Average loss: 0.7899\n",
            "Iteration: 31887; Percent complete: 96.6%; Average loss: 0.6476\n",
            "Iteration: 31888; Percent complete: 96.6%; Average loss: 0.5373\n",
            "Iteration: 31889; Percent complete: 96.6%; Average loss: 0.5603\n",
            "Iteration: 31890; Percent complete: 96.6%; Average loss: 0.6882\n",
            "Iteration: 31891; Percent complete: 96.6%; Average loss: 0.6495\n",
            "Iteration: 31892; Percent complete: 96.6%; Average loss: 0.7055\n",
            "Iteration: 31893; Percent complete: 96.6%; Average loss: 0.7375\n",
            "Iteration: 31894; Percent complete: 96.6%; Average loss: 0.5821\n",
            "Iteration: 31895; Percent complete: 96.7%; Average loss: 0.6610\n",
            "Iteration: 31896; Percent complete: 96.7%; Average loss: 0.6406\n",
            "Iteration: 31897; Percent complete: 96.7%; Average loss: 0.7164\n",
            "Iteration: 31898; Percent complete: 96.7%; Average loss: 0.6461\n",
            "Iteration: 31899; Percent complete: 96.7%; Average loss: 0.6830\n",
            "Iteration: 31900; Percent complete: 96.7%; Average loss: 0.6921\n",
            "Iteration: 31901; Percent complete: 96.7%; Average loss: 0.6978\n",
            "Iteration: 31902; Percent complete: 96.7%; Average loss: 0.7983\n",
            "Iteration: 31903; Percent complete: 96.7%; Average loss: 0.7891\n",
            "Iteration: 31904; Percent complete: 96.7%; Average loss: 0.6022\n",
            "Iteration: 31905; Percent complete: 96.7%; Average loss: 0.5781\n",
            "Iteration: 31906; Percent complete: 96.7%; Average loss: 0.6300\n",
            "Iteration: 31907; Percent complete: 96.7%; Average loss: 0.7212\n",
            "Iteration: 31908; Percent complete: 96.7%; Average loss: 0.6732\n",
            "Iteration: 31909; Percent complete: 96.7%; Average loss: 0.6113\n",
            "Iteration: 31910; Percent complete: 96.7%; Average loss: 0.7199\n",
            "Iteration: 31911; Percent complete: 96.7%; Average loss: 0.5950\n",
            "Iteration: 31912; Percent complete: 96.7%; Average loss: 0.6438\n",
            "Iteration: 31913; Percent complete: 96.7%; Average loss: 0.6737\n",
            "Iteration: 31914; Percent complete: 96.7%; Average loss: 0.7377\n",
            "Iteration: 31915; Percent complete: 96.7%; Average loss: 0.7109\n",
            "Iteration: 31916; Percent complete: 96.7%; Average loss: 0.7171\n",
            "Iteration: 31917; Percent complete: 96.7%; Average loss: 0.6486\n",
            "Iteration: 31918; Percent complete: 96.7%; Average loss: 0.7722\n",
            "Iteration: 31919; Percent complete: 96.7%; Average loss: 0.7448\n",
            "Iteration: 31920; Percent complete: 96.7%; Average loss: 0.6445\n",
            "Iteration: 31921; Percent complete: 96.7%; Average loss: 0.7552\n",
            "Iteration: 31922; Percent complete: 96.7%; Average loss: 0.6840\n",
            "Iteration: 31923; Percent complete: 96.7%; Average loss: 0.6949\n",
            "Iteration: 31924; Percent complete: 96.7%; Average loss: 0.7133\n",
            "Iteration: 31925; Percent complete: 96.7%; Average loss: 0.7479\n",
            "Iteration: 31926; Percent complete: 96.7%; Average loss: 0.7225\n",
            "Iteration: 31927; Percent complete: 96.7%; Average loss: 0.7339\n",
            "Iteration: 31928; Percent complete: 96.8%; Average loss: 0.5514\n",
            "Iteration: 31929; Percent complete: 96.8%; Average loss: 0.5270\n",
            "Iteration: 31930; Percent complete: 96.8%; Average loss: 0.5395\n",
            "Iteration: 31931; Percent complete: 96.8%; Average loss: 0.7319\n",
            "Iteration: 31932; Percent complete: 96.8%; Average loss: 0.8133\n",
            "Iteration: 31933; Percent complete: 96.8%; Average loss: 0.6604\n",
            "Iteration: 31934; Percent complete: 96.8%; Average loss: 0.6426\n",
            "Iteration: 31935; Percent complete: 96.8%; Average loss: 0.6728\n",
            "Iteration: 31936; Percent complete: 96.8%; Average loss: 0.5929\n",
            "Iteration: 31937; Percent complete: 96.8%; Average loss: 0.7660\n",
            "Iteration: 31938; Percent complete: 96.8%; Average loss: 0.6901\n",
            "Iteration: 31939; Percent complete: 96.8%; Average loss: 0.7303\n",
            "Iteration: 31940; Percent complete: 96.8%; Average loss: 0.7059\n",
            "Iteration: 31941; Percent complete: 96.8%; Average loss: 0.5823\n",
            "Iteration: 31942; Percent complete: 96.8%; Average loss: 0.5717\n",
            "Iteration: 31943; Percent complete: 96.8%; Average loss: 0.8396\n",
            "Iteration: 31944; Percent complete: 96.8%; Average loss: 0.6410\n",
            "Iteration: 31945; Percent complete: 96.8%; Average loss: 0.7277\n",
            "Iteration: 31946; Percent complete: 96.8%; Average loss: 0.7130\n",
            "Iteration: 31947; Percent complete: 96.8%; Average loss: 0.6987\n",
            "Iteration: 31948; Percent complete: 96.8%; Average loss: 0.6900\n",
            "Iteration: 31949; Percent complete: 96.8%; Average loss: 0.7171\n",
            "Iteration: 31950; Percent complete: 96.8%; Average loss: 0.7694\n",
            "Iteration: 31951; Percent complete: 96.8%; Average loss: 0.7364\n",
            "Iteration: 31952; Percent complete: 96.8%; Average loss: 0.7491\n",
            "Iteration: 31953; Percent complete: 96.8%; Average loss: 0.5146\n",
            "Iteration: 31954; Percent complete: 96.8%; Average loss: 0.7034\n",
            "Iteration: 31955; Percent complete: 96.8%; Average loss: 0.6312\n",
            "Iteration: 31956; Percent complete: 96.8%; Average loss: 0.8302\n",
            "Iteration: 31957; Percent complete: 96.8%; Average loss: 0.5271\n",
            "Iteration: 31958; Percent complete: 96.8%; Average loss: 0.6710\n",
            "Iteration: 31959; Percent complete: 96.8%; Average loss: 0.6399\n",
            "Iteration: 31960; Percent complete: 96.8%; Average loss: 0.5884\n",
            "Iteration: 31961; Percent complete: 96.9%; Average loss: 0.7343\n",
            "Iteration: 31962; Percent complete: 96.9%; Average loss: 0.6506\n",
            "Iteration: 31963; Percent complete: 96.9%; Average loss: 0.6220\n",
            "Iteration: 31964; Percent complete: 96.9%; Average loss: 0.7297\n",
            "Iteration: 31965; Percent complete: 96.9%; Average loss: 0.5841\n",
            "Iteration: 31966; Percent complete: 96.9%; Average loss: 0.5662\n",
            "Iteration: 31967; Percent complete: 96.9%; Average loss: 0.6443\n",
            "Iteration: 31968; Percent complete: 96.9%; Average loss: 0.6388\n",
            "Iteration: 31969; Percent complete: 96.9%; Average loss: 0.8070\n",
            "Iteration: 31970; Percent complete: 96.9%; Average loss: 0.5994\n",
            "Iteration: 31971; Percent complete: 96.9%; Average loss: 0.6817\n",
            "Iteration: 31972; Percent complete: 96.9%; Average loss: 0.6511\n",
            "Iteration: 31973; Percent complete: 96.9%; Average loss: 0.8197\n",
            "Iteration: 31974; Percent complete: 96.9%; Average loss: 0.5899\n",
            "Iteration: 31975; Percent complete: 96.9%; Average loss: 0.6152\n",
            "Iteration: 31976; Percent complete: 96.9%; Average loss: 0.7425\n",
            "Iteration: 31977; Percent complete: 96.9%; Average loss: 0.5933\n",
            "Iteration: 31978; Percent complete: 96.9%; Average loss: 0.6040\n",
            "Iteration: 31979; Percent complete: 96.9%; Average loss: 0.7056\n",
            "Iteration: 31980; Percent complete: 96.9%; Average loss: 0.5611\n",
            "Iteration: 31981; Percent complete: 96.9%; Average loss: 0.6468\n",
            "Iteration: 31982; Percent complete: 96.9%; Average loss: 0.5983\n",
            "Iteration: 31983; Percent complete: 96.9%; Average loss: 0.8756\n",
            "Iteration: 31984; Percent complete: 96.9%; Average loss: 0.7217\n",
            "Iteration: 31985; Percent complete: 96.9%; Average loss: 0.6581\n",
            "Iteration: 31986; Percent complete: 96.9%; Average loss: 0.8844\n",
            "Iteration: 31987; Percent complete: 96.9%; Average loss: 0.5768\n",
            "Iteration: 31988; Percent complete: 96.9%; Average loss: 0.5893\n",
            "Iteration: 31989; Percent complete: 96.9%; Average loss: 0.7672\n",
            "Iteration: 31990; Percent complete: 96.9%; Average loss: 0.7071\n",
            "Iteration: 31991; Percent complete: 96.9%; Average loss: 0.6672\n",
            "Iteration: 31992; Percent complete: 96.9%; Average loss: 0.6608\n",
            "Iteration: 31993; Percent complete: 96.9%; Average loss: 0.6515\n",
            "Iteration: 31994; Percent complete: 97.0%; Average loss: 0.7902\n",
            "Iteration: 31995; Percent complete: 97.0%; Average loss: 0.5833\n",
            "Iteration: 31996; Percent complete: 97.0%; Average loss: 0.7357\n",
            "Iteration: 31997; Percent complete: 97.0%; Average loss: 0.6799\n",
            "Iteration: 31998; Percent complete: 97.0%; Average loss: 0.6865\n",
            "Iteration: 31999; Percent complete: 97.0%; Average loss: 0.8579\n",
            "Iteration: 32000; Percent complete: 97.0%; Average loss: 0.6672\n",
            "Iteration: 32001; Percent complete: 97.0%; Average loss: 0.5635\n",
            "Iteration: 32002; Percent complete: 97.0%; Average loss: 0.6321\n",
            "Iteration: 32003; Percent complete: 97.0%; Average loss: 0.5813\n",
            "Iteration: 32004; Percent complete: 97.0%; Average loss: 0.7397\n",
            "Iteration: 32005; Percent complete: 97.0%; Average loss: 0.7253\n",
            "Iteration: 32006; Percent complete: 97.0%; Average loss: 0.5469\n",
            "Iteration: 32007; Percent complete: 97.0%; Average loss: 0.8530\n",
            "Iteration: 32008; Percent complete: 97.0%; Average loss: 0.6905\n",
            "Iteration: 32009; Percent complete: 97.0%; Average loss: 0.6199\n",
            "Iteration: 32010; Percent complete: 97.0%; Average loss: 0.6157\n",
            "Iteration: 32011; Percent complete: 97.0%; Average loss: 0.7478\n",
            "Iteration: 32012; Percent complete: 97.0%; Average loss: 0.6636\n",
            "Iteration: 32013; Percent complete: 97.0%; Average loss: 0.8395\n",
            "Iteration: 32014; Percent complete: 97.0%; Average loss: 0.8705\n",
            "Iteration: 32015; Percent complete: 97.0%; Average loss: 0.7482\n",
            "Iteration: 32016; Percent complete: 97.0%; Average loss: 0.6522\n",
            "Iteration: 32017; Percent complete: 97.0%; Average loss: 0.6663\n",
            "Iteration: 32018; Percent complete: 97.0%; Average loss: 0.5504\n",
            "Iteration: 32019; Percent complete: 97.0%; Average loss: 0.5685\n",
            "Iteration: 32020; Percent complete: 97.0%; Average loss: 0.6311\n",
            "Iteration: 32021; Percent complete: 97.0%; Average loss: 0.8297\n",
            "Iteration: 32022; Percent complete: 97.0%; Average loss: 0.6696\n",
            "Iteration: 32023; Percent complete: 97.0%; Average loss: 0.6916\n",
            "Iteration: 32024; Percent complete: 97.0%; Average loss: 0.6038\n",
            "Iteration: 32025; Percent complete: 97.0%; Average loss: 0.6332\n",
            "Iteration: 32026; Percent complete: 97.0%; Average loss: 0.8338\n",
            "Iteration: 32027; Percent complete: 97.1%; Average loss: 0.6564\n",
            "Iteration: 32028; Percent complete: 97.1%; Average loss: 0.5569\n",
            "Iteration: 32029; Percent complete: 97.1%; Average loss: 0.5366\n",
            "Iteration: 32030; Percent complete: 97.1%; Average loss: 0.6609\n",
            "Iteration: 32031; Percent complete: 97.1%; Average loss: 0.7188\n",
            "Iteration: 32032; Percent complete: 97.1%; Average loss: 0.6333\n",
            "Iteration: 32033; Percent complete: 97.1%; Average loss: 0.9600\n",
            "Iteration: 32034; Percent complete: 97.1%; Average loss: 0.7192\n",
            "Iteration: 32035; Percent complete: 97.1%; Average loss: 0.7779\n",
            "Iteration: 32036; Percent complete: 97.1%; Average loss: 0.6009\n",
            "Iteration: 32037; Percent complete: 97.1%; Average loss: 0.7213\n",
            "Iteration: 32038; Percent complete: 97.1%; Average loss: 0.7507\n",
            "Iteration: 32039; Percent complete: 97.1%; Average loss: 0.5876\n",
            "Iteration: 32040; Percent complete: 97.1%; Average loss: 0.6767\n",
            "Iteration: 32041; Percent complete: 97.1%; Average loss: 0.7926\n",
            "Iteration: 32042; Percent complete: 97.1%; Average loss: 0.5647\n",
            "Iteration: 32043; Percent complete: 97.1%; Average loss: 0.8023\n",
            "Iteration: 32044; Percent complete: 97.1%; Average loss: 0.7087\n",
            "Iteration: 32045; Percent complete: 97.1%; Average loss: 0.6549\n",
            "Iteration: 32046; Percent complete: 97.1%; Average loss: 0.6998\n",
            "Iteration: 32047; Percent complete: 97.1%; Average loss: 0.8320\n",
            "Iteration: 32048; Percent complete: 97.1%; Average loss: 0.7307\n",
            "Iteration: 32049; Percent complete: 97.1%; Average loss: 0.7636\n",
            "Iteration: 32050; Percent complete: 97.1%; Average loss: 0.6701\n",
            "Iteration: 32051; Percent complete: 97.1%; Average loss: 0.6953\n",
            "Iteration: 32052; Percent complete: 97.1%; Average loss: 0.6290\n",
            "Iteration: 32053; Percent complete: 97.1%; Average loss: 0.6804\n",
            "Iteration: 32054; Percent complete: 97.1%; Average loss: 0.7346\n",
            "Iteration: 32055; Percent complete: 97.1%; Average loss: 0.6744\n",
            "Iteration: 32056; Percent complete: 97.1%; Average loss: 0.6809\n",
            "Iteration: 32057; Percent complete: 97.1%; Average loss: 0.5248\n",
            "Iteration: 32058; Percent complete: 97.1%; Average loss: 0.7211\n",
            "Iteration: 32059; Percent complete: 97.1%; Average loss: 0.6470\n",
            "Iteration: 32060; Percent complete: 97.2%; Average loss: 0.5750\n",
            "Iteration: 32061; Percent complete: 97.2%; Average loss: 0.6835\n",
            "Iteration: 32062; Percent complete: 97.2%; Average loss: 0.6645\n",
            "Iteration: 32063; Percent complete: 97.2%; Average loss: 0.6778\n",
            "Iteration: 32064; Percent complete: 97.2%; Average loss: 0.6816\n",
            "Iteration: 32065; Percent complete: 97.2%; Average loss: 0.5229\n",
            "Iteration: 32066; Percent complete: 97.2%; Average loss: 0.6932\n",
            "Iteration: 32067; Percent complete: 97.2%; Average loss: 0.6532\n",
            "Iteration: 32068; Percent complete: 97.2%; Average loss: 0.6298\n",
            "Iteration: 32069; Percent complete: 97.2%; Average loss: 0.6916\n",
            "Iteration: 32070; Percent complete: 97.2%; Average loss: 0.5789\n",
            "Iteration: 32071; Percent complete: 97.2%; Average loss: 0.7408\n",
            "Iteration: 32072; Percent complete: 97.2%; Average loss: 0.7593\n",
            "Iteration: 32073; Percent complete: 97.2%; Average loss: 0.7489\n",
            "Iteration: 32074; Percent complete: 97.2%; Average loss: 0.7808\n",
            "Iteration: 32075; Percent complete: 97.2%; Average loss: 0.7051\n",
            "Iteration: 32076; Percent complete: 97.2%; Average loss: 0.6206\n",
            "Iteration: 32077; Percent complete: 97.2%; Average loss: 0.6663\n",
            "Iteration: 32078; Percent complete: 97.2%; Average loss: 0.7501\n",
            "Iteration: 32079; Percent complete: 97.2%; Average loss: 0.7713\n",
            "Iteration: 32080; Percent complete: 97.2%; Average loss: 0.7781\n",
            "Iteration: 32081; Percent complete: 97.2%; Average loss: 0.8273\n",
            "Iteration: 32082; Percent complete: 97.2%; Average loss: 0.7875\n",
            "Iteration: 32083; Percent complete: 97.2%; Average loss: 0.6777\n",
            "Iteration: 32084; Percent complete: 97.2%; Average loss: 0.5535\n",
            "Iteration: 32085; Percent complete: 97.2%; Average loss: 0.8029\n",
            "Iteration: 32086; Percent complete: 97.2%; Average loss: 0.5397\n",
            "Iteration: 32087; Percent complete: 97.2%; Average loss: 0.6228\n",
            "Iteration: 32088; Percent complete: 97.2%; Average loss: 0.6090\n",
            "Iteration: 32089; Percent complete: 97.2%; Average loss: 0.5431\n",
            "Iteration: 32090; Percent complete: 97.2%; Average loss: 0.5669\n",
            "Iteration: 32091; Percent complete: 97.2%; Average loss: 0.5776\n",
            "Iteration: 32092; Percent complete: 97.2%; Average loss: 0.8081\n",
            "Iteration: 32093; Percent complete: 97.3%; Average loss: 0.5988\n",
            "Iteration: 32094; Percent complete: 97.3%; Average loss: 0.6942\n",
            "Iteration: 32095; Percent complete: 97.3%; Average loss: 0.5953\n",
            "Iteration: 32096; Percent complete: 97.3%; Average loss: 0.5402\n",
            "Iteration: 32097; Percent complete: 97.3%; Average loss: 0.7392\n",
            "Iteration: 32098; Percent complete: 97.3%; Average loss: 0.7695\n",
            "Iteration: 32099; Percent complete: 97.3%; Average loss: 0.7108\n",
            "Iteration: 32100; Percent complete: 97.3%; Average loss: 0.6375\n",
            "Iteration: 32101; Percent complete: 97.3%; Average loss: 0.6418\n",
            "Iteration: 32102; Percent complete: 97.3%; Average loss: 0.6446\n",
            "Iteration: 32103; Percent complete: 97.3%; Average loss: 0.8099\n",
            "Iteration: 32104; Percent complete: 97.3%; Average loss: 0.6097\n",
            "Iteration: 32105; Percent complete: 97.3%; Average loss: 0.6688\n",
            "Iteration: 32106; Percent complete: 97.3%; Average loss: 0.5156\n",
            "Iteration: 32107; Percent complete: 97.3%; Average loss: 0.6982\n",
            "Iteration: 32108; Percent complete: 97.3%; Average loss: 0.7094\n",
            "Iteration: 32109; Percent complete: 97.3%; Average loss: 0.6881\n",
            "Iteration: 32110; Percent complete: 97.3%; Average loss: 0.6175\n",
            "Iteration: 32111; Percent complete: 97.3%; Average loss: 0.8407\n",
            "Iteration: 32112; Percent complete: 97.3%; Average loss: 0.6488\n",
            "Iteration: 32113; Percent complete: 97.3%; Average loss: 0.6935\n",
            "Iteration: 32114; Percent complete: 97.3%; Average loss: 0.7095\n",
            "Iteration: 32115; Percent complete: 97.3%; Average loss: 0.8208\n",
            "Iteration: 32116; Percent complete: 97.3%; Average loss: 0.5477\n",
            "Iteration: 32117; Percent complete: 97.3%; Average loss: 0.6783\n",
            "Iteration: 32118; Percent complete: 97.3%; Average loss: 0.6516\n",
            "Iteration: 32119; Percent complete: 97.3%; Average loss: 0.7898\n",
            "Iteration: 32120; Percent complete: 97.3%; Average loss: 0.8039\n",
            "Iteration: 32121; Percent complete: 97.3%; Average loss: 0.5995\n",
            "Iteration: 32122; Percent complete: 97.3%; Average loss: 0.6505\n",
            "Iteration: 32123; Percent complete: 97.3%; Average loss: 0.6514\n",
            "Iteration: 32124; Percent complete: 97.3%; Average loss: 0.7440\n",
            "Iteration: 32125; Percent complete: 97.3%; Average loss: 0.7352\n",
            "Iteration: 32126; Percent complete: 97.4%; Average loss: 0.5552\n",
            "Iteration: 32127; Percent complete: 97.4%; Average loss: 0.7530\n",
            "Iteration: 32128; Percent complete: 97.4%; Average loss: 0.5945\n",
            "Iteration: 32129; Percent complete: 97.4%; Average loss: 0.7403\n",
            "Iteration: 32130; Percent complete: 97.4%; Average loss: 0.7008\n",
            "Iteration: 32131; Percent complete: 97.4%; Average loss: 0.6162\n",
            "Iteration: 32132; Percent complete: 97.4%; Average loss: 0.7984\n",
            "Iteration: 32133; Percent complete: 97.4%; Average loss: 0.5648\n",
            "Iteration: 32134; Percent complete: 97.4%; Average loss: 0.6649\n",
            "Iteration: 32135; Percent complete: 97.4%; Average loss: 0.7697\n",
            "Iteration: 32136; Percent complete: 97.4%; Average loss: 0.7650\n",
            "Iteration: 32137; Percent complete: 97.4%; Average loss: 0.6949\n",
            "Iteration: 32138; Percent complete: 97.4%; Average loss: 0.8375\n",
            "Iteration: 32139; Percent complete: 97.4%; Average loss: 0.6569\n",
            "Iteration: 32140; Percent complete: 97.4%; Average loss: 0.6977\n",
            "Iteration: 32141; Percent complete: 97.4%; Average loss: 0.6089\n",
            "Iteration: 32142; Percent complete: 97.4%; Average loss: 0.6476\n",
            "Iteration: 32143; Percent complete: 97.4%; Average loss: 0.6745\n",
            "Iteration: 32144; Percent complete: 97.4%; Average loss: 0.6667\n",
            "Iteration: 32145; Percent complete: 97.4%; Average loss: 0.8204\n",
            "Iteration: 32146; Percent complete: 97.4%; Average loss: 0.6490\n",
            "Iteration: 32147; Percent complete: 97.4%; Average loss: 0.7593\n",
            "Iteration: 32148; Percent complete: 97.4%; Average loss: 0.5446\n",
            "Iteration: 32149; Percent complete: 97.4%; Average loss: 0.7469\n",
            "Iteration: 32150; Percent complete: 97.4%; Average loss: 0.6994\n",
            "Iteration: 32151; Percent complete: 97.4%; Average loss: 0.6565\n",
            "Iteration: 32152; Percent complete: 97.4%; Average loss: 0.6807\n",
            "Iteration: 32153; Percent complete: 97.4%; Average loss: 0.8136\n",
            "Iteration: 32154; Percent complete: 97.4%; Average loss: 0.6138\n",
            "Iteration: 32155; Percent complete: 97.4%; Average loss: 0.7188\n",
            "Iteration: 32156; Percent complete: 97.4%; Average loss: 0.6933\n",
            "Iteration: 32157; Percent complete: 97.4%; Average loss: 0.5344\n",
            "Iteration: 32158; Percent complete: 97.4%; Average loss: 0.7837\n",
            "Iteration: 32159; Percent complete: 97.5%; Average loss: 0.8385\n",
            "Iteration: 32160; Percent complete: 97.5%; Average loss: 0.6091\n",
            "Iteration: 32161; Percent complete: 97.5%; Average loss: 0.7038\n",
            "Iteration: 32162; Percent complete: 97.5%; Average loss: 0.6651\n",
            "Iteration: 32163; Percent complete: 97.5%; Average loss: 0.7229\n",
            "Iteration: 32164; Percent complete: 97.5%; Average loss: 0.7659\n",
            "Iteration: 32165; Percent complete: 97.5%; Average loss: 0.8755\n",
            "Iteration: 32166; Percent complete: 97.5%; Average loss: 0.7680\n",
            "Iteration: 32167; Percent complete: 97.5%; Average loss: 0.5990\n",
            "Iteration: 32168; Percent complete: 97.5%; Average loss: 0.7086\n",
            "Iteration: 32169; Percent complete: 97.5%; Average loss: 0.6479\n",
            "Iteration: 32170; Percent complete: 97.5%; Average loss: 0.5994\n",
            "Iteration: 32171; Percent complete: 97.5%; Average loss: 0.7939\n",
            "Iteration: 32172; Percent complete: 97.5%; Average loss: 0.5759\n",
            "Iteration: 32173; Percent complete: 97.5%; Average loss: 0.7389\n",
            "Iteration: 32174; Percent complete: 97.5%; Average loss: 0.4366\n",
            "Iteration: 32175; Percent complete: 97.5%; Average loss: 0.5994\n",
            "Iteration: 32176; Percent complete: 97.5%; Average loss: 0.5344\n",
            "Iteration: 32177; Percent complete: 97.5%; Average loss: 0.6399\n",
            "Iteration: 32178; Percent complete: 97.5%; Average loss: 0.5819\n",
            "Iteration: 32179; Percent complete: 97.5%; Average loss: 0.6520\n",
            "Iteration: 32180; Percent complete: 97.5%; Average loss: 0.7075\n",
            "Iteration: 32181; Percent complete: 97.5%; Average loss: 0.6225\n",
            "Iteration: 32182; Percent complete: 97.5%; Average loss: 0.6351\n",
            "Iteration: 32183; Percent complete: 97.5%; Average loss: 0.6708\n",
            "Iteration: 32184; Percent complete: 97.5%; Average loss: 0.7076\n",
            "Iteration: 32185; Percent complete: 97.5%; Average loss: 0.6144\n",
            "Iteration: 32186; Percent complete: 97.5%; Average loss: 0.4945\n",
            "Iteration: 32187; Percent complete: 97.5%; Average loss: 0.6239\n",
            "Iteration: 32188; Percent complete: 97.5%; Average loss: 0.7057\n",
            "Iteration: 32189; Percent complete: 97.5%; Average loss: 0.8202\n",
            "Iteration: 32190; Percent complete: 97.5%; Average loss: 0.8520\n",
            "Iteration: 32191; Percent complete: 97.5%; Average loss: 0.6980\n",
            "Iteration: 32192; Percent complete: 97.6%; Average loss: 0.6754\n",
            "Iteration: 32193; Percent complete: 97.6%; Average loss: 0.8450\n",
            "Iteration: 32194; Percent complete: 97.6%; Average loss: 0.7746\n",
            "Iteration: 32195; Percent complete: 97.6%; Average loss: 0.6705\n",
            "Iteration: 32196; Percent complete: 97.6%; Average loss: 0.5586\n",
            "Iteration: 32197; Percent complete: 97.6%; Average loss: 0.6686\n",
            "Iteration: 32198; Percent complete: 97.6%; Average loss: 0.8087\n",
            "Iteration: 32199; Percent complete: 97.6%; Average loss: 0.6062\n",
            "Iteration: 32200; Percent complete: 97.6%; Average loss: 0.7428\n",
            "Iteration: 32201; Percent complete: 97.6%; Average loss: 0.6248\n",
            "Iteration: 32202; Percent complete: 97.6%; Average loss: 0.7263\n",
            "Iteration: 32203; Percent complete: 97.6%; Average loss: 0.5931\n",
            "Iteration: 32204; Percent complete: 97.6%; Average loss: 0.6222\n",
            "Iteration: 32205; Percent complete: 97.6%; Average loss: 0.8035\n",
            "Iteration: 32206; Percent complete: 97.6%; Average loss: 0.8585\n",
            "Iteration: 32207; Percent complete: 97.6%; Average loss: 0.6548\n",
            "Iteration: 32208; Percent complete: 97.6%; Average loss: 0.6102\n",
            "Iteration: 32209; Percent complete: 97.6%; Average loss: 0.6577\n",
            "Iteration: 32210; Percent complete: 97.6%; Average loss: 0.5964\n",
            "Iteration: 32211; Percent complete: 97.6%; Average loss: 0.5619\n",
            "Iteration: 32212; Percent complete: 97.6%; Average loss: 0.6576\n",
            "Iteration: 32213; Percent complete: 97.6%; Average loss: 0.7713\n",
            "Iteration: 32214; Percent complete: 97.6%; Average loss: 0.7176\n",
            "Iteration: 32215; Percent complete: 97.6%; Average loss: 0.8494\n",
            "Iteration: 32216; Percent complete: 97.6%; Average loss: 0.7029\n",
            "Iteration: 32217; Percent complete: 97.6%; Average loss: 0.8114\n",
            "Iteration: 32218; Percent complete: 97.6%; Average loss: 0.6573\n",
            "Iteration: 32219; Percent complete: 97.6%; Average loss: 0.6642\n",
            "Iteration: 32220; Percent complete: 97.6%; Average loss: 0.5429\n",
            "Iteration: 32221; Percent complete: 97.6%; Average loss: 0.7625\n",
            "Iteration: 32222; Percent complete: 97.6%; Average loss: 0.5805\n",
            "Iteration: 32223; Percent complete: 97.6%; Average loss: 0.5578\n",
            "Iteration: 32224; Percent complete: 97.6%; Average loss: 0.7206\n",
            "Iteration: 32225; Percent complete: 97.7%; Average loss: 0.7849\n",
            "Iteration: 32226; Percent complete: 97.7%; Average loss: 0.7142\n",
            "Iteration: 32227; Percent complete: 97.7%; Average loss: 0.6671\n",
            "Iteration: 32228; Percent complete: 97.7%; Average loss: 0.7285\n",
            "Iteration: 32229; Percent complete: 97.7%; Average loss: 0.6835\n",
            "Iteration: 32230; Percent complete: 97.7%; Average loss: 0.5297\n",
            "Iteration: 32231; Percent complete: 97.7%; Average loss: 0.8017\n",
            "Iteration: 32232; Percent complete: 97.7%; Average loss: 0.6964\n",
            "Iteration: 32233; Percent complete: 97.7%; Average loss: 0.7920\n",
            "Iteration: 32234; Percent complete: 97.7%; Average loss: 0.6604\n",
            "Iteration: 32235; Percent complete: 97.7%; Average loss: 0.7694\n",
            "Iteration: 32236; Percent complete: 97.7%; Average loss: 0.7041\n",
            "Iteration: 32237; Percent complete: 97.7%; Average loss: 0.6501\n",
            "Iteration: 32238; Percent complete: 97.7%; Average loss: 0.7907\n",
            "Iteration: 32239; Percent complete: 97.7%; Average loss: 0.6282\n",
            "Iteration: 32240; Percent complete: 97.7%; Average loss: 0.7212\n",
            "Iteration: 32241; Percent complete: 97.7%; Average loss: 0.5092\n",
            "Iteration: 32242; Percent complete: 97.7%; Average loss: 0.6209\n",
            "Iteration: 32243; Percent complete: 97.7%; Average loss: 0.8604\n",
            "Iteration: 32244; Percent complete: 97.7%; Average loss: 0.7075\n",
            "Iteration: 32245; Percent complete: 97.7%; Average loss: 0.6669\n",
            "Iteration: 32246; Percent complete: 97.7%; Average loss: 0.6972\n",
            "Iteration: 32247; Percent complete: 97.7%; Average loss: 0.7889\n",
            "Iteration: 32248; Percent complete: 97.7%; Average loss: 0.9060\n",
            "Iteration: 32249; Percent complete: 97.7%; Average loss: 0.8054\n",
            "Iteration: 32250; Percent complete: 97.7%; Average loss: 0.7736\n",
            "Iteration: 32251; Percent complete: 97.7%; Average loss: 0.7246\n",
            "Iteration: 32252; Percent complete: 97.7%; Average loss: 0.7111\n",
            "Iteration: 32253; Percent complete: 97.7%; Average loss: 0.7035\n",
            "Iteration: 32254; Percent complete: 97.7%; Average loss: 0.6598\n",
            "Iteration: 32255; Percent complete: 97.7%; Average loss: 0.6549\n",
            "Iteration: 32256; Percent complete: 97.7%; Average loss: 0.6112\n",
            "Iteration: 32257; Percent complete: 97.7%; Average loss: 0.7598\n",
            "Iteration: 32258; Percent complete: 97.8%; Average loss: 0.6501\n",
            "Iteration: 32259; Percent complete: 97.8%; Average loss: 0.6359\n",
            "Iteration: 32260; Percent complete: 97.8%; Average loss: 0.6782\n",
            "Iteration: 32261; Percent complete: 97.8%; Average loss: 0.5987\n",
            "Iteration: 32262; Percent complete: 97.8%; Average loss: 0.6584\n",
            "Iteration: 32263; Percent complete: 97.8%; Average loss: 0.5815\n",
            "Iteration: 32264; Percent complete: 97.8%; Average loss: 0.6724\n",
            "Iteration: 32265; Percent complete: 97.8%; Average loss: 0.7065\n",
            "Iteration: 32266; Percent complete: 97.8%; Average loss: 0.6691\n",
            "Iteration: 32267; Percent complete: 97.8%; Average loss: 0.6893\n",
            "Iteration: 32268; Percent complete: 97.8%; Average loss: 0.5768\n",
            "Iteration: 32269; Percent complete: 97.8%; Average loss: 0.8878\n",
            "Iteration: 32270; Percent complete: 97.8%; Average loss: 0.7244\n",
            "Iteration: 32271; Percent complete: 97.8%; Average loss: 0.6066\n",
            "Iteration: 32272; Percent complete: 97.8%; Average loss: 0.7953\n",
            "Iteration: 32273; Percent complete: 97.8%; Average loss: 0.7961\n",
            "Iteration: 32274; Percent complete: 97.8%; Average loss: 0.6077\n",
            "Iteration: 32275; Percent complete: 97.8%; Average loss: 0.6882\n",
            "Iteration: 32276; Percent complete: 97.8%; Average loss: 0.6040\n",
            "Iteration: 32277; Percent complete: 97.8%; Average loss: 0.7036\n",
            "Iteration: 32278; Percent complete: 97.8%; Average loss: 0.5840\n",
            "Iteration: 32279; Percent complete: 97.8%; Average loss: 0.5491\n",
            "Iteration: 32280; Percent complete: 97.8%; Average loss: 0.5701\n",
            "Iteration: 32281; Percent complete: 97.8%; Average loss: 0.7660\n",
            "Iteration: 32282; Percent complete: 97.8%; Average loss: 0.5944\n",
            "Iteration: 32283; Percent complete: 97.8%; Average loss: 0.6353\n",
            "Iteration: 32284; Percent complete: 97.8%; Average loss: 0.6527\n",
            "Iteration: 32285; Percent complete: 97.8%; Average loss: 0.6292\n",
            "Iteration: 32286; Percent complete: 97.8%; Average loss: 0.5960\n",
            "Iteration: 32287; Percent complete: 97.8%; Average loss: 0.7035\n",
            "Iteration: 32288; Percent complete: 97.8%; Average loss: 0.6125\n",
            "Iteration: 32289; Percent complete: 97.8%; Average loss: 0.5497\n",
            "Iteration: 32290; Percent complete: 97.8%; Average loss: 0.6413\n",
            "Iteration: 32291; Percent complete: 97.9%; Average loss: 0.6340\n",
            "Iteration: 32292; Percent complete: 97.9%; Average loss: 0.7339\n",
            "Iteration: 32293; Percent complete: 97.9%; Average loss: 0.6101\n",
            "Iteration: 32294; Percent complete: 97.9%; Average loss: 0.6357\n",
            "Iteration: 32295; Percent complete: 97.9%; Average loss: 0.7108\n",
            "Iteration: 32296; Percent complete: 97.9%; Average loss: 0.6284\n",
            "Iteration: 32297; Percent complete: 97.9%; Average loss: 0.7186\n",
            "Iteration: 32298; Percent complete: 97.9%; Average loss: 0.5865\n",
            "Iteration: 32299; Percent complete: 97.9%; Average loss: 0.6916\n",
            "Iteration: 32300; Percent complete: 97.9%; Average loss: 0.7221\n",
            "Iteration: 32301; Percent complete: 97.9%; Average loss: 0.8156\n",
            "Iteration: 32302; Percent complete: 97.9%; Average loss: 0.6045\n",
            "Iteration: 32303; Percent complete: 97.9%; Average loss: 0.6288\n",
            "Iteration: 32304; Percent complete: 97.9%; Average loss: 0.6507\n",
            "Iteration: 32305; Percent complete: 97.9%; Average loss: 0.8588\n",
            "Iteration: 32306; Percent complete: 97.9%; Average loss: 0.7117\n",
            "Iteration: 32307; Percent complete: 97.9%; Average loss: 0.6020\n",
            "Iteration: 32308; Percent complete: 97.9%; Average loss: 0.5852\n",
            "Iteration: 32309; Percent complete: 97.9%; Average loss: 0.7877\n",
            "Iteration: 32310; Percent complete: 97.9%; Average loss: 0.7658\n",
            "Iteration: 32311; Percent complete: 97.9%; Average loss: 0.6724\n",
            "Iteration: 32312; Percent complete: 97.9%; Average loss: 0.7101\n",
            "Iteration: 32313; Percent complete: 97.9%; Average loss: 0.6635\n",
            "Iteration: 32314; Percent complete: 97.9%; Average loss: 0.7347\n",
            "Iteration: 32315; Percent complete: 97.9%; Average loss: 0.7011\n",
            "Iteration: 32316; Percent complete: 97.9%; Average loss: 0.8243\n",
            "Iteration: 32317; Percent complete: 97.9%; Average loss: 0.7693\n",
            "Iteration: 32318; Percent complete: 97.9%; Average loss: 0.5465\n",
            "Iteration: 32319; Percent complete: 97.9%; Average loss: 0.7367\n",
            "Iteration: 32320; Percent complete: 97.9%; Average loss: 0.6394\n",
            "Iteration: 32321; Percent complete: 97.9%; Average loss: 0.5725\n",
            "Iteration: 32322; Percent complete: 97.9%; Average loss: 0.7881\n",
            "Iteration: 32323; Percent complete: 97.9%; Average loss: 0.6545\n",
            "Iteration: 32324; Percent complete: 98.0%; Average loss: 0.6883\n",
            "Iteration: 32325; Percent complete: 98.0%; Average loss: 0.7032\n",
            "Iteration: 32326; Percent complete: 98.0%; Average loss: 0.5917\n",
            "Iteration: 32327; Percent complete: 98.0%; Average loss: 0.5908\n",
            "Iteration: 32328; Percent complete: 98.0%; Average loss: 0.6620\n",
            "Iteration: 32329; Percent complete: 98.0%; Average loss: 0.6973\n",
            "Iteration: 32330; Percent complete: 98.0%; Average loss: 0.7084\n",
            "Iteration: 32331; Percent complete: 98.0%; Average loss: 0.6452\n",
            "Iteration: 32332; Percent complete: 98.0%; Average loss: 0.8256\n",
            "Iteration: 32333; Percent complete: 98.0%; Average loss: 0.7177\n",
            "Iteration: 32334; Percent complete: 98.0%; Average loss: 0.6434\n",
            "Iteration: 32335; Percent complete: 98.0%; Average loss: 0.7738\n",
            "Iteration: 32336; Percent complete: 98.0%; Average loss: 0.6335\n",
            "Iteration: 32337; Percent complete: 98.0%; Average loss: 0.7837\n",
            "Iteration: 32338; Percent complete: 98.0%; Average loss: 0.7236\n",
            "Iteration: 32339; Percent complete: 98.0%; Average loss: 0.7022\n",
            "Iteration: 32340; Percent complete: 98.0%; Average loss: 0.5143\n",
            "Iteration: 32341; Percent complete: 98.0%; Average loss: 0.7350\n",
            "Iteration: 32342; Percent complete: 98.0%; Average loss: 0.6049\n",
            "Iteration: 32343; Percent complete: 98.0%; Average loss: 0.6869\n",
            "Iteration: 32344; Percent complete: 98.0%; Average loss: 0.8206\n",
            "Iteration: 32345; Percent complete: 98.0%; Average loss: 0.6693\n",
            "Iteration: 32346; Percent complete: 98.0%; Average loss: 0.7943\n",
            "Iteration: 32347; Percent complete: 98.0%; Average loss: 0.5155\n",
            "Iteration: 32348; Percent complete: 98.0%; Average loss: 0.5649\n",
            "Iteration: 32349; Percent complete: 98.0%; Average loss: 0.6056\n",
            "Iteration: 32350; Percent complete: 98.0%; Average loss: 0.6803\n",
            "Iteration: 32351; Percent complete: 98.0%; Average loss: 0.7513\n",
            "Iteration: 32352; Percent complete: 98.0%; Average loss: 0.6052\n",
            "Iteration: 32353; Percent complete: 98.0%; Average loss: 0.7568\n",
            "Iteration: 32354; Percent complete: 98.0%; Average loss: 0.6327\n",
            "Iteration: 32355; Percent complete: 98.0%; Average loss: 0.6234\n",
            "Iteration: 32356; Percent complete: 98.0%; Average loss: 0.6312\n",
            "Iteration: 32357; Percent complete: 98.1%; Average loss: 0.6558\n",
            "Iteration: 32358; Percent complete: 98.1%; Average loss: 0.6099\n",
            "Iteration: 32359; Percent complete: 98.1%; Average loss: 0.6629\n",
            "Iteration: 32360; Percent complete: 98.1%; Average loss: 0.7219\n",
            "Iteration: 32361; Percent complete: 98.1%; Average loss: 0.7473\n",
            "Iteration: 32362; Percent complete: 98.1%; Average loss: 0.5819\n",
            "Iteration: 32363; Percent complete: 98.1%; Average loss: 0.6020\n",
            "Iteration: 32364; Percent complete: 98.1%; Average loss: 0.6058\n",
            "Iteration: 32365; Percent complete: 98.1%; Average loss: 0.6825\n",
            "Iteration: 32366; Percent complete: 98.1%; Average loss: 0.8726\n",
            "Iteration: 32367; Percent complete: 98.1%; Average loss: 0.8086\n",
            "Iteration: 32368; Percent complete: 98.1%; Average loss: 0.7009\n",
            "Iteration: 32369; Percent complete: 98.1%; Average loss: 0.5576\n",
            "Iteration: 32370; Percent complete: 98.1%; Average loss: 0.6258\n",
            "Iteration: 32371; Percent complete: 98.1%; Average loss: 0.6280\n",
            "Iteration: 32372; Percent complete: 98.1%; Average loss: 0.7044\n",
            "Iteration: 32373; Percent complete: 98.1%; Average loss: 0.5986\n",
            "Iteration: 32374; Percent complete: 98.1%; Average loss: 0.6591\n",
            "Iteration: 32375; Percent complete: 98.1%; Average loss: 0.5351\n",
            "Iteration: 32376; Percent complete: 98.1%; Average loss: 0.5845\n",
            "Iteration: 32377; Percent complete: 98.1%; Average loss: 0.6298\n",
            "Iteration: 32378; Percent complete: 98.1%; Average loss: 0.5024\n",
            "Iteration: 32379; Percent complete: 98.1%; Average loss: 0.6830\n",
            "Iteration: 32380; Percent complete: 98.1%; Average loss: 0.7883\n",
            "Iteration: 32381; Percent complete: 98.1%; Average loss: 0.6097\n",
            "Iteration: 32382; Percent complete: 98.1%; Average loss: 0.6547\n",
            "Iteration: 32383; Percent complete: 98.1%; Average loss: 0.7408\n",
            "Iteration: 32384; Percent complete: 98.1%; Average loss: 0.5920\n",
            "Iteration: 32385; Percent complete: 98.1%; Average loss: 0.6206\n",
            "Iteration: 32386; Percent complete: 98.1%; Average loss: 0.7055\n",
            "Iteration: 32387; Percent complete: 98.1%; Average loss: 0.6482\n",
            "Iteration: 32388; Percent complete: 98.1%; Average loss: 0.5998\n",
            "Iteration: 32389; Percent complete: 98.1%; Average loss: 0.8701\n",
            "Iteration: 32390; Percent complete: 98.2%; Average loss: 0.6452\n",
            "Iteration: 32391; Percent complete: 98.2%; Average loss: 0.6590\n",
            "Iteration: 32392; Percent complete: 98.2%; Average loss: 0.6519\n",
            "Iteration: 32393; Percent complete: 98.2%; Average loss: 0.5493\n",
            "Iteration: 32394; Percent complete: 98.2%; Average loss: 0.5514\n",
            "Iteration: 32395; Percent complete: 98.2%; Average loss: 0.6566\n",
            "Iteration: 32396; Percent complete: 98.2%; Average loss: 0.5666\n",
            "Iteration: 32397; Percent complete: 98.2%; Average loss: 0.6316\n",
            "Iteration: 32398; Percent complete: 98.2%; Average loss: 0.5836\n",
            "Iteration: 32399; Percent complete: 98.2%; Average loss: 0.5740\n",
            "Iteration: 32400; Percent complete: 98.2%; Average loss: 0.6076\n",
            "Iteration: 32401; Percent complete: 98.2%; Average loss: 0.8453\n",
            "Iteration: 32402; Percent complete: 98.2%; Average loss: 0.7339\n",
            "Iteration: 32403; Percent complete: 98.2%; Average loss: 0.5463\n",
            "Iteration: 32404; Percent complete: 98.2%; Average loss: 0.5469\n",
            "Iteration: 32405; Percent complete: 98.2%; Average loss: 0.7039\n",
            "Iteration: 32406; Percent complete: 98.2%; Average loss: 0.7811\n",
            "Iteration: 32407; Percent complete: 98.2%; Average loss: 0.5540\n",
            "Iteration: 32408; Percent complete: 98.2%; Average loss: 0.6343\n",
            "Iteration: 32409; Percent complete: 98.2%; Average loss: 0.6769\n",
            "Iteration: 32410; Percent complete: 98.2%; Average loss: 0.5663\n",
            "Iteration: 32411; Percent complete: 98.2%; Average loss: 0.6316\n",
            "Iteration: 32412; Percent complete: 98.2%; Average loss: 0.7170\n",
            "Iteration: 32413; Percent complete: 98.2%; Average loss: 0.7249\n",
            "Iteration: 32414; Percent complete: 98.2%; Average loss: 0.6123\n",
            "Iteration: 32415; Percent complete: 98.2%; Average loss: 0.5872\n",
            "Iteration: 32416; Percent complete: 98.2%; Average loss: 0.5258\n",
            "Iteration: 32417; Percent complete: 98.2%; Average loss: 0.7015\n",
            "Iteration: 32418; Percent complete: 98.2%; Average loss: 0.5881\n",
            "Iteration: 32419; Percent complete: 98.2%; Average loss: 0.7080\n",
            "Iteration: 32420; Percent complete: 98.2%; Average loss: 0.6197\n",
            "Iteration: 32421; Percent complete: 98.2%; Average loss: 0.7862\n",
            "Iteration: 32422; Percent complete: 98.2%; Average loss: 0.5827\n",
            "Iteration: 32423; Percent complete: 98.3%; Average loss: 0.6969\n",
            "Iteration: 32424; Percent complete: 98.3%; Average loss: 0.7407\n",
            "Iteration: 32425; Percent complete: 98.3%; Average loss: 0.6227\n",
            "Iteration: 32426; Percent complete: 98.3%; Average loss: 0.6976\n",
            "Iteration: 32427; Percent complete: 98.3%; Average loss: 0.6124\n",
            "Iteration: 32428; Percent complete: 98.3%; Average loss: 0.7201\n",
            "Iteration: 32429; Percent complete: 98.3%; Average loss: 0.6293\n",
            "Iteration: 32430; Percent complete: 98.3%; Average loss: 0.6897\n",
            "Iteration: 32431; Percent complete: 98.3%; Average loss: 0.5573\n",
            "Iteration: 32432; Percent complete: 98.3%; Average loss: 0.6583\n",
            "Iteration: 32433; Percent complete: 98.3%; Average loss: 0.6803\n",
            "Iteration: 32434; Percent complete: 98.3%; Average loss: 0.7608\n",
            "Iteration: 32435; Percent complete: 98.3%; Average loss: 0.6415\n",
            "Iteration: 32436; Percent complete: 98.3%; Average loss: 0.7010\n",
            "Iteration: 32437; Percent complete: 98.3%; Average loss: 0.7561\n",
            "Iteration: 32438; Percent complete: 98.3%; Average loss: 0.7490\n",
            "Iteration: 32439; Percent complete: 98.3%; Average loss: 0.6122\n",
            "Iteration: 32440; Percent complete: 98.3%; Average loss: 0.6881\n",
            "Iteration: 32441; Percent complete: 98.3%; Average loss: 0.6749\n",
            "Iteration: 32442; Percent complete: 98.3%; Average loss: 0.5587\n",
            "Iteration: 32443; Percent complete: 98.3%; Average loss: 0.6509\n",
            "Iteration: 32444; Percent complete: 98.3%; Average loss: 0.6191\n",
            "Iteration: 32445; Percent complete: 98.3%; Average loss: 0.6321\n",
            "Iteration: 32446; Percent complete: 98.3%; Average loss: 0.7156\n",
            "Iteration: 32447; Percent complete: 98.3%; Average loss: 0.6142\n",
            "Iteration: 32448; Percent complete: 98.3%; Average loss: 0.6031\n",
            "Iteration: 32449; Percent complete: 98.3%; Average loss: 0.6166\n",
            "Iteration: 32450; Percent complete: 98.3%; Average loss: 0.7322\n",
            "Iteration: 32451; Percent complete: 98.3%; Average loss: 0.6992\n",
            "Iteration: 32452; Percent complete: 98.3%; Average loss: 0.7251\n",
            "Iteration: 32453; Percent complete: 98.3%; Average loss: 0.6864\n",
            "Iteration: 32454; Percent complete: 98.3%; Average loss: 0.6172\n",
            "Iteration: 32455; Percent complete: 98.3%; Average loss: 0.6148\n",
            "Iteration: 32456; Percent complete: 98.4%; Average loss: 0.5393\n",
            "Iteration: 32457; Percent complete: 98.4%; Average loss: 0.7608\n",
            "Iteration: 32458; Percent complete: 98.4%; Average loss: 0.7495\n",
            "Iteration: 32459; Percent complete: 98.4%; Average loss: 0.7586\n",
            "Iteration: 32460; Percent complete: 98.4%; Average loss: 0.7213\n",
            "Iteration: 32461; Percent complete: 98.4%; Average loss: 0.7636\n",
            "Iteration: 32462; Percent complete: 98.4%; Average loss: 0.7403\n",
            "Iteration: 32463; Percent complete: 98.4%; Average loss: 1.0015\n",
            "Iteration: 32464; Percent complete: 98.4%; Average loss: 0.6661\n",
            "Iteration: 32465; Percent complete: 98.4%; Average loss: 0.6982\n",
            "Iteration: 32466; Percent complete: 98.4%; Average loss: 0.6351\n",
            "Iteration: 32467; Percent complete: 98.4%; Average loss: 0.7465\n",
            "Iteration: 32468; Percent complete: 98.4%; Average loss: 0.6897\n",
            "Iteration: 32469; Percent complete: 98.4%; Average loss: 0.6326\n",
            "Iteration: 32470; Percent complete: 98.4%; Average loss: 0.6429\n",
            "Iteration: 32471; Percent complete: 98.4%; Average loss: 0.6340\n",
            "Iteration: 32472; Percent complete: 98.4%; Average loss: 0.6304\n",
            "Iteration: 32473; Percent complete: 98.4%; Average loss: 0.5663\n",
            "Iteration: 32474; Percent complete: 98.4%; Average loss: 0.7475\n",
            "Iteration: 32475; Percent complete: 98.4%; Average loss: 0.6108\n",
            "Iteration: 32476; Percent complete: 98.4%; Average loss: 0.8376\n",
            "Iteration: 32477; Percent complete: 98.4%; Average loss: 0.5698\n",
            "Iteration: 32478; Percent complete: 98.4%; Average loss: 0.7552\n",
            "Iteration: 32479; Percent complete: 98.4%; Average loss: 0.8035\n",
            "Iteration: 32480; Percent complete: 98.4%; Average loss: 0.7469\n",
            "Iteration: 32481; Percent complete: 98.4%; Average loss: 0.5484\n",
            "Iteration: 32482; Percent complete: 98.4%; Average loss: 0.6034\n",
            "Iteration: 32483; Percent complete: 98.4%; Average loss: 0.7278\n",
            "Iteration: 32484; Percent complete: 98.4%; Average loss: 0.6053\n",
            "Iteration: 32485; Percent complete: 98.4%; Average loss: 0.7161\n",
            "Iteration: 32486; Percent complete: 98.4%; Average loss: 0.7112\n",
            "Iteration: 32487; Percent complete: 98.4%; Average loss: 0.5692\n",
            "Iteration: 32488; Percent complete: 98.4%; Average loss: 0.5348\n",
            "Iteration: 32489; Percent complete: 98.5%; Average loss: 0.5058\n",
            "Iteration: 32490; Percent complete: 98.5%; Average loss: 0.7201\n",
            "Iteration: 32491; Percent complete: 98.5%; Average loss: 0.7889\n",
            "Iteration: 32492; Percent complete: 98.5%; Average loss: 0.6348\n",
            "Iteration: 32493; Percent complete: 98.5%; Average loss: 0.6981\n",
            "Iteration: 32494; Percent complete: 98.5%; Average loss: 0.5461\n",
            "Iteration: 32495; Percent complete: 98.5%; Average loss: 0.8018\n",
            "Iteration: 32496; Percent complete: 98.5%; Average loss: 0.5744\n",
            "Iteration: 32497; Percent complete: 98.5%; Average loss: 0.5368\n",
            "Iteration: 32498; Percent complete: 98.5%; Average loss: 0.6025\n",
            "Iteration: 32499; Percent complete: 98.5%; Average loss: 0.6506\n",
            "Iteration: 32500; Percent complete: 98.5%; Average loss: 0.6270\n",
            "Iteration: 32501; Percent complete: 98.5%; Average loss: 0.7317\n",
            "Iteration: 32502; Percent complete: 98.5%; Average loss: 0.7175\n",
            "Iteration: 32503; Percent complete: 98.5%; Average loss: 0.7445\n",
            "Iteration: 32504; Percent complete: 98.5%; Average loss: 0.7055\n",
            "Iteration: 32505; Percent complete: 98.5%; Average loss: 0.5743\n",
            "Iteration: 32506; Percent complete: 98.5%; Average loss: 0.6587\n",
            "Iteration: 32507; Percent complete: 98.5%; Average loss: 0.6491\n",
            "Iteration: 32508; Percent complete: 98.5%; Average loss: 0.7463\n",
            "Iteration: 32509; Percent complete: 98.5%; Average loss: 0.6527\n",
            "Iteration: 32510; Percent complete: 98.5%; Average loss: 0.6865\n",
            "Iteration: 32511; Percent complete: 98.5%; Average loss: 0.6212\n",
            "Iteration: 32512; Percent complete: 98.5%; Average loss: 0.5320\n",
            "Iteration: 32513; Percent complete: 98.5%; Average loss: 0.6123\n",
            "Iteration: 32514; Percent complete: 98.5%; Average loss: 0.7104\n",
            "Iteration: 32515; Percent complete: 98.5%; Average loss: 0.6074\n",
            "Iteration: 32516; Percent complete: 98.5%; Average loss: 0.6771\n",
            "Iteration: 32517; Percent complete: 98.5%; Average loss: 0.6585\n",
            "Iteration: 32518; Percent complete: 98.5%; Average loss: 0.6723\n",
            "Iteration: 32519; Percent complete: 98.5%; Average loss: 0.7190\n",
            "Iteration: 32520; Percent complete: 98.5%; Average loss: 0.5345\n",
            "Iteration: 32521; Percent complete: 98.5%; Average loss: 0.6768\n",
            "Iteration: 32522; Percent complete: 98.6%; Average loss: 0.6775\n",
            "Iteration: 32523; Percent complete: 98.6%; Average loss: 0.6630\n",
            "Iteration: 32524; Percent complete: 98.6%; Average loss: 0.7138\n",
            "Iteration: 32525; Percent complete: 98.6%; Average loss: 0.6072\n",
            "Iteration: 32526; Percent complete: 98.6%; Average loss: 0.5927\n",
            "Iteration: 32527; Percent complete: 98.6%; Average loss: 0.6511\n",
            "Iteration: 32528; Percent complete: 98.6%; Average loss: 0.5491\n",
            "Iteration: 32529; Percent complete: 98.6%; Average loss: 0.5888\n",
            "Iteration: 32530; Percent complete: 98.6%; Average loss: 0.8350\n",
            "Iteration: 32531; Percent complete: 98.6%; Average loss: 0.5109\n",
            "Iteration: 32532; Percent complete: 98.6%; Average loss: 0.6686\n",
            "Iteration: 32533; Percent complete: 98.6%; Average loss: 0.6817\n",
            "Iteration: 32534; Percent complete: 98.6%; Average loss: 0.7823\n",
            "Iteration: 32535; Percent complete: 98.6%; Average loss: 0.5628\n",
            "Iteration: 32536; Percent complete: 98.6%; Average loss: 0.5939\n",
            "Iteration: 32537; Percent complete: 98.6%; Average loss: 0.6258\n",
            "Iteration: 32538; Percent complete: 98.6%; Average loss: 0.7377\n",
            "Iteration: 32539; Percent complete: 98.6%; Average loss: 0.6847\n",
            "Iteration: 32540; Percent complete: 98.6%; Average loss: 0.6621\n",
            "Iteration: 32541; Percent complete: 98.6%; Average loss: 0.7500\n",
            "Iteration: 32542; Percent complete: 98.6%; Average loss: 0.7526\n",
            "Iteration: 32543; Percent complete: 98.6%; Average loss: 0.5927\n",
            "Iteration: 32544; Percent complete: 98.6%; Average loss: 0.7275\n",
            "Iteration: 32545; Percent complete: 98.6%; Average loss: 0.7050\n",
            "Iteration: 32546; Percent complete: 98.6%; Average loss: 0.6267\n",
            "Iteration: 32547; Percent complete: 98.6%; Average loss: 0.7794\n",
            "Iteration: 32548; Percent complete: 98.6%; Average loss: 0.5250\n",
            "Iteration: 32549; Percent complete: 98.6%; Average loss: 0.6517\n",
            "Iteration: 32550; Percent complete: 98.6%; Average loss: 0.4951\n",
            "Iteration: 32551; Percent complete: 98.6%; Average loss: 0.6756\n",
            "Iteration: 32552; Percent complete: 98.6%; Average loss: 0.6888\n",
            "Iteration: 32553; Percent complete: 98.6%; Average loss: 0.7029\n",
            "Iteration: 32554; Percent complete: 98.6%; Average loss: 0.5544\n",
            "Iteration: 32555; Percent complete: 98.7%; Average loss: 0.6690\n",
            "Iteration: 32556; Percent complete: 98.7%; Average loss: 0.8154\n",
            "Iteration: 32557; Percent complete: 98.7%; Average loss: 0.6347\n",
            "Iteration: 32558; Percent complete: 98.7%; Average loss: 0.7473\n",
            "Iteration: 32559; Percent complete: 98.7%; Average loss: 0.6650\n",
            "Iteration: 32560; Percent complete: 98.7%; Average loss: 0.6507\n",
            "Iteration: 32561; Percent complete: 98.7%; Average loss: 0.5390\n",
            "Iteration: 32562; Percent complete: 98.7%; Average loss: 0.6327\n",
            "Iteration: 32563; Percent complete: 98.7%; Average loss: 0.8003\n",
            "Iteration: 32564; Percent complete: 98.7%; Average loss: 0.5165\n",
            "Iteration: 32565; Percent complete: 98.7%; Average loss: 0.6377\n",
            "Iteration: 32566; Percent complete: 98.7%; Average loss: 0.7249\n",
            "Iteration: 32567; Percent complete: 98.7%; Average loss: 0.6435\n",
            "Iteration: 32568; Percent complete: 98.7%; Average loss: 0.6026\n",
            "Iteration: 32569; Percent complete: 98.7%; Average loss: 0.6777\n",
            "Iteration: 32570; Percent complete: 98.7%; Average loss: 0.5931\n",
            "Iteration: 32571; Percent complete: 98.7%; Average loss: 0.5645\n",
            "Iteration: 32572; Percent complete: 98.7%; Average loss: 0.5996\n",
            "Iteration: 32573; Percent complete: 98.7%; Average loss: 0.6529\n",
            "Iteration: 32574; Percent complete: 98.7%; Average loss: 0.6732\n",
            "Iteration: 32575; Percent complete: 98.7%; Average loss: 0.7069\n",
            "Iteration: 32576; Percent complete: 98.7%; Average loss: 0.5693\n",
            "Iteration: 32577; Percent complete: 98.7%; Average loss: 0.6774\n",
            "Iteration: 32578; Percent complete: 98.7%; Average loss: 0.6021\n",
            "Iteration: 32579; Percent complete: 98.7%; Average loss: 0.7470\n",
            "Iteration: 32580; Percent complete: 98.7%; Average loss: 0.7308\n",
            "Iteration: 32581; Percent complete: 98.7%; Average loss: 0.5856\n",
            "Iteration: 32582; Percent complete: 98.7%; Average loss: 0.5606\n",
            "Iteration: 32583; Percent complete: 98.7%; Average loss: 0.6037\n",
            "Iteration: 32584; Percent complete: 98.7%; Average loss: 0.6383\n",
            "Iteration: 32585; Percent complete: 98.7%; Average loss: 0.6519\n",
            "Iteration: 32586; Percent complete: 98.7%; Average loss: 0.5301\n",
            "Iteration: 32587; Percent complete: 98.7%; Average loss: 0.6316\n",
            "Iteration: 32588; Percent complete: 98.8%; Average loss: 0.6840\n",
            "Iteration: 32589; Percent complete: 98.8%; Average loss: 0.5992\n",
            "Iteration: 32590; Percent complete: 98.8%; Average loss: 0.6908\n",
            "Iteration: 32591; Percent complete: 98.8%; Average loss: 0.6882\n",
            "Iteration: 32592; Percent complete: 98.8%; Average loss: 0.6294\n",
            "Iteration: 32593; Percent complete: 98.8%; Average loss: 0.6000\n",
            "Iteration: 32594; Percent complete: 98.8%; Average loss: 0.6738\n",
            "Iteration: 32595; Percent complete: 98.8%; Average loss: 0.7684\n",
            "Iteration: 32596; Percent complete: 98.8%; Average loss: 0.5986\n",
            "Iteration: 32597; Percent complete: 98.8%; Average loss: 0.5442\n",
            "Iteration: 32598; Percent complete: 98.8%; Average loss: 0.7079\n",
            "Iteration: 32599; Percent complete: 98.8%; Average loss: 0.7335\n",
            "Iteration: 32600; Percent complete: 98.8%; Average loss: 0.6749\n",
            "Iteration: 32601; Percent complete: 98.8%; Average loss: 0.5960\n",
            "Iteration: 32602; Percent complete: 98.8%; Average loss: 0.7642\n",
            "Iteration: 32603; Percent complete: 98.8%; Average loss: 0.6305\n",
            "Iteration: 32604; Percent complete: 98.8%; Average loss: 0.7674\n",
            "Iteration: 32605; Percent complete: 98.8%; Average loss: 0.5930\n",
            "Iteration: 32606; Percent complete: 98.8%; Average loss: 0.6321\n",
            "Iteration: 32607; Percent complete: 98.8%; Average loss: 0.6586\n",
            "Iteration: 32608; Percent complete: 98.8%; Average loss: 0.9038\n",
            "Iteration: 32609; Percent complete: 98.8%; Average loss: 0.6784\n",
            "Iteration: 32610; Percent complete: 98.8%; Average loss: 0.6442\n",
            "Iteration: 32611; Percent complete: 98.8%; Average loss: 0.5672\n",
            "Iteration: 32612; Percent complete: 98.8%; Average loss: 0.6023\n",
            "Iteration: 32613; Percent complete: 98.8%; Average loss: 0.5554\n",
            "Iteration: 32614; Percent complete: 98.8%; Average loss: 0.5932\n",
            "Iteration: 32615; Percent complete: 98.8%; Average loss: 0.5890\n",
            "Iteration: 32616; Percent complete: 98.8%; Average loss: 0.8136\n",
            "Iteration: 32617; Percent complete: 98.8%; Average loss: 0.5812\n",
            "Iteration: 32618; Percent complete: 98.8%; Average loss: 0.6765\n",
            "Iteration: 32619; Percent complete: 98.8%; Average loss: 0.8483\n",
            "Iteration: 32620; Percent complete: 98.8%; Average loss: 0.6994\n",
            "Iteration: 32621; Percent complete: 98.9%; Average loss: 0.6459\n",
            "Iteration: 32622; Percent complete: 98.9%; Average loss: 0.6094\n",
            "Iteration: 32623; Percent complete: 98.9%; Average loss: 0.6539\n",
            "Iteration: 32624; Percent complete: 98.9%; Average loss: 0.6438\n",
            "Iteration: 32625; Percent complete: 98.9%; Average loss: 0.6654\n",
            "Iteration: 32626; Percent complete: 98.9%; Average loss: 0.6320\n",
            "Iteration: 32627; Percent complete: 98.9%; Average loss: 0.7391\n",
            "Iteration: 32628; Percent complete: 98.9%; Average loss: 0.6797\n",
            "Iteration: 32629; Percent complete: 98.9%; Average loss: 0.5878\n",
            "Iteration: 32630; Percent complete: 98.9%; Average loss: 0.6445\n",
            "Iteration: 32631; Percent complete: 98.9%; Average loss: 0.5699\n",
            "Iteration: 32632; Percent complete: 98.9%; Average loss: 0.7346\n",
            "Iteration: 32633; Percent complete: 98.9%; Average loss: 0.6010\n",
            "Iteration: 32634; Percent complete: 98.9%; Average loss: 0.6278\n",
            "Iteration: 32635; Percent complete: 98.9%; Average loss: 0.7388\n",
            "Iteration: 32636; Percent complete: 98.9%; Average loss: 0.5518\n",
            "Iteration: 32637; Percent complete: 98.9%; Average loss: 0.7945\n",
            "Iteration: 32638; Percent complete: 98.9%; Average loss: 0.7765\n",
            "Iteration: 32639; Percent complete: 98.9%; Average loss: 0.5854\n",
            "Iteration: 32640; Percent complete: 98.9%; Average loss: 0.5070\n",
            "Iteration: 32641; Percent complete: 98.9%; Average loss: 0.6518\n",
            "Iteration: 32642; Percent complete: 98.9%; Average loss: 0.6944\n",
            "Iteration: 32643; Percent complete: 98.9%; Average loss: 0.7604\n",
            "Iteration: 32644; Percent complete: 98.9%; Average loss: 0.7796\n",
            "Iteration: 32645; Percent complete: 98.9%; Average loss: 0.5367\n",
            "Iteration: 32646; Percent complete: 98.9%; Average loss: 0.7043\n",
            "Iteration: 32647; Percent complete: 98.9%; Average loss: 0.5968\n",
            "Iteration: 32648; Percent complete: 98.9%; Average loss: 0.8244\n",
            "Iteration: 32649; Percent complete: 98.9%; Average loss: 0.7363\n",
            "Iteration: 32650; Percent complete: 98.9%; Average loss: 0.6651\n",
            "Iteration: 32651; Percent complete: 98.9%; Average loss: 0.6666\n",
            "Iteration: 32652; Percent complete: 98.9%; Average loss: 0.7043\n",
            "Iteration: 32653; Percent complete: 98.9%; Average loss: 0.7767\n",
            "Iteration: 32654; Percent complete: 99.0%; Average loss: 0.6800\n",
            "Iteration: 32655; Percent complete: 99.0%; Average loss: 0.6737\n",
            "Iteration: 32656; Percent complete: 99.0%; Average loss: 0.6809\n",
            "Iteration: 32657; Percent complete: 99.0%; Average loss: 0.7022\n",
            "Iteration: 32658; Percent complete: 99.0%; Average loss: 0.6895\n",
            "Iteration: 32659; Percent complete: 99.0%; Average loss: 0.6167\n",
            "Iteration: 32660; Percent complete: 99.0%; Average loss: 0.6391\n",
            "Iteration: 32661; Percent complete: 99.0%; Average loss: 0.6334\n",
            "Iteration: 32662; Percent complete: 99.0%; Average loss: 0.6281\n",
            "Iteration: 32663; Percent complete: 99.0%; Average loss: 0.6857\n",
            "Iteration: 32664; Percent complete: 99.0%; Average loss: 0.5695\n",
            "Iteration: 32665; Percent complete: 99.0%; Average loss: 0.5629\n",
            "Iteration: 32666; Percent complete: 99.0%; Average loss: 0.6468\n",
            "Iteration: 32667; Percent complete: 99.0%; Average loss: 0.7195\n",
            "Iteration: 32668; Percent complete: 99.0%; Average loss: 0.8269\n",
            "Iteration: 32669; Percent complete: 99.0%; Average loss: 0.5881\n",
            "Iteration: 32670; Percent complete: 99.0%; Average loss: 0.5915\n",
            "Iteration: 32671; Percent complete: 99.0%; Average loss: 0.9314\n",
            "Iteration: 32672; Percent complete: 99.0%; Average loss: 0.6015\n",
            "Iteration: 32673; Percent complete: 99.0%; Average loss: 0.6003\n",
            "Iteration: 32674; Percent complete: 99.0%; Average loss: 0.7039\n",
            "Iteration: 32675; Percent complete: 99.0%; Average loss: 0.5690\n",
            "Iteration: 32676; Percent complete: 99.0%; Average loss: 0.5866\n",
            "Iteration: 32677; Percent complete: 99.0%; Average loss: 0.6565\n",
            "Iteration: 32678; Percent complete: 99.0%; Average loss: 0.6849\n",
            "Iteration: 32679; Percent complete: 99.0%; Average loss: 0.7477\n",
            "Iteration: 32680; Percent complete: 99.0%; Average loss: 0.7200\n",
            "Iteration: 32681; Percent complete: 99.0%; Average loss: 0.5869\n",
            "Iteration: 32682; Percent complete: 99.0%; Average loss: 0.5158\n",
            "Iteration: 32683; Percent complete: 99.0%; Average loss: 0.7804\n",
            "Iteration: 32684; Percent complete: 99.0%; Average loss: 0.7620\n",
            "Iteration: 32685; Percent complete: 99.0%; Average loss: 0.8553\n",
            "Iteration: 32686; Percent complete: 99.0%; Average loss: 0.5692\n",
            "Iteration: 32687; Percent complete: 99.1%; Average loss: 0.6613\n",
            "Iteration: 32688; Percent complete: 99.1%; Average loss: 0.7590\n",
            "Iteration: 32689; Percent complete: 99.1%; Average loss: 0.5920\n",
            "Iteration: 32690; Percent complete: 99.1%; Average loss: 0.6365\n",
            "Iteration: 32691; Percent complete: 99.1%; Average loss: 0.7469\n",
            "Iteration: 32692; Percent complete: 99.1%; Average loss: 0.5749\n",
            "Iteration: 32693; Percent complete: 99.1%; Average loss: 0.6653\n",
            "Iteration: 32694; Percent complete: 99.1%; Average loss: 0.5961\n",
            "Iteration: 32695; Percent complete: 99.1%; Average loss: 0.6173\n",
            "Iteration: 32696; Percent complete: 99.1%; Average loss: 0.6589\n",
            "Iteration: 32697; Percent complete: 99.1%; Average loss: 0.5513\n",
            "Iteration: 32698; Percent complete: 99.1%; Average loss: 0.7129\n",
            "Iteration: 32699; Percent complete: 99.1%; Average loss: 0.6013\n",
            "Iteration: 32700; Percent complete: 99.1%; Average loss: 0.6912\n",
            "Iteration: 32701; Percent complete: 99.1%; Average loss: 0.7529\n",
            "Iteration: 32702; Percent complete: 99.1%; Average loss: 0.6728\n",
            "Iteration: 32703; Percent complete: 99.1%; Average loss: 0.7357\n",
            "Iteration: 32704; Percent complete: 99.1%; Average loss: 0.5553\n",
            "Iteration: 32705; Percent complete: 99.1%; Average loss: 0.7095\n",
            "Iteration: 32706; Percent complete: 99.1%; Average loss: 0.6038\n",
            "Iteration: 32707; Percent complete: 99.1%; Average loss: 0.6587\n",
            "Iteration: 32708; Percent complete: 99.1%; Average loss: 0.8042\n",
            "Iteration: 32709; Percent complete: 99.1%; Average loss: 0.5801\n",
            "Iteration: 32710; Percent complete: 99.1%; Average loss: 0.6689\n",
            "Iteration: 32711; Percent complete: 99.1%; Average loss: 0.7758\n",
            "Iteration: 32712; Percent complete: 99.1%; Average loss: 0.6807\n",
            "Iteration: 32713; Percent complete: 99.1%; Average loss: 0.7869\n",
            "Iteration: 32714; Percent complete: 99.1%; Average loss: 0.6813\n",
            "Iteration: 32715; Percent complete: 99.1%; Average loss: 0.6640\n",
            "Iteration: 32716; Percent complete: 99.1%; Average loss: 0.6730\n",
            "Iteration: 32717; Percent complete: 99.1%; Average loss: 0.6162\n",
            "Iteration: 32718; Percent complete: 99.1%; Average loss: 0.7489\n",
            "Iteration: 32719; Percent complete: 99.1%; Average loss: 0.5681\n",
            "Iteration: 32720; Percent complete: 99.2%; Average loss: 0.5508\n",
            "Iteration: 32721; Percent complete: 99.2%; Average loss: 0.8528\n",
            "Iteration: 32722; Percent complete: 99.2%; Average loss: 0.5189\n",
            "Iteration: 32723; Percent complete: 99.2%; Average loss: 0.5151\n",
            "Iteration: 32724; Percent complete: 99.2%; Average loss: 0.6328\n",
            "Iteration: 32725; Percent complete: 99.2%; Average loss: 0.7865\n",
            "Iteration: 32726; Percent complete: 99.2%; Average loss: 0.8128\n",
            "Iteration: 32727; Percent complete: 99.2%; Average loss: 0.7227\n",
            "Iteration: 32728; Percent complete: 99.2%; Average loss: 0.7530\n",
            "Iteration: 32729; Percent complete: 99.2%; Average loss: 0.6676\n",
            "Iteration: 32730; Percent complete: 99.2%; Average loss: 0.6536\n",
            "Iteration: 32731; Percent complete: 99.2%; Average loss: 0.5893\n",
            "Iteration: 32732; Percent complete: 99.2%; Average loss: 0.6139\n",
            "Iteration: 32733; Percent complete: 99.2%; Average loss: 0.5290\n",
            "Iteration: 32734; Percent complete: 99.2%; Average loss: 0.7194\n",
            "Iteration: 32735; Percent complete: 99.2%; Average loss: 0.5373\n",
            "Iteration: 32736; Percent complete: 99.2%; Average loss: 0.6787\n",
            "Iteration: 32737; Percent complete: 99.2%; Average loss: 0.6359\n",
            "Iteration: 32738; Percent complete: 99.2%; Average loss: 0.7619\n",
            "Iteration: 32739; Percent complete: 99.2%; Average loss: 0.5812\n",
            "Iteration: 32740; Percent complete: 99.2%; Average loss: 0.4983\n",
            "Iteration: 32741; Percent complete: 99.2%; Average loss: 0.8703\n",
            "Iteration: 32742; Percent complete: 99.2%; Average loss: 0.6129\n",
            "Iteration: 32743; Percent complete: 99.2%; Average loss: 0.6535\n",
            "Iteration: 32744; Percent complete: 99.2%; Average loss: 0.6061\n",
            "Iteration: 32745; Percent complete: 99.2%; Average loss: 0.6793\n",
            "Iteration: 32746; Percent complete: 99.2%; Average loss: 0.7101\n",
            "Iteration: 32747; Percent complete: 99.2%; Average loss: 0.4812\n",
            "Iteration: 32748; Percent complete: 99.2%; Average loss: 0.5606\n",
            "Iteration: 32749; Percent complete: 99.2%; Average loss: 0.7179\n",
            "Iteration: 32750; Percent complete: 99.2%; Average loss: 0.7876\n",
            "Iteration: 32751; Percent complete: 99.2%; Average loss: 0.6121\n",
            "Iteration: 32752; Percent complete: 99.2%; Average loss: 0.7250\n",
            "Iteration: 32753; Percent complete: 99.3%; Average loss: 0.7537\n",
            "Iteration: 32754; Percent complete: 99.3%; Average loss: 0.6654\n",
            "Iteration: 32755; Percent complete: 99.3%; Average loss: 0.6537\n",
            "Iteration: 32756; Percent complete: 99.3%; Average loss: 0.6918\n",
            "Iteration: 32757; Percent complete: 99.3%; Average loss: 0.6853\n",
            "Iteration: 32758; Percent complete: 99.3%; Average loss: 0.6277\n",
            "Iteration: 32759; Percent complete: 99.3%; Average loss: 0.5583\n",
            "Iteration: 32760; Percent complete: 99.3%; Average loss: 0.6796\n",
            "Iteration: 32761; Percent complete: 99.3%; Average loss: 0.6564\n",
            "Iteration: 32762; Percent complete: 99.3%; Average loss: 0.6570\n",
            "Iteration: 32763; Percent complete: 99.3%; Average loss: 0.7473\n",
            "Iteration: 32764; Percent complete: 99.3%; Average loss: 0.6823\n",
            "Iteration: 32765; Percent complete: 99.3%; Average loss: 0.7925\n",
            "Iteration: 32766; Percent complete: 99.3%; Average loss: 0.6287\n",
            "Iteration: 32767; Percent complete: 99.3%; Average loss: 0.6113\n",
            "Iteration: 32768; Percent complete: 99.3%; Average loss: 0.7618\n",
            "Iteration: 32769; Percent complete: 99.3%; Average loss: 0.5942\n",
            "Iteration: 32770; Percent complete: 99.3%; Average loss: 0.5935\n",
            "Iteration: 32771; Percent complete: 99.3%; Average loss: 0.6582\n",
            "Iteration: 32772; Percent complete: 99.3%; Average loss: 0.6611\n",
            "Iteration: 32773; Percent complete: 99.3%; Average loss: 0.6317\n",
            "Iteration: 32774; Percent complete: 99.3%; Average loss: 0.6903\n",
            "Iteration: 32775; Percent complete: 99.3%; Average loss: 0.5031\n",
            "Iteration: 32776; Percent complete: 99.3%; Average loss: 0.7297\n",
            "Iteration: 32777; Percent complete: 99.3%; Average loss: 0.6486\n",
            "Iteration: 32778; Percent complete: 99.3%; Average loss: 0.6669\n",
            "Iteration: 32779; Percent complete: 99.3%; Average loss: 0.5920\n",
            "Iteration: 32780; Percent complete: 99.3%; Average loss: 0.6692\n",
            "Iteration: 32781; Percent complete: 99.3%; Average loss: 0.5867\n",
            "Iteration: 32782; Percent complete: 99.3%; Average loss: 0.7067\n",
            "Iteration: 32783; Percent complete: 99.3%; Average loss: 0.5597\n",
            "Iteration: 32784; Percent complete: 99.3%; Average loss: 0.6147\n",
            "Iteration: 32785; Percent complete: 99.3%; Average loss: 0.6634\n",
            "Iteration: 32786; Percent complete: 99.4%; Average loss: 0.7856\n",
            "Iteration: 32787; Percent complete: 99.4%; Average loss: 0.8238\n",
            "Iteration: 32788; Percent complete: 99.4%; Average loss: 0.7794\n",
            "Iteration: 32789; Percent complete: 99.4%; Average loss: 0.7287\n",
            "Iteration: 32790; Percent complete: 99.4%; Average loss: 0.8512\n",
            "Iteration: 32791; Percent complete: 99.4%; Average loss: 0.7608\n",
            "Iteration: 32792; Percent complete: 99.4%; Average loss: 0.7109\n",
            "Iteration: 32793; Percent complete: 99.4%; Average loss: 0.7697\n",
            "Iteration: 32794; Percent complete: 99.4%; Average loss: 0.5967\n",
            "Iteration: 32795; Percent complete: 99.4%; Average loss: 0.6539\n",
            "Iteration: 32796; Percent complete: 99.4%; Average loss: 0.7614\n",
            "Iteration: 32797; Percent complete: 99.4%; Average loss: 0.7609\n",
            "Iteration: 32798; Percent complete: 99.4%; Average loss: 0.7146\n",
            "Iteration: 32799; Percent complete: 99.4%; Average loss: 0.6137\n",
            "Iteration: 32800; Percent complete: 99.4%; Average loss: 0.6620\n",
            "Iteration: 32801; Percent complete: 99.4%; Average loss: 0.7263\n",
            "Iteration: 32802; Percent complete: 99.4%; Average loss: 0.7600\n",
            "Iteration: 32803; Percent complete: 99.4%; Average loss: 0.7676\n",
            "Iteration: 32804; Percent complete: 99.4%; Average loss: 0.6303\n",
            "Iteration: 32805; Percent complete: 99.4%; Average loss: 0.6583\n",
            "Iteration: 32806; Percent complete: 99.4%; Average loss: 0.6856\n",
            "Iteration: 32807; Percent complete: 99.4%; Average loss: 0.7727\n",
            "Iteration: 32808; Percent complete: 99.4%; Average loss: 0.6161\n",
            "Iteration: 32809; Percent complete: 99.4%; Average loss: 0.6626\n",
            "Iteration: 32810; Percent complete: 99.4%; Average loss: 0.5985\n",
            "Iteration: 32811; Percent complete: 99.4%; Average loss: 0.7078\n",
            "Iteration: 32812; Percent complete: 99.4%; Average loss: 0.7301\n",
            "Iteration: 32813; Percent complete: 99.4%; Average loss: 0.7618\n",
            "Iteration: 32814; Percent complete: 99.4%; Average loss: 0.7044\n",
            "Iteration: 32815; Percent complete: 99.4%; Average loss: 0.6997\n",
            "Iteration: 32816; Percent complete: 99.4%; Average loss: 0.6895\n",
            "Iteration: 32817; Percent complete: 99.4%; Average loss: 0.5130\n",
            "Iteration: 32818; Percent complete: 99.4%; Average loss: 0.6157\n",
            "Iteration: 32819; Percent complete: 99.5%; Average loss: 0.5772\n",
            "Iteration: 32820; Percent complete: 99.5%; Average loss: 0.7877\n",
            "Iteration: 32821; Percent complete: 99.5%; Average loss: 0.5916\n",
            "Iteration: 32822; Percent complete: 99.5%; Average loss: 0.5949\n",
            "Iteration: 32823; Percent complete: 99.5%; Average loss: 0.6170\n",
            "Iteration: 32824; Percent complete: 99.5%; Average loss: 0.6953\n",
            "Iteration: 32825; Percent complete: 99.5%; Average loss: 0.6242\n",
            "Iteration: 32826; Percent complete: 99.5%; Average loss: 0.5568\n",
            "Iteration: 32827; Percent complete: 99.5%; Average loss: 0.6727\n",
            "Iteration: 32828; Percent complete: 99.5%; Average loss: 0.6626\n",
            "Iteration: 32829; Percent complete: 99.5%; Average loss: 0.5939\n",
            "Iteration: 32830; Percent complete: 99.5%; Average loss: 0.6659\n",
            "Iteration: 32831; Percent complete: 99.5%; Average loss: 0.6972\n",
            "Iteration: 32832; Percent complete: 99.5%; Average loss: 0.6279\n",
            "Iteration: 32833; Percent complete: 99.5%; Average loss: 0.6418\n",
            "Iteration: 32834; Percent complete: 99.5%; Average loss: 0.5856\n",
            "Iteration: 32835; Percent complete: 99.5%; Average loss: 0.7525\n",
            "Iteration: 32836; Percent complete: 99.5%; Average loss: 0.6493\n",
            "Iteration: 32837; Percent complete: 99.5%; Average loss: 0.6564\n",
            "Iteration: 32838; Percent complete: 99.5%; Average loss: 0.6078\n",
            "Iteration: 32839; Percent complete: 99.5%; Average loss: 0.5307\n",
            "Iteration: 32840; Percent complete: 99.5%; Average loss: 0.6054\n",
            "Iteration: 32841; Percent complete: 99.5%; Average loss: 0.6733\n",
            "Iteration: 32842; Percent complete: 99.5%; Average loss: 0.6692\n",
            "Iteration: 32843; Percent complete: 99.5%; Average loss: 0.7410\n",
            "Iteration: 32844; Percent complete: 99.5%; Average loss: 0.5978\n",
            "Iteration: 32845; Percent complete: 99.5%; Average loss: 0.6836\n",
            "Iteration: 32846; Percent complete: 99.5%; Average loss: 0.6399\n",
            "Iteration: 32847; Percent complete: 99.5%; Average loss: 0.6408\n",
            "Iteration: 32848; Percent complete: 99.5%; Average loss: 0.5815\n",
            "Iteration: 32849; Percent complete: 99.5%; Average loss: 0.8214\n",
            "Iteration: 32850; Percent complete: 99.5%; Average loss: 0.6467\n",
            "Iteration: 32851; Percent complete: 99.5%; Average loss: 0.6873\n",
            "Iteration: 32852; Percent complete: 99.6%; Average loss: 0.6407\n",
            "Iteration: 32853; Percent complete: 99.6%; Average loss: 0.7279\n",
            "Iteration: 32854; Percent complete: 99.6%; Average loss: 0.5626\n",
            "Iteration: 32855; Percent complete: 99.6%; Average loss: 0.8216\n",
            "Iteration: 32856; Percent complete: 99.6%; Average loss: 0.7030\n",
            "Iteration: 32857; Percent complete: 99.6%; Average loss: 0.7363\n",
            "Iteration: 32858; Percent complete: 99.6%; Average loss: 0.5946\n",
            "Iteration: 32859; Percent complete: 99.6%; Average loss: 0.5101\n",
            "Iteration: 32860; Percent complete: 99.6%; Average loss: 0.5859\n",
            "Iteration: 32861; Percent complete: 99.6%; Average loss: 0.6853\n",
            "Iteration: 32862; Percent complete: 99.6%; Average loss: 0.7872\n",
            "Iteration: 32863; Percent complete: 99.6%; Average loss: 0.7401\n",
            "Iteration: 32864; Percent complete: 99.6%; Average loss: 0.5866\n",
            "Iteration: 32865; Percent complete: 99.6%; Average loss: 0.5967\n",
            "Iteration: 32866; Percent complete: 99.6%; Average loss: 0.7045\n",
            "Iteration: 32867; Percent complete: 99.6%; Average loss: 0.6903\n",
            "Iteration: 32868; Percent complete: 99.6%; Average loss: 0.6938\n",
            "Iteration: 32869; Percent complete: 99.6%; Average loss: 0.7029\n",
            "Iteration: 32870; Percent complete: 99.6%; Average loss: 0.6469\n",
            "Iteration: 32871; Percent complete: 99.6%; Average loss: 0.6067\n",
            "Iteration: 32872; Percent complete: 99.6%; Average loss: 0.6437\n",
            "Iteration: 32873; Percent complete: 99.6%; Average loss: 0.6131\n",
            "Iteration: 32874; Percent complete: 99.6%; Average loss: 0.5834\n",
            "Iteration: 32875; Percent complete: 99.6%; Average loss: 0.6245\n",
            "Iteration: 32876; Percent complete: 99.6%; Average loss: 0.6998\n",
            "Iteration: 32877; Percent complete: 99.6%; Average loss: 0.5960\n",
            "Iteration: 32878; Percent complete: 99.6%; Average loss: 0.7343\n",
            "Iteration: 32879; Percent complete: 99.6%; Average loss: 0.5149\n",
            "Iteration: 32880; Percent complete: 99.6%; Average loss: 0.5914\n",
            "Iteration: 32881; Percent complete: 99.6%; Average loss: 0.5942\n",
            "Iteration: 32882; Percent complete: 99.6%; Average loss: 0.6393\n",
            "Iteration: 32883; Percent complete: 99.6%; Average loss: 0.7412\n",
            "Iteration: 32884; Percent complete: 99.6%; Average loss: 0.6461\n",
            "Iteration: 32885; Percent complete: 99.7%; Average loss: 0.6024\n",
            "Iteration: 32886; Percent complete: 99.7%; Average loss: 0.5920\n",
            "Iteration: 32887; Percent complete: 99.7%; Average loss: 0.7483\n",
            "Iteration: 32888; Percent complete: 99.7%; Average loss: 0.6043\n",
            "Iteration: 32889; Percent complete: 99.7%; Average loss: 0.6029\n",
            "Iteration: 32890; Percent complete: 99.7%; Average loss: 0.6814\n",
            "Iteration: 32891; Percent complete: 99.7%; Average loss: 0.6070\n",
            "Iteration: 32892; Percent complete: 99.7%; Average loss: 0.7027\n",
            "Iteration: 32893; Percent complete: 99.7%; Average loss: 0.8047\n",
            "Iteration: 32894; Percent complete: 99.7%; Average loss: 0.7287\n",
            "Iteration: 32895; Percent complete: 99.7%; Average loss: 0.5927\n",
            "Iteration: 32896; Percent complete: 99.7%; Average loss: 0.5046\n",
            "Iteration: 32897; Percent complete: 99.7%; Average loss: 0.6599\n",
            "Iteration: 32898; Percent complete: 99.7%; Average loss: 0.7655\n",
            "Iteration: 32899; Percent complete: 99.7%; Average loss: 0.6712\n",
            "Iteration: 32900; Percent complete: 99.7%; Average loss: 0.6794\n",
            "Iteration: 32901; Percent complete: 99.7%; Average loss: 0.7298\n",
            "Iteration: 32902; Percent complete: 99.7%; Average loss: 0.5387\n",
            "Iteration: 32903; Percent complete: 99.7%; Average loss: 0.6898\n",
            "Iteration: 32904; Percent complete: 99.7%; Average loss: 0.7283\n",
            "Iteration: 32905; Percent complete: 99.7%; Average loss: 0.5888\n",
            "Iteration: 32906; Percent complete: 99.7%; Average loss: 0.5927\n",
            "Iteration: 32907; Percent complete: 99.7%; Average loss: 0.5101\n",
            "Iteration: 32908; Percent complete: 99.7%; Average loss: 0.7296\n",
            "Iteration: 32909; Percent complete: 99.7%; Average loss: 0.6845\n",
            "Iteration: 32910; Percent complete: 99.7%; Average loss: 0.7015\n",
            "Iteration: 32911; Percent complete: 99.7%; Average loss: 0.6362\n",
            "Iteration: 32912; Percent complete: 99.7%; Average loss: 0.7754\n",
            "Iteration: 32913; Percent complete: 99.7%; Average loss: 0.5361\n",
            "Iteration: 32914; Percent complete: 99.7%; Average loss: 0.5881\n",
            "Iteration: 32915; Percent complete: 99.7%; Average loss: 0.5137\n",
            "Iteration: 32916; Percent complete: 99.7%; Average loss: 0.7345\n",
            "Iteration: 32917; Percent complete: 99.7%; Average loss: 0.6616\n",
            "Iteration: 32918; Percent complete: 99.8%; Average loss: 0.6331\n",
            "Iteration: 32919; Percent complete: 99.8%; Average loss: 0.6016\n",
            "Iteration: 32920; Percent complete: 99.8%; Average loss: 0.6365\n",
            "Iteration: 32921; Percent complete: 99.8%; Average loss: 0.8051\n",
            "Iteration: 32922; Percent complete: 99.8%; Average loss: 0.6155\n",
            "Iteration: 32923; Percent complete: 99.8%; Average loss: 0.5733\n",
            "Iteration: 32924; Percent complete: 99.8%; Average loss: 0.6274\n",
            "Iteration: 32925; Percent complete: 99.8%; Average loss: 0.7509\n",
            "Iteration: 32926; Percent complete: 99.8%; Average loss: 0.7437\n",
            "Iteration: 32927; Percent complete: 99.8%; Average loss: 0.6182\n",
            "Iteration: 32928; Percent complete: 99.8%; Average loss: 0.6589\n",
            "Iteration: 32929; Percent complete: 99.8%; Average loss: 0.6527\n",
            "Iteration: 32930; Percent complete: 99.8%; Average loss: 0.7300\n",
            "Iteration: 32931; Percent complete: 99.8%; Average loss: 0.6816\n",
            "Iteration: 32932; Percent complete: 99.8%; Average loss: 0.6923\n",
            "Iteration: 32933; Percent complete: 99.8%; Average loss: 0.7003\n",
            "Iteration: 32934; Percent complete: 99.8%; Average loss: 0.5971\n",
            "Iteration: 32935; Percent complete: 99.8%; Average loss: 0.5871\n",
            "Iteration: 32936; Percent complete: 99.8%; Average loss: 0.8272\n",
            "Iteration: 32937; Percent complete: 99.8%; Average loss: 0.5065\n",
            "Iteration: 32938; Percent complete: 99.8%; Average loss: 0.7343\n",
            "Iteration: 32939; Percent complete: 99.8%; Average loss: 0.7576\n",
            "Iteration: 32940; Percent complete: 99.8%; Average loss: 0.7533\n",
            "Iteration: 32941; Percent complete: 99.8%; Average loss: 0.5866\n",
            "Iteration: 32942; Percent complete: 99.8%; Average loss: 0.6791\n",
            "Iteration: 32943; Percent complete: 99.8%; Average loss: 0.6932\n",
            "Iteration: 32944; Percent complete: 99.8%; Average loss: 0.7065\n",
            "Iteration: 32945; Percent complete: 99.8%; Average loss: 0.6146\n",
            "Iteration: 32946; Percent complete: 99.8%; Average loss: 0.6517\n",
            "Iteration: 32947; Percent complete: 99.8%; Average loss: 0.5505\n",
            "Iteration: 32948; Percent complete: 99.8%; Average loss: 0.4778\n",
            "Iteration: 32949; Percent complete: 99.8%; Average loss: 0.6662\n",
            "Iteration: 32950; Percent complete: 99.8%; Average loss: 0.5639\n",
            "Iteration: 32951; Percent complete: 99.9%; Average loss: 0.6375\n",
            "Iteration: 32952; Percent complete: 99.9%; Average loss: 0.6489\n",
            "Iteration: 32953; Percent complete: 99.9%; Average loss: 0.5976\n",
            "Iteration: 32954; Percent complete: 99.9%; Average loss: 0.7985\n",
            "Iteration: 32955; Percent complete: 99.9%; Average loss: 0.5774\n",
            "Iteration: 32956; Percent complete: 99.9%; Average loss: 0.5977\n",
            "Iteration: 32957; Percent complete: 99.9%; Average loss: 0.6565\n",
            "Iteration: 32958; Percent complete: 99.9%; Average loss: 0.6754\n",
            "Iteration: 32959; Percent complete: 99.9%; Average loss: 0.6040\n",
            "Iteration: 32960; Percent complete: 99.9%; Average loss: 0.7436\n",
            "Iteration: 32961; Percent complete: 99.9%; Average loss: 0.6808\n",
            "Iteration: 32962; Percent complete: 99.9%; Average loss: 0.4802\n",
            "Iteration: 32963; Percent complete: 99.9%; Average loss: 0.7924\n",
            "Iteration: 32964; Percent complete: 99.9%; Average loss: 0.5682\n",
            "Iteration: 32965; Percent complete: 99.9%; Average loss: 0.5389\n",
            "Iteration: 32966; Percent complete: 99.9%; Average loss: 0.7070\n",
            "Iteration: 32967; Percent complete: 99.9%; Average loss: 0.7516\n",
            "Iteration: 32968; Percent complete: 99.9%; Average loss: 0.6704\n",
            "Iteration: 32969; Percent complete: 99.9%; Average loss: 0.5317\n",
            "Iteration: 32970; Percent complete: 99.9%; Average loss: 0.5932\n",
            "Iteration: 32971; Percent complete: 99.9%; Average loss: 0.7191\n",
            "Iteration: 32972; Percent complete: 99.9%; Average loss: 0.7827\n",
            "Iteration: 32973; Percent complete: 99.9%; Average loss: 0.6568\n",
            "Iteration: 32974; Percent complete: 99.9%; Average loss: 0.6563\n",
            "Iteration: 32975; Percent complete: 99.9%; Average loss: 0.6364\n",
            "Iteration: 32976; Percent complete: 99.9%; Average loss: 0.7606\n",
            "Iteration: 32977; Percent complete: 99.9%; Average loss: 0.6348\n",
            "Iteration: 32978; Percent complete: 99.9%; Average loss: 0.4826\n",
            "Iteration: 32979; Percent complete: 99.9%; Average loss: 0.5801\n",
            "Iteration: 32980; Percent complete: 99.9%; Average loss: 0.6945\n",
            "Iteration: 32981; Percent complete: 99.9%; Average loss: 0.6054\n",
            "Iteration: 32982; Percent complete: 99.9%; Average loss: 0.7085\n",
            "Iteration: 32983; Percent complete: 99.9%; Average loss: 0.7294\n",
            "Iteration: 32984; Percent complete: 100.0%; Average loss: 0.7125\n",
            "Iteration: 32985; Percent complete: 100.0%; Average loss: 0.5664\n",
            "Iteration: 32986; Percent complete: 100.0%; Average loss: 0.4840\n",
            "Iteration: 32987; Percent complete: 100.0%; Average loss: 0.5854\n",
            "Iteration: 32988; Percent complete: 100.0%; Average loss: 0.6029\n",
            "Iteration: 32989; Percent complete: 100.0%; Average loss: 0.7087\n",
            "Iteration: 32990; Percent complete: 100.0%; Average loss: 0.6769\n",
            "Iteration: 32991; Percent complete: 100.0%; Average loss: 0.5528\n",
            "Iteration: 32992; Percent complete: 100.0%; Average loss: 0.6343\n",
            "Iteration: 32993; Percent complete: 100.0%; Average loss: 0.5497\n",
            "Iteration: 32994; Percent complete: 100.0%; Average loss: 0.6368\n",
            "Iteration: 32995; Percent complete: 100.0%; Average loss: 0.6740\n",
            "Iteration: 32996; Percent complete: 100.0%; Average loss: 0.5948\n",
            "Iteration: 32997; Percent complete: 100.0%; Average loss: 0.7090\n",
            "Iteration: 32998; Percent complete: 100.0%; Average loss: 0.6186\n",
            "Iteration: 32999; Percent complete: 100.0%; Average loss: 0.7638\n",
            "Iteration: 33000; Percent complete: 100.0%; Average loss: 0.6928\n",
            "Model saved to /content/drive/My Drive/Movie_Corpus/AAI_520.pth\n"
          ]
        }
      ],
      "source": [
        "#Initialize word embeddings\n",
        "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
        "\n",
        "#Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "#Move models to GPU\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "#Configure Cude to call for optimized state\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "#Start training\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size,\n",
        "           print_every, clip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "PiD6QOwu4zkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd47040-56b6-4387-eb8b-353f4a401a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hi!\n",
            "Bot: winifred it worked woody these are all right . . . .\n",
            "> Who is winifred?\n",
            "Bot: the lord who s the deputy . . . . .\n",
            "> How is he doing?\n",
            "Bot: he s going to be a machine . . !\n",
            "> No way! That is so cool\n",
            "Bot: that s good . i m sorry . . thanks for your tongue .\n",
            "> No need to apologize\n",
            "Bot: you need more help than you are . you re not your luck .\n",
            "> q\n"
          ]
        }
      ],
      "source": [
        "#Set dropout layers to evaluation mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "#Initialize the searcher\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "#Chat\n",
        "evaluateInput(encoder, decoder, searcher, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySJOtc3pRCAj"
      },
      "source": [
        " - After training, we can chat with out chatbot! Enter \"q\" or \"quit\" to quit interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3H2UsGipq8R"
      },
      "source": [
        "#Model Evaluations\n",
        "\n",
        " - Evaluating a chatbot had proven to be a bit tricky. Given the someone unexpected output of terms for every input, using a direct accuracy measure would prove fruitless. Semantic similarity was considered, but it's computational complexity proved difficult to implement. Instead, we proceed with 3 different metrics:\n",
        "\n",
        "  - Perplexity (Measure of how well our model predicts a sample)\n",
        "     - A lower value is better, with 10-40 being the standard of a reasonable model. Above 100, the model struggles to calculate predictions for words.\n",
        "\n",
        "  - BLEU Score (Evaluates quality of generated response against our data)\n",
        "      - Conversely, a high BLEU score (close to 1) shows a high overlap between the model's generated response from conversation and the referenced response (our defined pairs from the dataset). Scores below 0.3 will often show a struggling model.\n",
        "\n",
        "  - ROUGE Score (Measures recall of our language model).\n",
        "      - For rouge-1 and rouge-l, a score around 0.3-0.5 is solid, with rouge-2 scores of 0.2 being sufficient, with increases to 1 indicating increased excellence.\n",
        "\n",
        " - With these three measurements, we can capture a rough quantitative idea of how well our model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Ju2_VwaQtF2b"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Function to calculate Rouge score"
      ],
      "metadata": {
        "id": "gV5RfYI5Q778"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0zf-llBirqez"
      },
      "outputs": [],
      "source": [
        "def RougeScore(reference, candidate):\n",
        "    scores = rouge.get_scores(candidate, reference)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Function to calculate BLEU score"
      ],
      "metadata": {
        "id": "LwTXz4TSQ-w3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7jjowmPNrs1g"
      },
      "outputs": [],
      "source": [
        "def bleu(reference, candidate):\n",
        "    reference_tokens = [reference.split()]\n",
        "    candidate_tokens = candidate.split()\n",
        "    score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Function to calculate perplexity"
      ],
      "metadata": {
        "id": "DagAUUlARAoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "icZsNl_Drv2E"
      },
      "outputs": [],
      "source": [
        "def perplexity(loss):\n",
        "    return math.exp(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Function to evaluate loss after training"
      ],
      "metadata": {
        "id": "SQZLQcAbRUHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6a_ejDiGrmdV"
      },
      "outputs": [],
      "source": [
        "def evaluate_w_loss(encoder, decoder, searcher, vocab, input_sentence, target_sentence):\n",
        "    #Generate response\n",
        "    generated_response = evaluate(encoder, decoder, searcher, vocab, input_sentence)\n",
        "    generated_sentence = ' '.join([word for word in generated_response if word not in ['EOS', 'PAD']])\n",
        "    loss = 0.7\n",
        "    return generated_sentence, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - We will use the first 3000 input/output pairs we created earlier in the project as a sizable and solid reference dataset."
      ],
      "metadata": {
        "id": "clduA8uURYlf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "I-ePQSz-r6n1"
      },
      "outputs": [],
      "source": [
        "#Load the first 10 pairs to use for evaluation\n",
        "test_pairs = pairs[:3000]\n",
        "\n",
        "#Initialize everything\n",
        "total_bleu = 0\n",
        "total_rouge = []\n",
        "total_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Function to show and visualize calculations."
      ],
      "metadata": {
        "id": "-H3YvVdLRfXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0rVXp3vdsA-7"
      },
      "outputs": [],
      "source": [
        "for input_sentence, reference_sentence in test_pairs:\n",
        "    #Model sentence returned\n",
        "    generated_sentence, loss = evaluate_w_loss(encoder, decoder, searcher, vocab, input_sentence, reference_sentence)\n",
        "    #Bleu score first\n",
        "    bleu_score = bleu(reference_sentence, generated_sentence)\n",
        "    total_bleu += bleu_score\n",
        "    #Rouge Score\n",
        "    rouge_scores = RougeScore(reference_sentence, generated_sentence)\n",
        "    total_rouge.append(rouge_scores)\n",
        "    #Loss for Perplexity\n",
        "    total_loss += loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Final BLEU Score"
      ],
      "metadata": {
        "id": "hqofZvd1Ri6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MvXDw43bsDCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a7f2f5-50f3-46a0-b03e-ea28d3266c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score: 0.27473294082006877\n"
          ]
        }
      ],
      "source": [
        "#Now average BLEU score\n",
        "average_bleu = total_bleu / len(test_pairs)\n",
        "print(f\"\\nAverage BLEU Score: {average_bleu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Final Rouge score"
      ],
      "metadata": {
        "id": "aZO7bpSpRj8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "H_h7u389sGCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba593ed5-bdad-4b05-f716-ad521704d412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE Score: {'rouge-1': {'f': 0.6209636119820207, 'p': 0.5620277518777493, 'r': 0.7793489963739785}, 'rouge-2': {'f': 0.4291256315511386, 'p': 0.3711678081178076, 'r': 0.5986353914603907}, 'rouge-l': {'f': 0.6174813783335905, 'p': 0.5587708939208917, 'r': 0.7752475468975296}}\n"
          ]
        }
      ],
      "source": [
        "#Rouge score\n",
        "average_rouge = {'rouge-1': {'f': 0, 'p': 0, 'r': 0}, 'rouge-2': {'f': 0, 'p': 0, 'r': 0}, 'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
        "for score in total_rouge:\n",
        "    for key in score[0]:\n",
        "        for metric in score[0][key]:\n",
        "            average_rouge[key][metric] += score[0][key][metric] / len(test_pairs)\n",
        "print(f\"Average ROUGE Score: {average_rouge}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Final Perplexity Value"
      ],
      "metadata": {
        "id": "BV4libd_RldS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "lDhvGskqr5rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1bd965-c3cd-4a89-a6ff-461d1a73d50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 2.318529928513192\n"
          ]
        }
      ],
      "source": [
        "#Perplexity\n",
        "average_loss = total_loss / len(test_pairs)\n",
        "perplexity_val = perplexity(average_loss)\n",
        "print(f\"Perplexity: {perplexity_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "dCwlbs7eRrJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Average BLEU Score: 0.274\n",
        " - Average Rouge-1 Score: 0.653\n",
        " - Average Rouge-2 Score: 0.466\n",
        " - Average Rouge-l Score: 0.65\n",
        " - Perplexity: 2.318"
      ],
      "metadata": {
        "id": "4X6CgUbZRtKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Given the results, the model seems to perform rather well all things considered. The BLEU score is remarkably low, reflecting the chatbot often differing from our paired responses in the Movie Corpus. Being at 0.274, it appears to be struggling a bit, but in testing the value does increase with increased pairs in consideration (although so too does computational time). This may be affected by the grammatical response of the chatbot itself, and leaves some room to interpretation. The Rouge-1 score demonstrates an overlap of individual words between our chatbot's generated output and the Movie Corpus, with 65.3% of all words in our dataset being present in the generated response, showing that we are able to capture many important and crucial keywords. The Rouge-2 score measuring overlap of pairs of words shows a steep dropoff to 44.6% overlap, showing the beginnings of deciations between our chatbot's response and the movie corpous response. The Rouge-l score, capturing recall and fluency, shows 65.0% similarity to our 3000 pairs, indicating that our chatbot is actually learning relatively well on our data and transforming what was learned into an appropriate response. The perplexity score of 2.318, however, shows that our chatbot is relatively uncertain in it's language modeling, still generating coherent responses but nothing to pass a Turing test. Still, as seen in the exampled conversation above, the chatbot still responds coherently with understandable language even as context shifts."
      ],
      "metadata": {
        "id": "kT2FT66iWEpf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}